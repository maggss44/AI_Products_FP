{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Carga el archivo .env\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [\n",
    "#     \"Carbon trading is a market-based approach to reducing greenhouse gas emissions by providing economic incentives for companies to limit their carbon footprint.\",\n",
    "#     \"In a carbon trading system, companies are allocated a certain number of carbon credits, which represent the right to emit a specific amount of carbon dioxide or other greenhouse gases.\",\n",
    "#     \"Companies that emit less than their allocated carbon credits can sell their excess credits to companies that exceed their emissions limits, creating a market for carbon credits.\",\n",
    "#     \"The goal of carbon trading is to encourage companies to invest in cleaner technologies and adopt more sustainable practices to reduce their emissions and avoid the cost of purchasing additional carbon credits.\",\n",
    "#     \"Environmental, Social, and Governance (ESG) criteria are a set of standards used by investors to evaluate a company's sustainability and ethical impact.\",\n",
    "#     \"ESG factors consider a company's environmental impact, such as its carbon footprint, waste management, and use of renewable energy.\",\n",
    "#     \"Social aspects of ESG include a company's labor practices, diversity and inclusion policies, and community engagement.\",\n",
    "#     \"Governance factors in ESG assess a company's leadership structure, executive compensation, and transparency in decision-making processes.\",\n",
    "#     \"Investors are increasingly using ESG criteria to identify companies that are better positioned to manage risks and opportunities related to sustainability and social responsibility.\",\n",
    "#     \"Companies with strong ESG performance tend to have better long-term financial prospects, as they are more resilient to environmental and social challenges and are favored by environmentally and socially conscious consumers.\",\n",
    "#     \"Carbon trading and ESG are closely related, as companies with lower carbon emissions and better sustainability practices tend to have higher ESG ratings.\",\n",
    "#     \"Governments and international organizations are promoting carbon trading and ESG investing as key strategies for mitigating climate change and transitioning to a low-carbon economy.\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 3595861 characters\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "data = data.map(lambda x: {\n",
    "    'uid': f\"{x['doi']}-{x['chunk-id']}\"\n",
    "})\n",
    "\n",
    "data = data[\"chunk\"]\n",
    "\n",
    "total_length = sum(len(text) for text in data)\n",
    "print(f\"Total length: {total_length} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(text) for text in data)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743.2536171971889"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_length =  total_length / len(data)\n",
    "mean_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the list: 4838\n"
     ]
    }
   ],
   "source": [
    "num_elements = len(data)\n",
    "print(f\"Number of elements in the list: {num_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = [text for text in data if len(text) <= 700]\n",
    "filtered_data_length = len(filtered_data)\n",
    "filtered_data_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements to retain: 993\n"
     ]
    }
   ],
   "source": [
    "# Calcular el 30% del nÃºmero total de elementos\n",
    "num_elements = len(filtered_data)\n",
    "num_to_retain = int(num_elements * 0.5)\n",
    "\n",
    "# Seleccionar aleatoriamente el 30% de los elementos\n",
    "data = random.sample(filtered_data, num_to_retain)\n",
    "\n",
    "print(f\"Number of elements to retain: {num_to_retain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_form_llama2 = [\n",
    "    \"What is LLaMA 2?\",\n",
    "    \"Who developed LLaMA 2?\",\n",
    "    \"What are the key features of LLaMA 2?\",\n",
    "    \"How does LLaMA 2 differ from its predecessor, LLaMA?\",\n",
    "    \"What are the applications of LLaMA 2?\",\n",
    "    \"What industries benefit most from using LLaMA 2?\",\n",
    "    \"What are the limitations of LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle multi-language support?\",\n",
    "    \"What datasets were used to train LLaMA 2?\",\n",
    "    \"Is LLaMA 2 open-source?\",\n",
    "    \"What is the architecture of LLaMA 2?\",\n",
    "    \"How many parameters does LLaMA 2 have?\",\n",
    "    \"What kind of data is LLaMA 2 trained on?\",\n",
    "    \"What are the hardware requirements for running LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle contextual understanding?\",\n",
    "    \"What optimization techniques are used in LLaMA 2?\",\n",
    "    \"How does LLaMA 2 compare to GPT-4 in terms of performance?\",\n",
    "    \"What is the training process of LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle long-form content generation?\",\n",
    "    \"What are the memory and processing requirements for LLaMA 2?\",\n",
    "    \"How to integrate LLaMA 2 into existing applications?\",\n",
    "    \"What programming languages are compatible with LLaMA 2?\",\n",
    "    \"How to fine-tune LLaMA 2 for specific tasks?\",\n",
    "    \"What are the best practices for deploying LLaMA 2 in production?\",\n",
    "    \"How to use LLaMA 2 for natural language understanding (NLU)?\",\n",
    "    \"How to implement LLaMA 2 in a chatbot?\",\n",
    "    \"What are the API options available for LLaMA 2?\",\n",
    "    \"How to handle data privacy and security when using LLaMA 2?\",\n",
    "    \"What are some common use cases of LLaMA 2 in customer service?\",\n",
    "    \"How to monitor and maintain LLaMA 2 performance over time?\",\n",
    "    \"How does LLaMA 2 compare to other language models like GPT-4 and BERT?\",\n",
    "    \"What benchmarks are used to evaluate LLaMA 2?\",\n",
    "    \"What are the strengths of LLaMA 2 over other models?\",\n",
    "    \"What are the weaknesses of LLaMA 2 compared to its competitors?\",\n",
    "    \"How does LLaMA 2 perform in specific tasks like summarization and translation?\",\n",
    "    \"What are user reviews and feedback on LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle bias and ethical concerns?\",\n",
    "    \"What are the ethical implications of using LLaMA 2?\",\n",
    "    \"How to evaluate the output quality of LLaMA 2?\",\n",
    "    \"What are the performance metrics for LLaMA 2?\",\n",
    "    \"How to train a custom version of LLaMA 2 from scratch?\",\n",
    "    \"How to leverage transfer learning with LLaMA 2?\",\n",
    "    \"What are the latest research advancements related to LLaMA 2?\",\n",
    "    \"How to improve LLaMA 2's performance for specific domains?\",\n",
    "    \"What are some advanced techniques for optimizing LLaMA 2?\",\n",
    "    \"How to use LLaMA 2 for generating code?\",\n",
    "    \"How does LLaMA 2 handle rare and low-frequency words?\",\n",
    "    \"What are some innovative applications of LLaMA 2 in creative industries?\",\n",
    "    \"How to ensure the ethical use of LLaMA 2 in sensitive applications?\",\n",
    "    \"What future developments can be expected for LLaMA 2?\",\n",
    "    \"Where to find tutorials and guides on LLaMA 2?\",\n",
    "    \"What are some popular forums and communities discussing LLaMA 2?\",\n",
    "    \"How to contribute to the development of LLaMA 2?\",\n",
    "    \"What are some notable projects using LLaMA 2?\",\n",
    "    \"How to get support for LLaMA 2-related issues?\",\n",
    "    \"Are there any workshops or webinars on LLaMA 2?\",\n",
    "    \"What are the best books or research papers on LLaMA 2?\",\n",
    "    \"How to stay updated with the latest news on LLaMA 2?\",\n",
    "    \"What are some challenges faced by the LLaMA 2 user community?\",\n",
    "    \"How to network with other professionals using LLaMA 2?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    def __init__(self, data: List[str]):\n",
    "        \"\"\"\n",
    "        Initializes the DataSource object with a list of data.\n",
    "\n",
    "        Args:\n",
    "            data (List[str]): The input list of text data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, and removing extra whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "        return text\n",
    "\n",
    "    def preprocess_text_advanced(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, removing extra whitespace,\n",
    "        removing numbers, removing stop words, and lemmatizing the words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed and cleaned text.\n",
    "        \"\"\"\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower() \n",
    "        # Remove punctuation and numeric characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Lemmatize the words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        # Join the tokens back into a string\n",
    "        text = ' '.join(tokens)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the preprocessed text into a list of words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The preprocessed text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of tokens (words) from the text.\n",
    "        \"\"\"\n",
    "        return self.preprocess_text_advanced(text).split()  # Tokenize the preprocessed text by splitting on spaces\n",
    "\n",
    "    def process_data(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Processes the data by applying advanced preprocessing to each sentence in the data list.\n",
    "\n",
    "        Updates the object's processed_data attribute with the cleaned and preprocessed text.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.processed_data = [self.preprocess_text_advanced(sentence) for sentence in self.data] # Apply preprocessing to each sentence in the data list\n",
    "\n",
    "        return self.processed_data  # Apply preprocessing to each sentence in the data list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserQuery:\n",
    "    def __init__(self, query: str):\n",
    "        self.query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, model_name: str, device: str = 'cpu', use_local: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the Embedding object with the specified model name, device, and whether to use a local model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the embedding model to be used.\n",
    "            device (str): The device to be used for running the model (\"cpu\" or \"cuda\"). Default is \"cpu\".\n",
    "            use_local (bool): Whether to use a locally available model. Default is False.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.use_local = use_local\n",
    "        # Mapping of model names to their expected dimensions\n",
    "        self.model_dimensions = {\n",
    "            'all-MiniLM-L6-v2': 384,  # Example dimension for a SentenceTransformer model\n",
    "            'jina-v2-base-en-embed': 768,  # Specified dimension for your local model\n",
    "            # Add other models and their dimensions here\n",
    "        }\n",
    "        self.current_model_dimension = self.model_dimensions.get(model_name, None)\n",
    "\n",
    "        if use_local:\n",
    "            raise ValueError(f\"Not yet implemented\")\n",
    "        else:\n",
    "            self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches the embedding model to a different model and device.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new embedding model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        # Here we assume the model switch is successful and just set the dimension\n",
    "        if model_name in self.model_dimensions:\n",
    "            self.current_model_dimension = self.model_dimensions[model_name]\n",
    "            print(f\"Switched to model '{model_name}' with dimension {self.current_model_dimension}\")\n",
    "        else:\n",
    "            print(f\"Model '{model_name}' not recognized. Unable to switch models.\")\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Embeds the input text into a vector representation using the current embedding model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: The vector representation of the input text.\n",
    "        \"\"\"\n",
    "        if self.use_local:\n",
    "          raise ValueError(f\"Not yet implemented!\")\n",
    "        else:\n",
    "            result = self.model.encode(text).tolist()\n",
    "            if len(result) != self.current_model_dimension:\n",
    "                print(f\"Dimension mismatch detected: Expected {self.current_model_dimension}, got {len(result)}\")\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStorage:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]]):\n",
    "        # Placeholder method for storing vectors\n",
    "        pass\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int) -> List[int]:\n",
    "        # Placeholder method for searching vectors\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeVectorStorage(VectorStorage):\n",
    "    def __init__(self, index_name: str, embedding: Embedding):\n",
    "        \"\"\"\n",
    "        Initializes the PineconeVectorStorage object with the specified index name and embedding model.\n",
    "\n",
    "        Args:\n",
    "            index_name (str): The name of the Pinecone index to be created or used.\n",
    "            embedding (Embedding): An instance of the Embedding class providing the embedding model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "        self.pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        if index_name not in self.pinecone.list_indexes().names():\n",
    "            self.pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=embedding.model.get_sentence_embedding_dimension(),\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "            )\n",
    "        self.index = self.pinecone.Index(index_name)\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]], metadatas: List[dict]):\n",
    "        \"\"\"\n",
    "        Stores vectors and associated metadata in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            vectors (List[List[float]]): List of vectors to be stored in the index.\n",
    "            metadatas (List[dict]): List of dictionaries containing metadata associated with each vector.\n",
    "        \"\"\"\n",
    "        ids = [str(i) for i in range(len(vectors))]\n",
    "        records = zip(ids, vectors, metadatas)\n",
    "        self.index.upsert(vectors=records)\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int, filter_metadata: Dict[str, str] = None) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Searches for vectors similar to the given query vector in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            query_vector (List[float]): The query vector for similarity search.\n",
    "            top_n (int): The number of top results to return.\n",
    "            filter_metadata (Dict[str, str], optional): Metadata filters to apply during search. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: List of dictionaries containing metadata of the top similar vectors.\n",
    "        \"\"\"\n",
    "        query_params = {\n",
    "            'top_k': top_n,\n",
    "            'vector': query_vector,\n",
    "            'include_metadata': True,\n",
    "            'include_values': False\n",
    "        }\n",
    "        if filter_metadata is not None:\n",
    "            query_params['metadata'] = filter_metadata\n",
    "\n",
    "        results = self.index.query(**query_params)\n",
    "        return results['matches']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAndRanking:\n",
    "    def __init__(self, data_source: DataSource, embedding: Embedding, vector_storage: VectorStorage):\n",
    "        \"\"\"\n",
    "        Initializes the RetrievalAndRanking object with a data source, embedding model, and vector storage.\n",
    "\n",
    "        Args:\n",
    "            data_source (DataSource): The data source containing the preprocessed text data.\n",
    "            embedding (Embedding): The embedding model used to generate vector representations of text.\n",
    "            vector_storage (VectorStorage): The storage for vector representations of text data.\n",
    "        \"\"\"\n",
    "        self.data_source = data_source\n",
    "        self.embedding = embedding\n",
    "        self.vector_storage = vector_storage\n",
    "\n",
    "    def retrieve_relevant_chunks(self, query: str, top_n: int = 2) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Jaccard similarity.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            top_n (int): The number of top relevant chunks to retrieve (default is 2).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of top relevant chunks from the data source.\n",
    "        \"\"\"\n",
    "        # Tokenize and convert query to a set of tokens\n",
    "        query_tokens = set(self.data_source.tokenize(query)) \n",
    "        # Initialize a list to store chunks and their similarity scores\n",
    "        similarities: List[Tuple[str, float]] = []  \n",
    "\n",
    "        for chunk in self.data_source.processed_data:\n",
    "            # Tokenize and convert chunk to a set of tokens\n",
    "            chunk_tokens = set(self.data_source.tokenize(chunk))\n",
    "            # Calculate Jaccard similarity between query and chunk\n",
    "            similarity = len(query_tokens.intersection(chunk_tokens)) / len(\n",
    "                query_tokens.union(chunk_tokens)\n",
    "            )\n",
    "            # Append the chunk and its similarity score to the list\n",
    "            similarities.append((chunk, similarity))\n",
    "\n",
    "        # Sort the chunks by similarity score in descending order\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the top-n most relevant chunks\n",
    "        return [chunk for chunk, _ in similarities[:top_n]]\n",
    "\n",
    "    def retrieve_relevant_chunks_euclidean(self, query: str, top_n: int = 2) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Euclidean distance on TF-IDF vectors.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            top_n (int): The number of top relevant chunks to retrieve (default is 2).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of top relevant chunks from the data source.\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer()  # Create a TF-IDF vectorizer\n",
    "        # Tokenize each string in the data source and join the tokens back into strings\n",
    "        tokenized_data_source = [' '.join(self.data_source.tokenize(text)) for text in self.data_source.processed_data]\n",
    "        # Fit and transform the tokenized data source into a TF-IDF matrix\n",
    "        tfidf_matrix = vectorizer.fit_transform(tokenized_data_source)\n",
    "        # Tokenize the query and join the tokens back into a string\n",
    "        tokenized_query = ' '.join(self.data_source.tokenize(query))\n",
    "        # Transform the tokenized query into a TF-IDF vector\n",
    "        query_vector = vectorizer.transform([tokenized_query])\n",
    "        # Calculate the Euclidean distance between the query vector and each chunk vector\n",
    "        similarities = euclidean_distances(query_vector, tfidf_matrix).flatten()\n",
    "        # Get the indices of the top-n closest chunks\n",
    "        top_indices = similarities.argsort()[:top_n]\n",
    "\n",
    "        # Return the top-n closest chunks from the data source\n",
    "        return [self.data_source.processed_data[i] for i in top_indices]\n",
    "\n",
    "    def retrieve_relevant_chunks_pinecone(self, queries: List[str], top_n: int = 2, filter_metadata: Dict[str, str] = None) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Pinecone vector storage.\n",
    "\n",
    "        Args:\n",
    "            queries (List[str]): A list of user queries.\n",
    "            top_n (int): The number of top relevant chunks to retrieve for each query (default is 2).\n",
    "            filter_metadata (Dict[str, str], optional): A dictionary containing metadata key-value pairs to filter the results (default is None).\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A list containing the top-n most relevant chunks for each query.\n",
    "        \"\"\"\n",
    "        relevant_chunks = []\n",
    "\n",
    "        for query in queries:\n",
    "            query_embedding = self.embedding.embed(query)\n",
    "            results = self.vector_storage.search_vectors(query_embedding, top_n)\n",
    "\n",
    "            filtered_results = []\n",
    "            if filter_metadata is not None:\n",
    "                for result in results:\n",
    "                    metadata = result.get('metadata', {})\n",
    "                    if all(metadata.get(key) == value for key, value in filter_metadata.items()):\n",
    "                        filtered_results.append(result)\n",
    "            else:\n",
    "                filtered_results = results\n",
    "\n",
    "            relevant_chunks.append([result['metadata']['text'] for result in filtered_results])\n",
    "\n",
    "        return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, api_key: str = None, model_name: str = None, device: str = \"cuda\", consumer_group: str = \"mistral\"):\n",
    "        \"\"\"\n",
    "        Initializes the LLM object with the specified API key, model name, device, and consumer group.\n",
    "        Sets up the OpenAI client if an API key is provided.\n",
    "\n",
    "        Args:\n",
    "            api_key (str, optional): The API key for accessing the OpenAI service. Default is None.\n",
    "            model_name (str, optional): The name of the model to be used. Default is None.\n",
    "            device (str, optional): The device to be used for running the model (\"cuda\" or \"cpu\"). Default is \"cuda\".\n",
    "            consumer_group (str, optional): The consumer group to be used. Default is \"mistral\".\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.consumer_group = consumer_group\n",
    "\n",
    "        if self.api_key:\n",
    "            self.client = OpenAI(api_key=self.api_key)  # Set up the OpenAI client with the provided API key\n",
    "        else:\n",
    "            print('Coming Soon')  # Placeholder message for when API key is not provided\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches to a different model and device for the LLM. Creates a new reader if necessary.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        current_readers = self.takeoff_client.get_readers()  # Retrieve the current readers from the takeoff client\n",
    "\n",
    "        # Check if a reader for the desired model already exists\n",
    "        reader_id = None\n",
    "        for group, readers in current_readers.items():\n",
    "            for reader in readers:\n",
    "                if reader['model_name'] == model_name:  # Check if the desired model is already in use\n",
    "                    reader_id = reader['reader_id']\n",
    "                    break\n",
    "            if reader_id:\n",
    "                break\n",
    "\n",
    "        if reader_id:\n",
    "            print(f\"Reader for model '{model_name}' already exists with reader_id: {reader_id}\")\n",
    "        else:\n",
    "            reader_config = {\n",
    "                \"model_name\": model_name,\n",
    "                \"device\": device,\n",
    "                \"consumer_group\": self.consumer_group\n",
    "            }\n",
    "\n",
    "            reader_id, _ = self.takeoff_client.create_reader(reader_config=reader_config)  # Create a new reader\n",
    "            print(f\"Created a new reader with reader_id {reader_id}\")\n",
    "\n",
    "    def answer_query(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates an answer to a query based on the provided context using the LLM.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            context (str): The context to be used for answering the query.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated answer to the query.\n",
    "        \"\"\"\n",
    "        if context:\n",
    "            prompt = f\"Based on the provided context, answer the following query: {query}\\n\\nContext:\\n{context}. Do not use your knowledge, only the context\"\n",
    "        else:\n",
    "            prompt = f\"Answer the following query: {query}\"\n",
    "        \n",
    "        if self.api_key:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "                model=\"gpt-3.5-turbo\",  # Specify the model to be used for generating the response\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content.strip()  # Return the generated response\n",
    "        else:\n",
    "            response = self.takeoff_client.generate(prompt, consumer_group=self.consumer_group)  # Generate response using the takeoff client\n",
    "            if 'text' in response:\n",
    "                return response['text'].strip()  # Return the generated response\n",
    "            else:\n",
    "                print(f\"Error generating response: {response}\")\n",
    "                return \"Unable to generate a response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(queries: List[str], data_source: DataSource, canonical_data: DataSource, retrieval_and_ranking: RetrievalAndRanking, retrieval_and_ranking_canonical: RetrievalAndRanking, llm: LLM, retrieval_method: str = \"default\", threshold: float = 0.5) -> List[str]:\n",
    "    user_queries = [UserQuery(query) for query in queries]\n",
    "    answers = []\n",
    "\n",
    "    for user_query in user_queries:\n",
    "        print(f\"Processing query: {user_query.query}\")\n",
    "        query_embedding = retrieval_and_ranking_canonical.embedding.embed(user_query.query)\n",
    "\n",
    "        # Retrieve the closest canonical form\n",
    "        results = retrieval_and_ranking_canonical.vector_storage.search_vectors(query_embedding, top_n=1)\n",
    "\n",
    "        if results:\n",
    "            closest_uterance = results[0]['metadata']['text'] \n",
    "            closest_uterance_embedding = retrieval_and_ranking_canonical.embedding.embed(closest_uterance)\n",
    "            distance = np.linalg.norm(np.array(query_embedding) - np.array(closest_uterance_embedding))\n",
    "            print(\"Distance:\", distance)\n",
    "\n",
    "            \n",
    "            if distance < threshold:\n",
    "                print(\"Query vector match the canonical form criteria, RAG architecture in use\")\n",
    "                if retrieval_method == \"default\":\n",
    "                    relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks(user_query.query)\n",
    "                elif retrieval_method == \"euclidean\":\n",
    "                    relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_euclidean(user_query.query)\n",
    "                elif retrieval_method == \"pinecone\":\n",
    "                    relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_pinecone([user_query.query])\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown retrieval method: {retrieval_method}\")\n",
    "                \n",
    "                context = \"\\n\".join(\"\\n\".join(chunks) for chunks in relevant_chunks)\n",
    "                answer = llm.answer_query(user_query.query, context)\n",
    "            else:\n",
    "                print(\"Query vector doesn't match the canonical form criteria, RAG arcitecture not in use\")\n",
    "                answer = llm.answer_query(user_query.query, \"\")\n",
    "        else:\n",
    "            print(\"No anonical form detected, RAG arcitecture not in use\")\n",
    "            answer = llm.answer_query(user_query.query, \"\")\n",
    "\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_source:DataSource,\n",
    "         canonical_data:DataSource,\n",
    "         retrieval_method: str = \"default\",\n",
    "         model_choice: str = \"openai\",\n",
    "         model_name: str = None,\n",
    "         device: str = \"cpu\",\n",
    "         embedding_model_name = 'all-MiniLM-L6-v2',\n",
    "         use_local=False,\n",
    "         index_name=\"my-index\",\n",
    "         canonical_index_name=\"canonical-index\",\n",
    "         threshold = 1.0\n",
    "         ):\n",
    "    \"\"\"\n",
    "    Main function for running the retrieval and ranking system with user interaction.\n",
    "\n",
    "    Args:\n",
    "        data_source (DataSource): An instance of the DataSource class providing the data.\n",
    "        retrieval_method (str, optional): The method to use for retrieval and ranking. Defaults to \"default\".\n",
    "        model_choice (str, optional): The choice of model for language understanding. Defaults to \"openai\".\n",
    "        model_name (str, optional): The name of the model to be used. Defaults to None.\n",
    "        device (str, optional): The device to be used for embedding. Defaults to \"cpu\".\n",
    "        embedding_model_name (str, optional): The name of the embedding model to be used. Defaults to 'all-MiniLM-L6-v2'.\n",
    "        use_local (bool, optional): Whether to use a local model for embedding. Defaults to False.\n",
    "        index_name (str, optional): The name of the index for vector storage. Defaults to \"my-index\".\n",
    "    \"\"\"\n",
    "    embedding = Embedding(model_name=embedding_model_name,\n",
    "                          device=device,\n",
    "                          use_local=use_local)\n",
    "    vector_storage = PineconeVectorStorage(index_name, embedding)\n",
    "    canonical_vector_storage = PineconeVectorStorage(canonical_index_name, embedding)\n",
    "    processed_data = data_source.process_data()\n",
    "    processed_data_canonical = canonical_data.process_data()\n",
    "    metadatas = [{'text': text, 'category': 'finance'} for text in processed_data]\n",
    "    canonical_metadatas = [{'text': form} for form in processed_data_canonical]\n",
    "    vectors = [embedding.embed(text) for text in processed_data]\n",
    "    canonical_vectors = [embedding.embed(text) for text in processed_data_canonical]\n",
    "    vector_storage.store_vectors(vectors, metadatas)\n",
    "    canonical_vector_storage.store_vectors(canonical_vectors, canonical_metadatas)    \n",
    "\n",
    "    retrieval_and_ranking = RetrievalAndRanking(data_source, embedding, vector_storage)\n",
    "    retrieval_and_ranking_canonical = RetrievalAndRanking(canonical_data, embedding, canonical_vector_storage)\n",
    "\n",
    "    if model_choice == \"openai\":\n",
    "        llm = LLM(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Not yet implemented\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        queries = user_input.split(\";\")  # Split multiple queries separated by semicolon\n",
    "        answers = process_query(queries, data_source, canonical_data, retrieval_and_ranking, retrieval_and_ranking_canonical, llm, retrieval_method, threshold)\n",
    "\n",
    "        print(\"User Queries:\")\n",
    "        for query, answer in zip(queries, answers):\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: Integration of Llama 2 with natural language processing tools\n",
      "Distance: 0.7936999315973329\n",
      "Query vector match the canonical form criteria, RAG architecture in use\n",
      "Processing query:  Security and privacy in the use of Llama 2\n",
      "Distance: 0.6725523043726964\n",
      "Query vector match the canonical form criteria, RAG architecture in use\n",
      "Processing query:  Easy dinner recipes\n",
      "Distance: 1.269933893666347\n",
      "Query vector doesn't match the canonical form criteria, RAG arcitecture not in use\n",
      "Processing query:  How to keep indoor plants healthy\n",
      "Distance: 1.3323015297080252\n",
      "Query vector doesn't match the canonical form criteria, RAG arcitecture not in use\n",
      "User Queries:\n",
      "Query: Integration of Llama 2 with natural language processing tools\n",
      "Answer: Based on the context provided, there is no specific mention of the integration of Llama 2 with natural language processing tools. The context primarily discusses the evaluation of large language models trained by a group of individuals associated with different research areas and projects. It does not provide information on the integration of Llama 2 with natural language processing tools.\n",
      "\n",
      "Query:  Security and privacy in the use of Llama 2\n",
      "Answer: In the given context, there is no specific information provided about Llama 2, security, or privacy concerns related to its use. Therefore, it is not possible to determine the specific security and privacy aspects associated with Llama 2 without further details or a background on the subject.\n",
      "\n",
      "Query:  Easy dinner recipes\n",
      "Answer: Here are a few easy dinner recipes that you can try:\n",
      "\n",
      "1. One-Pan Baked Lemon Butter Garlic Salmon: Place salmon fillets on a baking sheet, season with salt, pepper, garlic powder, and drizzle with melted butter and lemon juice. Bake at 400Â°F for 15-20 minutes.\n",
      "\n",
      "2. Sheet Pan Chicken Fajitas: Marinate chicken strips in fajita seasoning and lime juice. Spread on a baking sheet with sliced bell peppers and onions. Bake at 400Â°F for 20-25 minutes.\n",
      "\n",
      "3. Creamy Pesto Pasta with Cherry Tomatoes: Cook pasta according to package instructions. Toss with store-bought pesto, cherry tomatoes, and a splash of cream. Top with grated Parmesan cheese.\n",
      "\n",
      "4. BBQ Pulled Pork Sandwiches: Slow-cook pork shoulder in a crockpot with BBQ sauce and spices until tender. Shred the meat and serve on buns with coleslaw.\n",
      "\n",
      "5. Greek Chicken Gyros: Marinate chicken thighs in a mixture of lemon juice, olive oil, garlic, oregano, and yogurt. Grill or pan-fry until cooked through. Serve in pita bread with tzatziki sauce, sliced cucumbers, and tomatoes.\n",
      "\n",
      "These are just a few ideas to get you started. Feel free to customize the recipes to suit your preferences and dietary restrictions. Happy cooking!\n",
      "\n",
      "Query:  How to keep indoor plants healthy\n",
      "Answer: To keep indoor plants healthy, here are some tips you can follow:\n",
      "\n",
      "1. **Light**: Make sure your plants are placed in an area where they receive adequate sunlight according to their specific light requirements. If natural light is limited, consider using artificial grow lights.\n",
      "\n",
      "2. **Watering**: Be mindful not to overwater or underwater your plants. Allow the soil to dry out slightly between waterings, and always check the plant's specific watering needs.\n",
      "\n",
      "3. **Humidity**: Indoor environments can have low humidity, which may not be ideal for some plants. Increase humidity by misting your plants, using a humidifier, or placing a tray of water near them.\n",
      "\n",
      "4. **Temperature**: Most indoor plants thrive in temperatures between 65-75 degrees Fahrenheit. Avoid placing them near drafty windows, heating or cooling vents.\n",
      "\n",
      "5. **Fertilization**: Use a balanced fertilizer to provide essential nutrients to your plants during their growing season. Follow the instructions on the fertilizer packaging for best results.\n",
      "\n",
      "6. **Pruning**: Regularly prune dead or yellowing leaves, as well as trim back overgrown branches to encourage healthy growth.\n",
      "\n",
      "7. **Pest Control**: Keep an eye out for common pests like spider mites, fungus gnats, and mealybugs. Treat any infestations promptly using natural or chemical pest control methods.\n",
      "\n",
      "8. **Repotting**: As your plants grow, repot them into slightly larger containers with fresh potting soil. This ensures they have enough space for root growth.\n",
      "\n",
      "9. **Cleaning**: Dust your plant's leaves regularly to allow them to absorb more sunlight and prevent pests from hiding in the foliage.\n",
      "\n",
      "By following these general guidelines and being attentive to the specific needs of your individual plants, you can help ensure they stay healthy and thrive in your indoor space.\n",
      "\n",
      "Processing query: Best practices for fine-tuning Llama 2\n",
      "Distance: 0.7124687641387859\n",
      "Query vector match the canonical form criteria, RAG architecture in use\n",
      "User Queries:\n",
      "Query: Best practices for fine-tuning Llama 2\n",
      "Answer: Based on the provided context, best practices for fine-tuning Llama 2 would include:\n",
      "\n",
      "1. Aim for an overall win rate of more than the equivalent of an equally sized Vicuna B model when fine-tuning.\n",
      "2. Perform thorough evaluation results, including human evaluations, to compare the performance of the fine-tuned Llama 2 models against open and closed source models.\n",
      "3. Make sure to have at least three raters per prompt for the human evaluation results to ensure reliability.\n",
      "4. Ensure that the largest Llama 2 model, especially the two-tab old style, has competitive performance with ChatGPT.\n",
      "5. Implement fine-tuning techniques that lead to improvement in the model's performance, potentially increasing the win rate.\n",
      "6. Encourage community involvement to contribute to responsible development of the Llama 2 model.\n",
      "7. Collaborate with other researchers by providing equal contributions and corresponding authorship when working on Llama 2-related projects.\n",
      "\n",
      "CPU times: user 11min 39s, sys: 1min 43s, total: 13min 23s\n",
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_source = DataSource(data)\n",
    "canonical_data = DataSource(canonical_form_llama2)\n",
    "data_source.process_data()\n",
    "canonical_data.process_data()\n",
    "main(data_source, canonical_data, retrieval_method='pinecone', index_name=\"my-index4\", embedding_model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Llama 2 in enterprise applications; Advantages of Llama 2 over other language models; Tutorial for training Llama 2 from scratch; Comparison between Llama 2 and GPT-4; Use cases of Llama 2 in customer service; Best practices for fine-tuning Llama 2; Performance of Llama 2 in creative text generation tasks; Hardware requirements to run Llama 2; Integration of Llama 2 with natural language processing tools; Security and privacy in the use of Llama 2; Easy dinner recipes; How to keep indoor plants healthy; Benefits of daily exercise; Best tourist destinations for 2024; Small space decoration ideas; Tips for saving money on weekly shopping; Effective study techniques; Recommended books to read in summer; How to organize a family event; Strategies to improve productivity at work\n",
    "\n",
    "Use cases of Llama 2 in customer service; Best practices for fine-tuning Llama 2; Tips for saving money on weekly shopping; Effective study techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_FP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
