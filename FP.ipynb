{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosmayorga/anaconda3/envs/AI_FP/lib/python3.8/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = \"5c83f505-ce5d-4790-ad9f-3ab2a148fbd8\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"Carbon trading is a market-based approach to reducing greenhouse gas emissions by providing economic incentives for companies to limit their carbon footprint.\",\n",
    "    \"In a carbon trading system, companies are allocated a certain number of carbon credits, which represent the right to emit a specific amount of carbon dioxide or other greenhouse gases.\",\n",
    "    \"Companies that emit less than their allocated carbon credits can sell their excess credits to companies that exceed their emissions limits, creating a market for carbon credits.\",\n",
    "    \"The goal of carbon trading is to encourage companies to invest in cleaner technologies and adopt more sustainable practices to reduce their emissions and avoid the cost of purchasing additional carbon credits.\",\n",
    "    \"Environmental, Social, and Governance (ESG) criteria are a set of standards used by investors to evaluate a company's sustainability and ethical impact.\",\n",
    "    \"ESG factors consider a company's environmental impact, such as its carbon footprint, waste management, and use of renewable energy.\",\n",
    "    \"Social aspects of ESG include a company's labor practices, diversity and inclusion policies, and community engagement.\",\n",
    "    \"Governance factors in ESG assess a company's leadership structure, executive compensation, and transparency in decision-making processes.\",\n",
    "    \"Investors are increasingly using ESG criteria to identify companies that are better positioned to manage risks and opportunities related to sustainability and social responsibility.\",\n",
    "    \"Companies with strong ESG performance tend to have better long-term financial prospects, as they are more resilient to environmental and social challenges and are favored by environmentally and socially conscious consumers.\",\n",
    "    \"Carbon trading and ESG are closely related, as companies with lower carbon emissions and better sustainability practices tend to have higher ESG ratings.\",\n",
    "    \"Governments and international organizations are promoting carbon trading and ESG investing as key strategies for mitigating climate change and transitioning to a low-carbon economy.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_form_llama2 = [\n",
    "    \"What is LLaMA 2?\",\n",
    "    \"Who developed LLaMA 2?\",\n",
    "    \"What are the key features of LLaMA 2?\",\n",
    "    \"How does LLaMA 2 differ from its predecessor, LLaMA?\",\n",
    "    \"What are the applications of LLaMA 2?\",\n",
    "    \"What industries benefit most from using LLaMA 2?\",\n",
    "    \"What are the limitations of LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle multi-language support?\",\n",
    "    \"What datasets were used to train LLaMA 2?\",\n",
    "    \"Is LLaMA 2 open-source?\",\n",
    "    \"What is the architecture of LLaMA 2?\",\n",
    "    \"How many parameters does LLaMA 2 have?\",\n",
    "    \"What kind of data is LLaMA 2 trained on?\",\n",
    "    \"What are the hardware requirements for running LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle contextual understanding?\",\n",
    "    \"What optimization techniques are used in LLaMA 2?\",\n",
    "    \"How does LLaMA 2 compare to GPT-4 in terms of performance?\",\n",
    "    \"What is the training process of LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle long-form content generation?\",\n",
    "    \"What are the memory and processing requirements for LLaMA 2?\",\n",
    "    \"How to integrate LLaMA 2 into existing applications?\",\n",
    "    \"What programming languages are compatible with LLaMA 2?\",\n",
    "    \"How to fine-tune LLaMA 2 for specific tasks?\",\n",
    "    \"What are the best practices for deploying LLaMA 2 in production?\",\n",
    "    \"How to use LLaMA 2 for natural language understanding (NLU)?\",\n",
    "    \"How to implement LLaMA 2 in a chatbot?\",\n",
    "    \"What are the API options available for LLaMA 2?\",\n",
    "    \"How to handle data privacy and security when using LLaMA 2?\",\n",
    "    \"What are some common use cases of LLaMA 2 in customer service?\",\n",
    "    \"How to monitor and maintain LLaMA 2 performance over time?\",\n",
    "    \"How does LLaMA 2 compare to other language models like GPT-4 and BERT?\",\n",
    "    \"What benchmarks are used to evaluate LLaMA 2?\",\n",
    "    \"What are the strengths of LLaMA 2 over other models?\",\n",
    "    \"What are the weaknesses of LLaMA 2 compared to its competitors?\",\n",
    "    \"How does LLaMA 2 perform in specific tasks like summarization and translation?\",\n",
    "    \"What are user reviews and feedback on LLaMA 2?\",\n",
    "    \"How does LLaMA 2 handle bias and ethical concerns?\",\n",
    "    \"What are the ethical implications of using LLaMA 2?\",\n",
    "    \"How to evaluate the output quality of LLaMA 2?\",\n",
    "    \"What are the performance metrics for LLaMA 2?\",\n",
    "    \"How to train a custom version of LLaMA 2 from scratch?\",\n",
    "    \"How to leverage transfer learning with LLaMA 2?\",\n",
    "    \"What are the latest research advancements related to LLaMA 2?\",\n",
    "    \"How to improve LLaMA 2's performance for specific domains?\",\n",
    "    \"What are some advanced techniques for optimizing LLaMA 2?\",\n",
    "    \"How to use LLaMA 2 for generating code?\",\n",
    "    \"How does LLaMA 2 handle rare and low-frequency words?\",\n",
    "    \"What are some innovative applications of LLaMA 2 in creative industries?\",\n",
    "    \"How to ensure the ethical use of LLaMA 2 in sensitive applications?\",\n",
    "    \"What future developments can be expected for LLaMA 2?\",\n",
    "    \"Where to find tutorials and guides on LLaMA 2?\",\n",
    "    \"What are some popular forums and communities discussing LLaMA 2?\",\n",
    "    \"How to contribute to the development of LLaMA 2?\",\n",
    "    \"What are some notable projects using LLaMA 2?\",\n",
    "    \"How to get support for LLaMA 2-related issues?\",\n",
    "    \"Are there any workshops or webinars on LLaMA 2?\",\n",
    "    \"What are the best books or research papers on LLaMA 2?\",\n",
    "    \"How to stay updated with the latest news on LLaMA 2?\",\n",
    "    \"What are some challenges faced by the LLaMA 2 user community?\",\n",
    "    \"How to network with other professionals using LLaMA 2?\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    def __init__(self, data: List[str]):\n",
    "        \"\"\"\n",
    "        Initializes the DataSource object with a list of data.\n",
    "\n",
    "        Args:\n",
    "            data (List[str]): The input list of text data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, and removing extra whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "        return text\n",
    "\n",
    "    def preprocess_text_advanced(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, removing extra whitespace,\n",
    "        removing numbers, removing stop words, and lemmatizing the words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed and cleaned text.\n",
    "        \"\"\"\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower() \n",
    "        # Remove punctuation and numeric characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Lemmatize the words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        # Join the tokens back into a string\n",
    "        text = ' '.join(tokens)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the preprocessed text into a list of words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The preprocessed text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of tokens (words) from the text.\n",
    "        \"\"\"\n",
    "        return self.preprocess_text_advanced(text).split()  # Tokenize the preprocessed text by splitting on spaces\n",
    "\n",
    "    def process_data(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Processes the data by applying advanced preprocessing to each sentence in the data list.\n",
    "\n",
    "        Updates the object's processed_data attribute with the cleaned and preprocessed text.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.processed_data = [self.preprocess_text_advanced(sentence) for sentence in self.data] # Apply preprocessing to each sentence in the data list\n",
    "\n",
    "        return self.processed_data  # Apply preprocessing to each sentence in the data list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserQuery:\n",
    "    def __init__(self, query: str):\n",
    "        self.query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, model_name: str, device: str = 'cpu', use_local: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the Embedding object with the specified model name, device, and whether to use a local model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the embedding model to be used.\n",
    "            device (str): The device to be used for running the model (\"cpu\" or \"cuda\"). Default is \"cpu\".\n",
    "            use_local (bool): Whether to use a locally available model. Default is False.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.use_local = use_local\n",
    "        # Mapping of model names to their expected dimensions\n",
    "        self.model_dimensions = {\n",
    "            'all-MiniLM-L6-v2': 384,  # Example dimension for a SentenceTransformer model\n",
    "            'jina-v2-base-en-embed': 768,  # Specified dimension for your local model\n",
    "            # Add other models and their dimensions here\n",
    "        }\n",
    "        self.current_model_dimension = self.model_dimensions.get(model_name, None)\n",
    "\n",
    "        if use_local:\n",
    "            raise ValueError(f\"Not yet implemented\")\n",
    "        else:\n",
    "            self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches the embedding model to a different model and device.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new embedding model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        # Here we assume the model switch is successful and just set the dimension\n",
    "        if model_name in self.model_dimensions:\n",
    "            self.current_model_dimension = self.model_dimensions[model_name]\n",
    "            print(f\"Switched to model '{model_name}' with dimension {self.current_model_dimension}\")\n",
    "        else:\n",
    "            print(f\"Model '{model_name}' not recognized. Unable to switch models.\")\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Embeds the input text into a vector representation using the current embedding model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: The vector representation of the input text.\n",
    "        \"\"\"\n",
    "        if self.use_local:\n",
    "          raise ValueError(f\"Not yet implemented!\")\n",
    "        else:\n",
    "            result = self.model.encode(text).tolist()\n",
    "            if len(result) != self.current_model_dimension:\n",
    "                print(f\"Dimension mismatch detected: Expected {self.current_model_dimension}, got {len(result)}\")\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStorage:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]]):\n",
    "        # Placeholder method for storing vectors\n",
    "        pass\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int) -> List[int]:\n",
    "        # Placeholder method for searching vectors\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeVectorStorage(VectorStorage):\n",
    "    def __init__(self, index_name: str, embedding: Embedding):\n",
    "        \"\"\"\n",
    "        Initializes the PineconeVectorStorage object with the specified index name and embedding model.\n",
    "\n",
    "        Args:\n",
    "            index_name (str): The name of the Pinecone index to be created or used.\n",
    "            embedding (Embedding): An instance of the Embedding class providing the embedding model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "        self.pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        if index_name not in self.pinecone.list_indexes().names():\n",
    "            self.pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=embedding.model.get_sentence_embedding_dimension(),\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "            )\n",
    "        self.index = self.pinecone.Index(index_name)\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]], metadatas: List[dict]):\n",
    "        \"\"\"\n",
    "        Stores vectors and associated metadata in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            vectors (List[List[float]]): List of vectors to be stored in the index.\n",
    "            metadatas (List[dict]): List of dictionaries containing metadata associated with each vector.\n",
    "        \"\"\"\n",
    "        ids = [str(i) for i in range(len(vectors))]\n",
    "        records = zip(ids, vectors, metadatas)\n",
    "        self.index.upsert(vectors=records)\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int, filter_metadata: Dict[str, str] = None) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Searches for vectors similar to the given query vector in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            query_vector (List[float]): The query vector for similarity search.\n",
    "            top_n (int): The number of top results to return.\n",
    "            filter_metadata (Dict[str, str], optional): Metadata filters to apply during search. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: List of dictionaries containing metadata of the top similar vectors.\n",
    "        \"\"\"\n",
    "        query_params = {\n",
    "            'top_k': top_n,\n",
    "            'vector': query_vector,\n",
    "            'include_metadata': True,\n",
    "            'include_values': False\n",
    "        }\n",
    "        if filter_metadata is not None:\n",
    "            query_params['metadata'] = filter_metadata\n",
    "\n",
    "        results = self.index.query(**query_params)\n",
    "        return results['matches']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAndRanking:\n",
    "    def __init__(self, data_source: DataSource, embedding: Embedding, vector_storage: VectorStorage):\n",
    "        \"\"\"\n",
    "        Initializes the RetrievalAndRanking object with a data source, embedding model, and vector storage.\n",
    "\n",
    "        Args:\n",
    "            data_source (DataSource): The data source containing the preprocessed text data.\n",
    "            embedding (Embedding): The embedding model used to generate vector representations of text.\n",
    "            vector_storage (VectorStorage): The storage for vector representations of text data.\n",
    "        \"\"\"\n",
    "        self.data_source = data_source\n",
    "        self.embedding = embedding\n",
    "        self.vector_storage = vector_storage\n",
    "\n",
    "    def retrieve_relevant_chunks(self, query: str, top_n: int = 2) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Jaccard similarity.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            top_n (int): The number of top relevant chunks to retrieve (default is 2).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of top relevant chunks from the data source.\n",
    "        \"\"\"\n",
    "        # Tokenize and convert query to a set of tokens\n",
    "        query_tokens = set(self.data_source.tokenize(query)) \n",
    "        # Initialize a list to store chunks and their similarity scores\n",
    "        similarities: List[Tuple[str, float]] = []  \n",
    "\n",
    "        for chunk in self.data_source.processed_data:\n",
    "            # Tokenize and convert chunk to a set of tokens\n",
    "            chunk_tokens = set(self.data_source.tokenize(chunk))\n",
    "            # Calculate Jaccard similarity between query and chunk\n",
    "            similarity = len(query_tokens.intersection(chunk_tokens)) / len(\n",
    "                query_tokens.union(chunk_tokens)\n",
    "            )\n",
    "            # Append the chunk and its similarity score to the list\n",
    "            similarities.append((chunk, similarity))\n",
    "\n",
    "        # Sort the chunks by similarity score in descending order\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the top-n most relevant chunks\n",
    "        return [chunk for chunk, _ in similarities[:top_n]]\n",
    "\n",
    "    def retrieve_relevant_chunks_euclidean(self, query: str, top_n: int = 2) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Euclidean distance on TF-IDF vectors.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            top_n (int): The number of top relevant chunks to retrieve (default is 2).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of top relevant chunks from the data source.\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer()  # Create a TF-IDF vectorizer\n",
    "        # Tokenize each string in the data source and join the tokens back into strings\n",
    "        tokenized_data_source = [' '.join(self.data_source.tokenize(text)) for text in self.data_source.processed_data]\n",
    "        # Fit and transform the tokenized data source into a TF-IDF matrix\n",
    "        tfidf_matrix = vectorizer.fit_transform(tokenized_data_source)\n",
    "        # Tokenize the query and join the tokens back into a string\n",
    "        tokenized_query = ' '.join(self.data_source.tokenize(query))\n",
    "        # Transform the tokenized query into a TF-IDF vector\n",
    "        query_vector = vectorizer.transform([tokenized_query])\n",
    "        # Calculate the Euclidean distance between the query vector and each chunk vector\n",
    "        similarities = euclidean_distances(query_vector, tfidf_matrix).flatten()\n",
    "        # Get the indices of the top-n closest chunks\n",
    "        top_indices = similarities.argsort()[:top_n]\n",
    "\n",
    "        # Return the top-n closest chunks from the data source\n",
    "        return [self.data_source.processed_data[i] for i in top_indices]\n",
    "\n",
    "    def retrieve_relevant_chunks_pinecone(self, queries: List[str], top_n: int = 2, filter_metadata: Dict[str, str] = None) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Pinecone vector storage.\n",
    "\n",
    "        Args:\n",
    "            queries (List[str]): A list of user queries.\n",
    "            top_n (int): The number of top relevant chunks to retrieve for each query (default is 2).\n",
    "            filter_metadata (Dict[str, str], optional): A dictionary containing metadata key-value pairs to filter the results (default is None).\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A list containing the top-n most relevant chunks for each query.\n",
    "        \"\"\"\n",
    "        relevant_chunks = []\n",
    "\n",
    "        for query in queries:\n",
    "            query_embedding = self.embedding.embed(query)\n",
    "            results = self.vector_storage.search_vectors(query_embedding, top_n)\n",
    "\n",
    "            filtered_results = []\n",
    "            if filter_metadata is not None:\n",
    "                for result in results:\n",
    "                    metadata = result.get('metadata', {})\n",
    "                    if all(metadata.get(key) == value for key, value in filter_metadata.items()):\n",
    "                        filtered_results.append(result)\n",
    "            else:\n",
    "                filtered_results = results\n",
    "\n",
    "            relevant_chunks.append([result['metadata']['text'] for result in filtered_results])\n",
    "\n",
    "        return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, api_key: str = None, model_name: str = None, device: str = \"cuda\", consumer_group: str = \"mistral\"):\n",
    "        \"\"\"\n",
    "        Initializes the LLM object with the specified API key, model name, device, and consumer group.\n",
    "        Sets up the OpenAI client if an API key is provided.\n",
    "\n",
    "        Args:\n",
    "            api_key (str, optional): The API key for accessing the OpenAI service. Default is None.\n",
    "            model_name (str, optional): The name of the model to be used. Default is None.\n",
    "            device (str, optional): The device to be used for running the model (\"cuda\" or \"cpu\"). Default is \"cuda\".\n",
    "            consumer_group (str, optional): The consumer group to be used. Default is \"mistral\".\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.consumer_group = consumer_group\n",
    "\n",
    "        if self.api_key:\n",
    "            self.client = OpenAI(api_key=self.api_key)  # Set up the OpenAI client with the provided API key\n",
    "        else:\n",
    "            print('Coming Soon')  # Placeholder message for when API key is not provided\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches to a different model and device for the LLM. Creates a new reader if necessary.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        current_readers = self.takeoff_client.get_readers()  # Retrieve the current readers from the takeoff client\n",
    "\n",
    "        # Check if a reader for the desired model already exists\n",
    "        reader_id = None\n",
    "        for group, readers in current_readers.items():\n",
    "            for reader in readers:\n",
    "                if reader['model_name'] == model_name:  # Check if the desired model is already in use\n",
    "                    reader_id = reader['reader_id']\n",
    "                    break\n",
    "            if reader_id:\n",
    "                break\n",
    "\n",
    "        if reader_id:\n",
    "            print(f\"Reader for model '{model_name}' already exists with reader_id: {reader_id}\")\n",
    "        else:\n",
    "            reader_config = {\n",
    "                \"model_name\": model_name,\n",
    "                \"device\": device,\n",
    "                \"consumer_group\": self.consumer_group\n",
    "            }\n",
    "\n",
    "            reader_id, _ = self.takeoff_client.create_reader(reader_config=reader_config)  # Create a new reader\n",
    "            print(f\"Created a new reader with reader_id {reader_id}\")\n",
    "\n",
    "    def answer_query(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates an answer to a query based on the provided context using the LLM.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            context (str): The context to be used for answering the query.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated answer to the query.\n",
    "        \"\"\"\n",
    "        prompt = f\"Based on the provided context, answer the following query: {query}\\n\\nContext:\\n{context}. Do not use your knowledge, only the context\"\n",
    "        \n",
    "        if self.api_key:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "                model=\"gpt-3.5-turbo\",  # Specify the model to be used for generating the response\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content.strip()  # Return the generated response\n",
    "        else:\n",
    "            response = self.takeoff_client.generate(prompt, consumer_group=self.consumer_group)  # Generate response using the takeoff client\n",
    "            if 'text' in response:\n",
    "                return response['text'].strip()  # Return the generated response\n",
    "            else:\n",
    "                print(f\"Error generating response: {response}\")\n",
    "                return \"Unable to generate a response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(queries: List[str], data_source: DataSource, retrieval_and_ranking: RetrievalAndRanking, llm: LLM, retrieval_method: str = \"default\") -> List[str]:\n",
    "    user_queries = [UserQuery(query) for query in queries]\n",
    "    answers = []\n",
    "\n",
    "    for user_query in user_queries:\n",
    "        if retrieval_method == \"default\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks(user_query.query)\n",
    "            context = \"\\n\".join(relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        elif retrieval_method == \"euclidean\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_euclidean(user_query.query)\n",
    "            context = \"\\n\".join(relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        elif retrieval_method == \"pinecone\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_pinecone([user_query.query], filter_metadata={'category': 'finance'})\n",
    "            answer = \"\\n\".join(\"\\n\".join(chunks) for chunks in relevant_chunks)  # Join each list of chunks separately\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown retrieval method: {retrieval_method}\")\n",
    "\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_source:DataSource,\n",
    "         retrieval_method: str = \"default\",\n",
    "         model_choice: str = \"openai\",\n",
    "         model_name: str = None,\n",
    "         device: str = \"cpu\",\n",
    "         embedding_model_name = 'all-MiniLM-L6-v2',\n",
    "         use_local=False,\n",
    "         index_name=\"my-index\"\n",
    "         ):\n",
    "    \"\"\"\n",
    "    Main function for running the retrieval and ranking system with user interaction.\n",
    "\n",
    "    Args:\n",
    "        data_source (DataSource): An instance of the DataSource class providing the data.\n",
    "        retrieval_method (str, optional): The method to use for retrieval and ranking. Defaults to \"default\".\n",
    "        model_choice (str, optional): The choice of model for language understanding. Defaults to \"openai\".\n",
    "        model_name (str, optional): The name of the model to be used. Defaults to None.\n",
    "        device (str, optional): The device to be used for embedding. Defaults to \"cpu\".\n",
    "        embedding_model_name (str, optional): The name of the embedding model to be used. Defaults to 'all-MiniLM-L6-v2'.\n",
    "        use_local (bool, optional): Whether to use a local model for embedding. Defaults to False.\n",
    "        index_name (str, optional): The name of the index for vector storage. Defaults to \"my-index\".\n",
    "    \"\"\"\n",
    "    embedding = Embedding(model_name=embedding_model_name,\n",
    "                          device=device,\n",
    "                          use_local=use_local)\n",
    "    vector_storage = PineconeVectorStorage(index_name, embedding)\n",
    "    processed_data = data_source.process_data()\n",
    "    metadatas = [{'text': text, 'category': 'finance'} for text in processed_data]\n",
    "    vectors = [embedding.embed(text) for text in processed_data]\n",
    "    vector_storage.store_vectors(vectors, metadatas)\n",
    "\n",
    "    retrieval_and_ranking = RetrievalAndRanking(data_source, embedding, vector_storage)\n",
    "\n",
    "    if model_choice == \"openai\":\n",
    "        llm = LLM(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Not yet implemented\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        queries = user_input.split(\";\")  # Split multiple queries separated by semicolon\n",
    "        answers = process_query(queries, data_source, retrieval_and_ranking, llm, retrieval_method)\n",
    "\n",
    "        print(\"User Queries:\")\n",
    "        for query, answer in zip(queries, answers):\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Queries:\n",
      "Query: What is the purpose of carbon trading?\n",
      "Answer: goal carbon trading encourage company invest cleaner technology adopt sustainable practice reduce emission avoid cost purchasing additional carbon credit\n",
      "carbon trading marketbased approach reducing greenhouse gas emission providing economic incentive company limit carbon footprint\n",
      "\n",
      "Query:  How do ESG criteria evaluate a company's environmental impact?\n",
      "Answer: esg factor consider company environmental impact carbon footprint waste management use renewable energy\n",
      "carbon trading esg closely related company lower carbon emission better sustainability practice tend higher esg rating\n",
      "\n",
      "Query:  What is the recipe for making a perfect pizza?\n",
      "Answer: governance factor esg ass company leadership structure executive compensation transparency decisionmaking process\n",
      "government international organization promoting carbon trading esg investing key strategy mitigating climate change transitioning lowcarbon economy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_source = DataSource(data)\n",
    "data_source.process_data()\n",
    "main(data_source, retrieval_method='pinecone', index_name=\"my-index4\", embedding_model_name=\"all-MiniLM-L6-v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_FP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
