{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "import pinecone\n",
    "from datasets import load_dataset\n",
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'uid'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda x: {\n",
    "    'uid': f\"{x['doi']}-{x['chunk-id']}\"\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
       " 'January 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but\\nrather learned in a supervised way. Our deep hierarchical architectures achieve the best\\npublished results on benchmarks for object classi\\x0ccation (NORB, CIFAR10) and handwritten\\ndigit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep\\nnets trained by simple back-propagation perform better than more shallow ones. Learning\\nis surprisingly rapid. NORB is completely trained within \\x0cve epochs. Test error rates on\\nMNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.\\n1 Introduction\\nThe human visual system e\\x0eciently recognizes and localizes objects within cluttered scenes. For\\narti\\x0ccial systems, however, this is still di\\x0ecult, due to viewpoint-dependent object variability,\\nand the high in-class variability of many object types. Deep hierarchical neural models roughly\\nmimick the nature of mammalian visual cortex, and by community consensus are among the most\\npromising architectures for such tasks. The most successful hierarchical object recognition systems\\nall extract localized features from input images, convolving image patches with \\x0clters. Filter',\n",
       " 'promising architectures for such tasks. The most successful hierarchical object recognition systems\\nall extract localized features from input images, convolving image patches with \\x0clters. Filter\\nresponses are then repeatedly sub-sampled and re-\\x0cltered, resulting in a deep feed-forward network\\narchitecture whose output feature vectors are eventually classi\\x0ced. One of the \\x0crst hierarchical\\nneural systems was the Neocognitron (Fukushima, 1980) which inspired many of the more recent\\nvariants.\\nUnsupervised learning methods applied to patches of natural images tend to produce localized\\n\\x0clters that resemble o\\x0b-center-on-surround \\x0clters, orientation-sensitive bar detectors, Gabor \\x0clters\\n(Schmidhuber et al. , 1996; Olshausen and Field, 1997; Hoyer and Hyv\\x7f arinen, 2000). These \\x0cndings\\nin conjunction with experimental studies of the visual cortex justify the use of such \\x0clters in the\\nso-called standard model for object recognition (Riesenhuber and Poggio, 1999; Serre et al. , 2007;\\nMutch and Lowe, 2008), whose \\x0clters are \\x0cxed, in contrast to those of Convolutional Neural',\n",
       " 'Mutch and Lowe, 2008), whose \\x0clters are \\x0cxed, in contrast to those of Convolutional Neural\\nNetworks (CNNs) (LeCun et al. , 1998; Behnke, 2003; Simard et al. , 2003), whose weights (\\x0clters)\\nare randomly initialized and changed in a supervised way using back-propagation (BP).\\nDespite the hardware progress of the past decades, computational speed is still a limiting\\nfactor for CNN architectures characterized by many building blocks typically set by trial and\\nerror. To systematically test the impact of various architectures on classi\\x0ccation performance,\\nwe present a fast CNN implementation on Graphics Processing Units (GPUs). Previous GPU\\nimplementations of CNNs (Chellapilla et al. , 2006; Uetz and Behnke, 2009) were hard-coded to\\nsatisfy GPU hardware constraints, whereas our implementation is \\rexible and fully online (i.e.,\\nTechnical Report No. IDSIA-01-11 2\\nweight updates after each image). It allows for training large CNNs within days instead of months,\\nsuch that we can investigate the in\\ruence of various structural parameters by exploring large\\nparameter spaces (Pinto et al. , 2009) and performing error analysis on repeated experiments.\\nWe evaluate various networks on the handwritten digit benchmark MNIST (LeCun et al. , 1998)',\n",
       " 'We evaluate various networks on the handwritten digit benchmark MNIST (LeCun et al. , 1998)\\nand two image classi\\x0ccation benchmarks: NORB (LeCun et al. , 2004) and CIFAR10 (Krizhevsky,\\n2009).\\n2 Convolutional neural networks\\nCNNs are hierarchical neural networks whose convolutional layers alternate with subsampling\\nlayers, reminiscent of simple and complex cells in the primary visual cortex (Wiesel and Hubel,\\n1959). CNNs vary in how convolutional and subsampling layers are realized and how the nets\\nare trained. The CNN architecture considered in this study di\\x0bers from the one of Simard et al.\\n(2003) in the sense that after each CNN-layer an optional max-pooling layer (Scherer et al. , 2010)\\ncan be used. Here we give a complete description of this independent implementation (Fig. 1).\\n2.1 Image processing layer\\nThe image processing layer is an optional pre-processing layer of prede\\x0cned \\x0clters that are kept\\n\\x0cxed during training. Thus additional information besides the raw input image can be provided\\nto the network, such as edges and gradients. In particular, we \\x0cnd that a contrast-extracting layer\\n(Fukushima, 2003) helps to improve the recognition rate for NORB.\\n2.2 Convolutional layer',\n",
       " '(Fukushima, 2003) helps to improve the recognition rate for NORB.\\n2.2 Convolutional layer\\nA convolutional layer is parametrized by the size and the number of the maps, kernel sizes, skipping\\nfactors, and the connection table. Each layer has Mmaps of equal size ( Mx,My). A kernel (blue\\nrectangle in Fig 1) of size ( Kx,Ky) is shifted over the valid region of the input image (i.e. the\\nkernel has to be completely inside the image). The skipping factors SxandSyde\\x0cne how many\\npixels the \\x0clter/kernel skips in x- and y-direction between subsequent convolutions. The size of\\nthe output map is then de\\x0cned as:\\nMn\\nx=Mn\\x001\\nx\\x00Kn\\nx\\nSnx+ 1+ 1;Mn\\ny=Mn\\x001\\ny\\x00Kn\\ny\\nSny+ 1+ 1 (1)\\nwhere index nindicates the layer. Each map in layer Lnis connected to at most Mn\\x001maps in\\nlayerLn\\x001. Neurons of a given map share their weights but have di\\x0berent receptive \\x0celds.\\n2.3 Max-pooling layer\\nThe biggest architectural di\\x0berence between our implementation and the CNN of LeCun et al.',\n",
       " '2.3 Max-pooling layer\\nThe biggest architectural di\\x0berence between our implementation and the CNN of LeCun et al.\\n(1998) is the use of a max-pooling layer instead of a sub-sampling layer. No such layer is used\\nby Simard et al. (2003) who simply skips nearby pixels prior to convolution, instead of pooling\\nor averaging. Scherer et al. (2010) found that max-pooling can lead to faster convergence, select\\nsuperior invariant features, and improve generalization. The output of the max-pooling layer is\\ngiven by the maximum activation over non-overlapping rectangular regions of size ( Kx,Ky). Maxpooling enables position invariance over larger local regions and downsamples the input image by\\na factor ofKxandKyalong each direction.\\nTechnical Report No. IDSIA-01-11 3\\n2.4 Classi\\x0ccation layer\\nKernel sizes of convolutional \\x0clters and max-pooling rectangles as well as skipping factors are\\nchosen such that either the output maps of the last convolutional layer are downsampled to 1\\npixel per map, or a fully connected layer combines the outputs of the topmost convolutional layer\\ninto a 1D feature vector. The top layer is always fully connected, with one output unit per class\\nlabel.',\n",
       " 'into a 1D feature vector. The top layer is always fully connected, with one output unit per class\\nlabel.\\nFigure 1: Architecture of a convolutional neural network. In this case, the convolutional layers\\nare fully connected. Both convolutional layers use a kernel of 5 x 5 and skipping factors of 1.\\n3 GPU implementation\\nThe latest generation of NVIDIA GPUs, the 400 and 500 series (we use GTX 480 & GTX 580), has\\nmany advantages over older GPUs, most notably the presence of a R/W L2 global cache for device\\nmemory. This permits faster programs and simpli\\x0ces writing the code. In fact, the corresponding\\ntransfer of complexity into hardware alleviates many software and optimization problems. Our\\nexperiments show that the CNN program becomes 2-3 times faster just by switching from GTX\\n285 to GTX 480.\\nTechnical Report No. IDSIA-01-11 4\\nManual optimization of CUDA code is very time-consuming and error prone. We optimize for\\nthe new architecture, relying on the L2 cache for many of the device memory accesses, instead of\\nmanually writing code that uses textures and shared memory. Code obtained by this pragmatic\\nstrategy is fast enough. We use the following types of optimization: pre-computed expressions,\\nunrolled loops within template kernels, strided matrices to obtain coalesced memory accesses and',\n",
       " 'strategy is fast enough. We use the following types of optimization: pre-computed expressions,\\nunrolled loops within template kernels, strided matrices to obtain coalesced memory accesses and\\nregisters wherever possible. Additional manual optimizations are possible in case future image\\nclassi\\x0ccation problems will require even more computing power.\\n3.1 Data structures\\nBoth outputs yand deltas\\x0eof layerLnare 2D strided. Their original size is Mx\\x02MM y, but they\\nare horizontally strided with a pitch of 32 \\roats (we use this stride for all 2D data), resulting in\\ncoalesced memory accesses. The vertical stride avoids additional bounding tests in CUDA kernels.\\nAll connections between maps of consecutive layers Ln\\x001andLnare stored in matrix Cn.\\nEach row of Cncontains all connections that feed into a particular map in layer Ln. Because\\nwe aim for a \\rexible architecture with partially connected layers, in the \\x0crst column we store the\\nnumber of previous connections. This index is useful for Forward Propagation (FP) and Adjusting\\nWeights (AW) CUDA kernels. The second column stores the number of connections, followed by\\ncorresponding indices of maps in Ln\\x001connected to the current map.',\n",
       " \"Weights (AW) CUDA kernels. The second column stores the number of connections, followed by\\ncorresponding indices of maps in Ln\\x001connected to the current map.\\nFor BP and FP, analogous information about connections is needed. We therefore store backward connections in CBP. AW requires a list of all map connections (see Subsection 3.4), stored\\nas an array of map index pairs. Dealing with biases in BP kernel requires to know where the\\nweights of particular connections start; this information is stored in a 2D array WIDX BPof size\\nMn\\x02Mn\\x001.\\n3.2 Forward propagation\\nA straightforward way of parallelizing FP is to assign a thread block to each map that has to\\nbe computed. For maps bigger than 1024 neurons, the job is further split into smaller blocks by\\nassigning a block to each line of the map, because the number of threads per block is limited (1024\\nfor GTX 480). A one to one correspondence between threads and the map's neurons is assumed.\\nBecause of weight sharing, threads inside a block can access data in parallel, in particular the\\nsame weights and inputs from the previous layer. Each thread starts by initializing its sum with\\nthe bias, then loops over all map connections, convolving the appropriate patch of the input map\\nwith the corresponding kernel. The output is obtained by passing the sum through a scaled tanh\",\n",
       " 'the bias, then loops over all map connections, convolving the appropriate patch of the input map\\nwith the corresponding kernel. The output is obtained by passing the sum through a scaled tanh\\nactivation function, and then written to device memory.\\n3.3 Backward propagation\\nBP of deltas can be done in two ways: by pushing or by pulling. Pushing deltas means taking each\\ndelta from the current layer and computing the corresponding deltas for the previous layer. For\\nan architecture with shared weights this has the disadvantage of being hard to code. Each delta\\nfrom the current layer contributes to many deltas in the previous layer, which translates into a lot\\nof programming. There are two ways of avoiding this: either writing partial deltas to a separated\\nblock of memory and then putting everything together by calling another kernel (slow because\\nof a tremendous increase in the number of memory accesses, and the need of another kernel), or\\nusing atomic writes (to avoid data hazards) to update deltas (very slow because many writings\\nare serialized). We implement pulling deltas, which has almost none of the above speed-limiting\\ndrawbacks, but is a bit more complicated.\\nTechnical Report No. IDSIA-01-11 5\\nThe (uni- or bi-dimensional) thread grid assigns a (bi- or uni-dimensional) thread block to each',\n",
       " 'Technical Report No. IDSIA-01-11 5\\nThe (uni- or bi-dimensional) thread grid assigns a (bi- or uni-dimensional) thread block to each\\nmap in the previous layer and a thread to each neuron in every map. Similar to FP, for maps\\nwith more than 1024 neurons, the 2D grid is further split into smaller 1D blocks by assigning a\\n2D block to each row of the map. Each thread computes the delta of its corresponding neuron by\\npulling deltas from the current layer. For every neuron in the previous layer we have to determine\\nthe list of neurons in the current layer which are connected to it. Let us consider neuron ( i;j)\\nfrom a map in layer Ln\\x001, and then assume that ( x;y) are the coordinates of neurons in maps\\nofLnthat contribute to the delta of neuron ( i;j). The (x;y) neuron is connected to kernel size\\nnumber neurons ( Kx\\x02Ky) from each connected map in the previous layer. The indices in Ln\\x001\\nof the neurons connected through a kernel to the ( x;y) neuron are:\\nx(Sx+ 1) \\x14i\\x14x(Sx+ 1) +Kx\\x001;\\ny(Sy+ 1) \\x14j\\x14y(Sy+ 1) +Ky\\x001:',\n",
       " 'y(Sy+ 1) \\x14j\\x14y(Sy+ 1) +Ky\\x001:\\nWe can now compute the inequalities for ( x;y):\\ni\\x00Kx+ 1\\nSx+ 1\\x14x\\x14i\\nSx+ 1;\\nj\\x00Ky+ 1\\nSy+ 1\\x14y\\x14j\\nSy+ 1:\\nBecause (x;y) has to be inside the map, the \\x0cnal inequalities are:\\nmax\\x12\\x18i\\x00Kx+ 1\\nSx+ 1\\x19\\n;0\\x13\\n\\x14x\\x14min\\x12\\x16i\\nSx+ 1\\x17\\n;Mx\\x001\\x13\\n;\\nmax\\x12\\x18j\\x00Ky+ 1\\nSy+ 1\\x19\\n;0\\x13\\n\\x14y\\x14min\\x12\\x16j\\nSy+ 1\\x17\\n;My\\x001\\x13\\n:\\nThe above inequalities state that the delta of neuron ( i;j) fromLn\\x001is computed from deltas of\\nneurons in a rectangular area in maps of Ln(Fig. 2). After summing up the deltas, each thread\\nmultiplies the result by the derivative of the activation function.\\n3.4 Adjusting weights\\nFP and BP have a grid on the list of maps, but the AW thread grid is on the list of kernels (\\x0clters)',\n",
       " \"3.4 Adjusting weights\\nFP and BP have a grid on the list of maps, but the AW thread grid is on the list of kernels (\\x0clters)\\nbetween maps of two consecutive layers. The 1D grid has a block for each connection between two\\nmaps. Thread blocks are 2D, with a corresponding thread for every kernel weight. The bias weight\\nis included as an entire row of threads, thus requiring thread blocks to have ( Kx+1)\\x02Kythreads.\\nMost of the time these additional Kythreads will do nothing, thread (0,0) being activated only\\nfor blocks that have to process the bias.\\n4 Experiments\\nWe use a system with a Core i7-920 (2.66GHz), 12 GB DDR3 and four graphics cards: 2 x\\nGTX 480 and 2 x GTX 580. The correctness of the CPU version is checked by comparing the\\nanalytical gradient with its \\x0cnite di\\x0berence approximation. On GPU this is not possible because\\nall computations are performed with single precision \\roating point numbers. Hence the GPU\\nimplementation's correctness is checked by comparing its results to those of a randomly initialized\\nnet after training it for several epochs on the more accurate CPU version. Obtaining identical\\nresults after trillions of operations is a strong indication of correctness.\\nTechnical Report No. IDSIA-01-11 6\",\n",
       " \"results after trillions of operations is a strong indication of correctness.\\nTechnical Report No. IDSIA-01-11 6\\nFigure 2: Back propagating deltas. A connection between two maps from two consecutive layers\\nis displayed. The map in Ln\\x001has 29 x 29 neurons; the map in Lnhas 13 x 13 neurons. They\\nare linked through a 5 x 5 kernel K. Skipping factors of Sx= 1 andSy= 1 are assumed. Arrows\\nand colors depict the correspondence between neurons in Ln\\x001and their sources in Ln.\\nThe implemented CNN's plain feed-forward architecture is trained using on-line gradient descent. All images from the training set are used for training and also for validation. If deformations\\nare enabled, only the images from the training set will be deformed. Weights are initialized according to a uniform random distribution in the range [ \\x000:05;0:05]. Each neuron's activation\\nfunction is a scaled hyperbolic tangent: y(a) = 1:7159 tanh(0 :6666a) (LeCun et al. , 1998).\\nWe pick the trained CNN with the lowest validation error, and evaluate it on the test set\\n(Test for best Validation - TfbV). The best test error (bT) is also listed for all experiments. The\",\n",
       " '(Test for best Validation - TfbV). The best test error (bT) is also listed for all experiments. The\\nreported computation times per epoch include training, validation and testing as well as all data\\ntransfers.\\n4.1 Experiments on MNIST\\nFor the MNIST dataset the networks are trained on deformed images, continually generated in\\non-line fashion. A\\x0ene (translation, rotation, scaling, horizontal shearing) and elastic deformations\\n(Simard et al. , 2003) are combined. We use a variable learning rate that shrinks by a multiplicative\\nconstant after each epoch, from 10\\x003down to 3 \\x0110\\x005after 500 epochs.\\nFully connected convolutional layers lead to an exploding number of network connections and\\nweights, making training of big and deep CNNs for hundreds of epochs impractical even on GPUs.\\nPartial connectivity alleviates this problem and is also biologically more plausible. We reduce\\nthe number of connections between convolutional layers in a random way. Table 1 lists results\\nof various networks with 2 to 7 hidden layers with random connections. Additional layers result\\nin better networks, the best one achieving a test error of 0.35% for best validation and a best\\ntest error of 0.27%. The best previous CNN result on MNIST is 0.40% (Simard et al. , 2003).',\n",
       " 'test error of 0.27%. The best previous CNN result on MNIST is 0.40% (Simard et al. , 2003).\\nA 0.35% error rate was recently also obtained by a big, deep MLP (Cire\\x18 san et al. , 2010) with\\nmany more free parameters. Deeper nets require more computation time to complete an epoch,\\nbut we observe that they also need fewer epochs to achieve good test errors. The deepest CNN\\nTechnical Report No. IDSIA-01-11 7\\nTable 1: Error rates on MNIST test set for randomly connected CNNs with 2 to 6 convolutional\\nlayers with M Maps and an optional fully connected layer with N neurons. Various kernel sizes\\nand skipping factors were used.\\n#M, #N bT TfbV\\nin Hidden Layers [%] [%]\\n20M-60M 0.95 1.02\\n20M-60M-150N 0.50 0.55\\n20M-60M-100M-150N 0.33 0.38\\n20M-40M-60M-80M-100M-120M-150N 0.27 0.35\\nfrom Table 1 reaches 2.42%, 0.97% and 0.48% after one, three and seventeen epochs, respectively.',\n",
       " 'from Table 1 reaches 2.42%, 0.97% and 0.48% after one, three and seventeen epochs, respectively.\\nOn the other hand, the network with 4 instead of 7 hidden layers reaches 4.71%, 1.58%, 0.68%\\nafter one, three and seventeen epochs, achieving a test error below 0.50% after only 34 epochs.\\nThis shows once more that deep networks, contrary to common belief, can be trained successfully\\nby back-propagation. Despite the numerous free parameters, deep networks seem to learn faster\\n(better recognition rates after fewer epochs) than shallow ones.\\nWe consider MNIST an almost solved problem. The remaining errors stem from digits that\\nare ambiguous or miss parts.\\n4.2 Experiments on NORB\\nNORB contains stereo images of 3D objects. Hence there are two maps on the input layer.\\nRotation, scaling, shearing and elastic distortions seem to have a negative impact on generalization.\\nThese deformations improve recognition rates for digits that are intrinsically 2D (Cire\\x18 san et al. ,\\n2010), but seem inadequate for 3D objects.\\nInitial experiments on NORB show that unlike with MNIST where we use deformations, the\\nCNN needs only 3 to 6 epochs to reach zero validation error. This allows us to quickly run numerous',\n",
       " 'Initial experiments on NORB show that unlike with MNIST where we use deformations, the\\nCNN needs only 3 to 6 epochs to reach zero validation error. This allows us to quickly run numerous\\nrepetitive experiments with huge networks with hundreds of maps per layer. We decided to use a\\nCNN with \\x0cve hidden layers: layer1, a convolutional layer with 300 maps, kernel size 6 \\x026 and\\nskipping factors 1 \\x021; layer2, a max-pooling layer over a 2 \\x022 region; layer3, a convolutional layer\\nwith 500 maps, kernel size 4 \\x024, skipping factors 0 \\x020; layer4, a max-pooling layer over a 4 \\x024\\nregion; layer5, a fully connected layer with 500 neurons. The learning rate is initialized by 0.001\\nand multiplied by 0.95 after every epoch.\\nTable 2 summarizes the results of four di\\x0berent experiments by switching on/o\\x0b translation as\\nwell as the \\x0cxed image processing layer. We report the average error rate as well as the standard\\ndeviation of N independent runs with identical architectures but di\\x0berent weight initializations.\\nFor the \\x0crst experiment without translation and no image processing (IP), an average test error\\nrate of 7.86% is obtained. With additional translations of at most 5%, the average error rate drops',\n",
       " 'rate of 7.86% is obtained. With additional translations of at most 5%, the average error rate drops\\nto 4.71%, contradicting the common belief that CNNs are translation invariant. These results are\\non par or better than others in the literature: 5.90% error rate for a combination of CNNs and\\nSVMs (LeCun et al. , 2004) and 5.20% error rate for restricted Boltzman machines (Nair and\\nHinton, 2009).\\nThe best previously published result on NORB (2.87%) was obtained by a hierarchical neural\\nnetwork which to every convolutional layer provides a subsampled version plus edge information\\nof the original image (Uetz and Behnke, 2009). This motivated us to implement a pre-processing\\nlayer with \\x0cxed \\x0clters. We tried simple edge masks (Sobel, Scharr) but obtained best results with a\\ncontrast-extraction layer (Fukushima, 2003) realized by Mexican hat-shaped \\x0clters of size 21 \\x0221,\\none with a concentric on-center receptive \\x0celd and one with a concentric o\\x0b-center receptive \\x0celd,\\nTechnical Report No. IDSIA-01-11 8\\nTable 2: Average error rates and standard deviations of N runs for a \\x0cve hidden layer CNN on',\n",
       " 'Technical Report No. IDSIA-01-11 8\\nTable 2: Average error rates and standard deviations of N runs for a \\x0cve hidden layer CNN on\\nthe NORB test set (see text for details).\\ntrans. [%] IP TfbV [%] runs time/epoch [s]\\n0 no 7.86 \\x060.55 50 1141\\n5 no 4.71 \\x060.57 50 1563\\n0 yes 3.94 \\x060.48 50 1658\\n5 yes 2.53\\x060.40 100 2080\\nsimilar to the \\x0clters automatically created by unsupervised Predictability Minimization (Schmidhuber, 1992) applied to natural images (Schmidhuber et al. , 1996). The \\x0crst \\x0clter extracts positive\\ncontrast in brightness, whereas the latter extracts negative contrast. Each image from the original\\nNORB is \\x0cltered, consequently the input of the CNN has six maps: the original image plus the\\npositive and negative contrast for each of the two stereo channels. Using such a pre-processing\\nlayer results in lower average error rates, 3.94% without translation and 2.53% with translation.\\nThis result improves the previous state of the art on NORB (Uetz and Behnke, 2009).\\nExperience with other image datasets tells us that NORB is unusual. The training set has only',\n",
       " \"Experience with other image datasets tells us that NORB is unusual. The training set has only\\n\\x0cve instances per class. The resulting poor training set variability makes the nets learn quickly\\nbut generalize badly. NORB is the only dataset that pro\\x0cts from a \\x0cxed pre-processing layer in\\na substantial way. For MNIST and CIFAR10 such pre-processing has little or no e\\x0bect. It is also\\nworth noting that NORB's standard error rate deviation is bigger than CIFAR10's (see Tables 2\\nand 3). Identical nets with di\\x0berent initializations do not produce very consistent results. The\\nbest net had an error rate of 1.72%, the worst 3.69%.\\n4.3 Experiments on CIFAR 10\\nCIFAR10 is a collection of natural color images of 32x32 pixels. It contains 10 classes, each of them\\nwith 5000 samples in the training set and 1000 in the test set. The images greatly vary inside each\\nclass. They are not necessarily centered, may contain only parts of the object, and have varying\\nbackgrounds. All of this makes CIFAR10 the hardest problem addressed in this paper. The CNN\\nhas three maps, one for each color channel (RGB). The CIFAR10 images are relatively small in\",\n",
       " \"has three maps, one for each color channel (RGB). The CIFAR10 images are relatively small in\\ncomparison to NORB's, and force us to use small kernels. The tested CNNs di\\x0ber only in the\\nnumber of maps per convolutional and max-pooling layer. All have eight hidden layers: layer1, a\\nconvolutional layer with 3 \\x023 kernels and skipping factor of 0; layer2, a max-pooling layer over a\\n3\\x023 region; layer3, a convolutional layer with 3 \\x023 kernels and skipping factors of 0 \\x020; layer4,\\na max-pooling over a 2 \\x022 region; layer5, a convolutional layer with 3 \\x023 kernels and a skipping\\nfactors of 0 \\x020; layer6, a max pooling layer over a 2 \\x022 region; layer7, a fully connected layer\\nwith 300 neurons; layer8, a fully connected layer with 100 neurons.\\nLike for MNIST, the learning rate is initialized by 0.001 and multiplied by 0.993 after every\\nepoch. Results in Table 3 show that without translation the error rate does not drop below 28%;\\nadding edge information does not help at all. Translations have a very positive e\\x0bect, decreasing\",\n",
       " 'adding edge information does not help at all. Translations have a very positive e\\x0bect, decreasing\\nthe error rate to almost 20%. Contrast extraction \\x0clters are better than the Sobel/Scharr \\x0clters but\\nstill worse than no pre-processing layer at all. Despite some CNN-inherent translation invariance,\\nadditional training image translations cause better generalization; additional image processing\\nproved useless though.\\nTo see if bigger nets are better, we increase the number of maps per layer from 100 to 200, 300\\nand 400, respectively (last three rows in Tab. 3). Training time increases exponentially, but the\\ntest error decreases, reaching a minimum for nets with 300 maps per layer. Our 19.51% error rate\\nis better than the previous state of the art for this dataset, 20.40% (Coates et al. , 2010) and 25.50%\\nTechnical Report No. IDSIA-01-11 9\\nTable 3: Average error rates and standard deviations for N runs of an eight hidden layer CNN on\\nthe CIFAR10 test set (see text for details). The \\x0crst \\x0cve nets have 100 maps per convolutional\\nand max-pooling layer, whereas the sixth, seventh and eighth have 200, 300 and 400 maps per',\n",
       " 'and max-pooling layer, whereas the sixth, seventh and eighth have 200, 300 and 400 maps per\\nhidden layer, respectively. IP - image processing layer: edge - 3 \\x023 Sobel and Scharr \\x0clters; hat 13\\x0213 positive and negative contrast extraction \\x0clters.\\ntrans. [%] maps IP TfbV [%] runs time/epoch [s]\\n0 100 no 28.87 \\x060.37 11 93\\n0 100 edge 29.11 \\x060.36 15 104\\n5 100 no 20.26 \\x060.21 11 111\\n5 100 edge 21.87 \\x060.57 5 120\\n5 100 hat 21.44 \\x060.44 4 136\\n5 200 no 19.90 \\x060.16 5 248\\n5 300 no 19.51 \\x060.18 5 532\\n5 400 no 19.54 \\x060.16 5 875\\n(Yu and Zhang, 2010). Unlike Coates et al. (2010), however, we use the original images without\\nany particular input normalization. Note that the error rate standard deviations are smaller than\\nthose obtained on NORB, that is, di\\x0berent initializations yield consistent results.\\n4.4 Speedup factor of GPU code\\nThe GPU code scales well with network size. For small nets the speedup is small (but still over 10)',\n",
       " '4.4 Speedup factor of GPU code\\nThe GPU code scales well with network size. For small nets the speedup is small (but still over 10)\\nsince they \\x0ct better inside the CPU cache, and GPU resources are underutilized. For huge nets\\n(ex: Table 2) the GPU implementation is more than 60 times faster than a compiler-optimized\\nCPU version. Given the \\rexibility of our GPU version, this is a signi\\x0ccant speedup. One epoch\\ntakes 35 GPU minutes but more than 35 CPU hours.\\n5 Conclusion\\nWe presented high-performance GPU-based CNN variants trained by on-line gradient descent, with\\nsparse random connectivity, computationally more e\\x0ecient and biologically more plausible than\\nfully connected CNNs. Principal advantages include state-of-the-art generalization capabilities,\\ngreat \\rexibility and speed. All structural CNN parameters such as input image size, number of\\nhidden layers, number of maps per layer, kernel sizes, skipping factors and connection tables are\\nadaptable to any particular application. We applied our networks to benchmark datasets for digit\\nrecognition (MNIST), 3D object recognition (NORB), and natural images (CIFAR10). On MNIST\\nthe best network achieved a recognition test error rate of 0.35%, on NORB 2.53% and on CIFAR10',\n",
       " 'the best network achieved a recognition test error rate of 0.35%, on NORB 2.53% and on CIFAR10\\n19.51%. Our results are raising the bars for all three benchmarks. Currently the particular CNN\\ntypes discussed in this paper seem to be the best adaptive image recognizers, provided there is a\\nlabeled dataset of su\\x0ecient size. No unsupervised pretraining is required. Good results require\\nbig and deep but sparsely connected CNNs, computationally prohibitive on CPUs, but feasible on\\ncurrent GPUs, where our implementation is 10 to 60 times faster than a compiler-optimized CPU\\nversion.\\nTechnical Report No. IDSIA-01-11 10\\nAcknowledgment\\nThis work was partially funded by the Swiss Commission for Technology and Innovation (CTI),\\nProject n. 9688.1 IFF: Intelligent Fill in Form.\\nReferences\\nS. Behnke. Hierarchical Neural Networks for Image Interpretation , volume 2766 of Lecture Notes\\nin Computer Science . Springer, 2003.\\nK. Chellapilla, S. Puri, and P. Simard. High performance convolutional neural networks for\\ndocument processing. In International Workshop on Frontiers in Handwriting Recognition ,\\n2006.\\nD. C. Cire\\x18 san, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets',\n",
       " \"2006.\\nD. C. Cire\\x18 san, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets\\nfor handwritten digit recogntion. Neural Computation , 22(12):3207{3220, 2010.\\nA. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning.\\nInAdvances in Neural Information Processing Systems , 2010.\\nK. Fukushima. Neocognitron: A self-organizing neural network for a mechanism of pattern recognition una\\x0bected by shift in position. Biological Cybernetics , 36(4):193{202, 1980.\\nK. Fukushima. Neocognitron for handwritten digit recognition. Neurocomputing , 51:161{180,\\n2003.\\nP. O. Hoyer and A. Hyv\\x7f arinen. Independent component analysis applied to feature extraction\\nfrom colour and stero images. Network: Computation in Neural Systems , 11(3):191{210, 2000.\\nA. Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, Computer\\nScience Department, University of Toronto, 2009.\\nY. LeCun, L. Bottou, Y. Bengio, and P. Ha\\x0bner. Gradient-based learning applied to document\",\n",
       " 'Y. LeCun, L. Bottou, Y. Bengio, and P. Ha\\x0bner. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE , 86(11):2278{2324, November 1998.\\nY. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Proc. of Computer Vision and Pattern Recognition Conference ,\\n2004.\\nJ. Mutch and D. G. Lowe. Object class recognition and localization using sparse features with\\nlimited receptive \\x0celds. Int. J. Comput. Vision , 56(6):503{511, 2008.\\nV. Nair and G. E. Hinton. 3d object recognition with deep belief nets. In Advances in Neural\\nInformation Processing Systems , 2009.\\nB. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy\\nemployed by v1? Vision Research , 37(23):3311{3325, December 1997.\\nN. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A high-throughput screening approach\\nto discovering good forms of biologically inspired visual representation. PLoS computational\\nbiology , 5(11):e1000579, November 2009.',\n",
       " 'to discovering good forms of biologically inspired visual representation. PLoS computational\\nbiology , 5(11):e1000579, November 2009.\\nM. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nat. Neurosci. ,\\n2(11):1019{1025, 1999.\\nTechnical Report No. IDSIA-01-11 11\\nD. Scherer, A. M\\x7f uller, and S. Behnke. Evaluation of pooling operations in convolutional architectures for object recognition. In International Conference on Arti\\x0ccial Neural Networks , 2010.\\nJ. Schmidhuber, M. Eldracher, and B. Foltin. Semilinear predictability minimization produces\\nwell-known feature detectors. Neural Computation , 8(4):773{786, 1996.\\nJ. Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation ,\\n4(6):863{879, 1992.\\nT. Serre, L. Wolf, and T. Poggio. Object recognition with features inspired by visual cortex. In\\nProc. of Computer Vision and Pattern Recognition Conference , 2007.\\nP. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks applied\\nto visual document analysis. In Seventh International Conference on Document Analysis and',\n",
       " \"to visual document analysis. In Seventh International Conference on Document Analysis and\\nRecognition , pages 958{963, 2003.\\nR. Uetz and S. Behnke. Large-scale object recognition with cuda-accelerated hierarchical neural\\nnetworks. In IEEE International Converence on Intelligent Computing and Intelligent Systems\\n(ICIS) , 2009.\\nD. H. Wiesel and T. N. Hubel. Receptive \\x0celds of single neurones in the cat's striate cortex. J.\\nPhysiol. , 148:574{591, 1959.\\nK. Yu and T. Zhang. Improved local coordinate coding using local tangents. In Proceedings of the\\nInternational Conference on Machine Learning , 2010.\",\n",
       " \"Knowledge Matters: Importance of Prior Information for\\nOptimization\\nC \\x18 a\\x15 glar G\\x7f ul\\x18 cehre gulcehrc@iro.umontreal.ca\\nD\\x13 epartement d'informatique et de recherche op\\x13 erationnelle\\nUniversit\\x13 e de Montr\\x13 eal, Montr\\x13 eal, QC, Canada\\nYoshua Bengio bengioy@iro.umontreal.ca\\nD\\x13 epartement d'informatique et de recherche op\\x13 erationnelle\\nUniversit\\x13 e de Montr\\x13 eal, Montr\\x13 eal, QC, Canada\\nEditor: Not Assigned\\nAbstract\\nWe explore the e\\x0bect of introducing prior information into the intermediate level of deep\\nsupervised neural networks for a learning task on which all the black-box state-of-the-art machine learning algorithms tested have failed to learn. We motivate our work from the hypothesis\\nthat there is an optimization obstacle involved in the nature of such tasks, and that humans\\nlearn useful intermediate concepts from other individuals via a form of supervision or guidance\\nusing a curriculum. The experiments we have conducted provide positive evidence in favor of\\nthis hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset for\",\n",
       " 'using a curriculum. The experiments we have conducted provide positive evidence in favor of\\nthis hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset for\\nwhich each image input contains three sprites, and the binary target class is 1 if all three have\\nthe same shape. Black-box machine learning algorithms only got chance on this task. Standard\\ndeep supervised neural networks also failed. However, using a particular structure and guiding\\nthe learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allows to nail the task. Much better than chance but imperfect results are\\nalso obtained by exploring architecture and optimization variants, pointing towards a di\\x0ecult\\noptimization task. We hypothesize that the learning di\\x0eculty is due to the composition of two\\nhighly non-linear tasks. Our \\x0cndings are also consistent with hypotheses on cultural learning\\ninspired by the observations of e\\x0bective local minima (possibly due to ill-conditioning and the\\ntraining procedure not being able to escape what appears like a local minimum).\\nKeywords: Deep Learning, Neural Networks, Optimization, Evolution of Culture, Curriculum Learning, Training with Hints\\n1. Introduction\\nThere is a recent emerging interest in di\\x0berent \\x0celds of science for cultural learning (Henrich\\nand McElreath, 2003) and how groups of individuals exchanging information can learn in ways',\n",
       " 'and McElreath, 2003) and how groups of individuals exchanging information can learn in ways\\nsuperior to individual learning. This is also witnessed by the emergence of new research \\x0celds\\nsuch as \"Social Neuroscience\". Learning from other agents in an environment by the means\\nof cultural transmission of knowledge with a peer-to-peer communication is an e\\x0ecient and\\nnatural way of acquiring or propagating common knowledge. The most popular belief on how\\nthe information is transmitted between individuals is that bits of information are transmitted\\nby small units, called memes, which share some characteristics of genes, such as self-replication,\\nmutation and response to selective pressures (Dawkins, 1976).\\n1arXiv:1301.4083v6  [cs.LG]  13 Jul 2013\\nThis paper is based on the hypothesis (which is further elaborated in Bengio (2013a)) that\\nhuman culture and the evolution of ideas have been crucial to counter an optimization issue:\\nthis di\\x0eculty would otherwise make it di\\x0ecult for human brains to capture high level knowledge of the world without the help of other educated humans. In this paper machine learning\\nexperiments are used to investigate some elements of this hypothesis by seeking answers for\\nthe following questions: are there machine learning tasks which are intrinsically hard for a\\nlone learning agent but that may become very easy when intermediate concepts are provided',\n",
       " 'the following questions: are there machine learning tasks which are intrinsically hard for a\\nlone learning agent but that may become very easy when intermediate concepts are provided\\nby another agent as additional intermediate learning cues, in the spirit of Curriculum Learning (Bengio et al., 2009b)? What makes such learning tasks more di\\x0ecult? Can speci\\x0cc initial\\nvalues of the neural network parameters yield success when random initialization yield complete failure? Is it possible to verify that the problem being faced is an optimization problem or\\nwith a regularization problem? These are the questions discussed (if not completely addressed)\\nhere, which relate to the following broader question: how can humans (and potentially one day,\\nmachines) learn complex concepts?\\nIn this paper, results of di\\x0berent machine learning algorithms on an arti\\x0ccial learning task\\ninvolving binary 64 \\x0264 images are presented. In that task, each image in the dataset contains\\n3 Pentomino tetris sprites (simple shapes). The task is to \\x0cgure out if all the sprites in the\\nimage are the same or if there are di\\x0berent sprite shapes in the image. Several state-of-the-art\\nmachine learning algorithms have been tested and none of them could perform better than\\na random predictor on the test set. Nevertheless by providing hints about the intermediate\\nconcepts (the presence and location of particular sprite classes), the problem can easily be solved',\n",
       " 'a random predictor on the test set. Nevertheless by providing hints about the intermediate\\nconcepts (the presence and location of particular sprite classes), the problem can easily be solved\\nwhere the same-architecture neural network without the intermediate concepts guidance fails.\\nSurprisingly, our attempts at solving this problem with unsupervised pre-training algorithms\\nfailed solve this problem. However, with speci\\x0cc variations in the network architecture or\\ntraining procedure, it is found that one can make a big dent in the problem. For showing the\\nimpact of intermediate level guidance, we experimented with a two-tiered neural network, with\\nsupervised pre-training of the \\x0crst part to recognize the category of sprites independently of\\ntheir orientation and scale, at di\\x0berent locations, while the second part learns from the output\\nof the \\x0crst part and predicts the binary task of interest.\\nThe objective of this paper is not to propose a novel learning algorithm or architecture,\\nbut rather to re\\x0cne our understanding of the learning di\\x0eculties involved with composed tasks\\n(here a logical formula composed with the detection of object classes), in particular the training\\ndi\\x0eculties involved for deep neural networks. The results also bring empirical evidence in favor\\nof some of the hypotheses from Bengio (2013a), discussed below, as well as introducing a\\nparticular form of curriculum learning (Bengio et al., 2009b).',\n",
       " \"of some of the hypotheses from Bengio (2013a), discussed below, as well as introducing a\\nparticular form of curriculum learning (Bengio et al., 2009b).\\nBuilding di\\x0ecult AI problems has a long history in computer science. Speci\\x0ccally hard\\nAI problems have been studied to create CAPTCHA's that are easy to solve for humans, but\\nhard to solve for machines (Von Ahn et al., 2003). In this paper we are investigating a di\\x0ecult\\nproblem for the o\\x0b-the-shelf black-box machine learning algorithms.1\\n1.1 Curriculum Learning and Cultural Evolution Against E\\x0bective Local Minima\\nWhat Bengio (2013a) calls an e\\x0bective local minimum is a point where iterative training\\nstalls, either because of an actual local minimum or because the optimization algorithm is\\n1. You can access the source code of some experiments presented in that paper and their hyperparameters from\\nhere: https://github.com/caglar/kmatters\\n2\\nunable (in reasonable time) to \\x0cnd a descent path (e.g., because of serious ill-conditioning). In\\nthis paper, it is hypothesized that some more abstract learning tasks such as those obtained\\nby composing simpler tasks are more likely to yield e\\x0bective local minima for neural networks,\\nand are generally hard for general-purpose machine learning algorithms.\",\n",
       " \"by composing simpler tasks are more likely to yield e\\x0bective local minima for neural networks,\\nand are generally hard for general-purpose machine learning algorithms.\\nThe idea that learning can be enhanced by guiding the learner through intermediate easier\\ntasks is old, starting with animal training by shaping (Skinner, 1958; Peterson, 2004; Krueger\\nand Dayan, 2009). Bengio et al. (2009b) introduce a computational hypothesis related to a\\npresumed issue with e\\x0bective local minima when directly learning the target task: the good\\nsolutions correspond to hard-to-\\x0cnd-by-chance e\\x0bective local minima, and intermediate tasks\\nprepare the learner's internal con\\x0cguration (parameters) in a way similar to continuation methods in global optimization (which go through a sequence of intermediate optimization problems,\\nstarting with a convex one where local minima are no issue, and gradually morphing into the\\ntarget task of interest).\\nIn a related vein, Bengio (2013a) makes the following inferences based on experimental\\nobservations of deep learning and neural network learning:\\nPoint 1: Training deep architectures is easier when some hints are given about the function\\nthat the intermediate levels should compute (Hinton et al., 2006; Weston et al., 2008;\\nSalakhutdinov and Hinton, 2009; Bengio, 2009). The experiments performed here expand\",\n",
       " 'Salakhutdinov and Hinton, 2009; Bengio, 2009). The experiments performed here expand\\nin particular on this point.\\nPoint 2: It is much easier to train a neural network with supervision (where examples ar\\nprovided to it of when a concept is present and when it is not present in a variety of\\nexamples) than to expect unsupervised learning to discover the concept (which may also\\nhappen but usually leads to poorer renditions of the concept). The poor results obtained\\nwith unsupervised pre-training reinforce that hypothesis .\\nPoint 3: Directly training all the layers of a deep network together not only makes it di\\x0ecult to\\nexploit all the extra modeling power of a deeper architecture but in many cases it actually\\nyields worse results as the number of required layers is increased (Larochelle et al., 2009;\\nErhan et al., 2010). The experiments performed here also reinforce that hypothesis.\\nPoint 4: Erhan et al. (2010) observed that no two training trajectories ended up in the\\nsame e\\x0bective local minimum, out of hundreds of runs, even when comparing solutions as\\nfunctions from input to output, rather than in parameter space (thus eliminating from the\\npicture the presence of symmetries and multiple local minima due to relabeling and other',\n",
       " 'functions from input to output, rather than in parameter space (thus eliminating from the\\npicture the presence of symmetries and multiple local minima due to relabeling and other\\nreparametrizations). This suggests that the number of di\\x0berent e\\x0bective local minima\\n(even when considering them only in function space) must be huge.\\nPoint 5: Unsupervised pre-training, which changes the initial conditions of the descent procedure, sometimes allows to reach substantially better e\\x0bective local minima (in terms\\nof generalization error!), and these better local minima do not appear to be reachable\\nby chance alone (Erhan et al., 2010). The experiments performed here provide another\\npiece of evidence in favor of the hypothesis that where random initialization can yield\\nrather poor results, speci\\x0ccally targeted initialization can have a drastic impact, i.e., that\\n3\\ne\\x0bective local minima are not just numerous but that some small subset of them are much\\nbetter and hard to reach by chance.2\\nBased on the above points, Bengio (2013a) then proposed the following hypotheses regarding\\nlearning of high-level abstractions.\\n\\x0fOptimization Hypothesis: When it learns, a biological agent performs an approximate\\noptimization with respect to some implicit objective function.',\n",
       " 'learning of high-level abstractions.\\n\\x0fOptimization Hypothesis: When it learns, a biological agent performs an approximate\\noptimization with respect to some implicit objective function.\\n\\x0fDeep Abstractions Hypothesis: Higher level abstractions represented in brains require deeper computations (involving the composition of more non-linearities).\\n\\x0fLocal Descent Hypothesis: The brain of a biological agent relies on approximate local\\ndescent and gradually improves itself while learning.\\n\\x0fE\\x0bective Local Minima Hypothesis: The learning process of a single human learner\\n(not helped by others) is limited by e\\x0bective local minima.\\n\\x0fDeeper Harder Hypothesis: E\\x0bective local minima are more likely to hamper learning\\nas the required depth of the architecture increases.\\n\\x0fAbstractions Harder Hypothesis: High-level abstractions are unlikely to be discovered by a single human learner by chance, because these abstractions are represented by\\na deep subnetwork of the brain, which learns by local descent.\\n\\x0fGuided Learning Hypothesis: A human brain can learn high level abstractions if\\nguided by the signals produced by other agents that act as hints or indirect supervision\\nfor these high-level abstractions.\\n\\x0fMemes Divide-and-Conquer Hypothesis: Linguistic exchange, individual learning',\n",
       " 'for these high-level abstractions.\\n\\x0fMemes Divide-and-Conquer Hypothesis: Linguistic exchange, individual learning\\nand the recombination of memes constitute an e\\x0ecient evolutionary recombination operator in the meme-space. This helps human learners to collectively build better internal\\nrepresentations of their environment, including fairly high-level abstractions.\\nThis paper is focused on \\\\ Point 1 \" and testing the \\\\ Guided Learning Hypothesis \", using\\nmachine learning algorithms to provide experimental evidence. The experiments performed\\nalso provide evidence in favor of the \\\\ Deeper Harder Hypothesis \" and associated \\\\ Abstractions\\nHarder Hypothesis \". Machine Learning is still far beyond the current capabilities of humans,\\nand it is important to tackle the remaining obstacles to approach AI. For this purpose, the\\nquestion to be answered is why tasks that humans learn e\\x0bortlessly from very few examples,\\nwhile machine learning algorithms fail miserably?\\n2. Recent work showed that rather deep feedforward networks can be very successfully trained when large\\nquantities of labeled data are available (Ciresan et al., 2010; Glorot et al., 2011a; Krizhevsky et al., 2012).\\nNonetheless, the experiments reported here suggest that it all depends on the task being considered, since\\neven with very large quantities of labeled examples, the deep networks trained here were unsuccessful.\\n4',\n",
       " 'Nonetheless, the experiments reported here suggest that it all depends on the task being considered, since\\neven with very large quantities of labeled examples, the deep networks trained here were unsuccessful.\\n4\\n2. Culture and Optimization Di\\x0eculty\\nAs hypothesized in the \\\\ Local Descent Hypothesis \", human brains would rely on a local approximate descent, just like a Multi-Layer Perceptron trained by a gradient-based iterative optimization. The main argument in favor of this hypothesis relies on the biologically-grounded\\nassumption that although \\x0cring patterns in the brain change rapidly, synaptic strengths underlying these neural activities change only gradually, making sure that behaviors are generally\\nconsistent across time. If a learning algorithm is based on a form of local (e.g. gradient-based)\\ndescent, it can be sensitive to e\\x0bective local minima (Bengio, 2013a).\\nWhen one trains a neural network, at some point in the training phase the evaluation of\\nerror seems to saturate, even if new examples are introduced. In particular Erhan et al. (2010)\\n\\x0cnd that early examples have a much larger weight in the \\x0cnal solution. It looks like the learner\\nis stuck in or near a local minimum. But since it is di\\x0ecult to verify if this is near a true local',\n",
       " 'is stuck in or near a local minimum. But since it is di\\x0ecult to verify if this is near a true local\\nminimum or simply an e\\x0bect of strong ill-conditioning, we call such a \\\\stuck\" con\\x0cguration an\\ne\\x0bective local minimum , whose de\\x0cnition depends not just on the optimization objective but\\nalso on the limitations of the optimization algorithm.\\nErhan et al. (2010) highlighted both the issue of e\\x0bective local minima and a regularization e\\x0bect when initializing a deep network with unsupervised pre-training. Interestingly, as\\nthe network gets deeper the di\\x0eculty due to e\\x0bective local minima seems to be get more pronounced. That might be because of the number of e\\x0bective local minima increases (more like\\nan actual local minima issue), or maybe because the good ones are harder to reach (more like\\nan ill-conditioning issue) and more work will be needed to clarify this question.\\nAs a result of Point 4 we hypothesize that it is very di\\x0ecult for an individual\\'s brain to\\ndiscover some higher level abstractions by chance only. As mentioned in the \\\\ Guided Learning\\nHypothesis \" humans get hints from other humans and learn high-level concepts with the guidance of other humans3. Curriculum learning (Bengio et al., 2009a) and incremental learning',\n",
       " '(Solomono\\x0b, 1989), are examples of this. This is done by properly choosing the sequence of\\nexamples seen by the learner, where simpler examples are introduced \\x0crst and more complex\\nexamples shown when the learner is ready for them. One of the hypothesis on why curriculum\\nworks states that curriculum learning acts as a continuation method that allows one to discover\\na good minimum, by \\x0crst \\x0cnding a good minimum of a smoother error function. Recent experiments on human subjects also indicates that humans teach by using a curriculum strategy\\n(Khan et al., 2011).\\nSome parts of the human brain are known to have a hierarchical organization (i.e. visual\\ncortex) consistent with the deep architecture studied in machine learning papers. As we go from\\nthe sensory level to higher levels of the visual cortex, we \\x0cnd higher level areas corresponding\\nto more abstract concepts. This is consistent with the Deep Abstractions Hypothesis .\\nTraining neural networks and machine learning algorithms by decomposing the learning\\ntask into sub-tasks and exploiting prior information about the task is well-established and\\nin fact constitutes the main approach to solving industrial problems with machine learning.\\nThe contribution of this paper is rather on rendering explicit the e\\x0bective local minima issue\\nand providing evidence on the type of problems for which this di\\x0eculty arises. This prior',\n",
       " \"The contribution of this paper is rather on rendering explicit the e\\x0bective local minima issue\\nand providing evidence on the type of problems for which this di\\x0eculty arises. This prior\\ninformation and hints given to the learner can be viewed as inductive bias for a particular task,\\nan important ingredient to obtain a good generalization error (Mitchell, 1980). An interesting\\n3. But some high-level concepts may also be hardwired in the brain, as assumed in the universal grammar\\nhypothesis (Montague, 1970), or in nature vs nurture discussions in cognitive science.\\n5\\nearlier \\x0cnding in that line of research was done with Explanation Based Neural Networks\\n(EBNN) in which a neural network transfers knowledge across multiple learning tasks. An\\nEBNN uses previously learned domain knowledge as an initialization or search bias (i.e. to\\nconstrain the learner in the parameter space) (O'Sullivan, 1996; Mitchell and Thrun, 1993).\\nAnother related work in machine learning is mainly focused on reinforcement learning algorithms, based on incorporating prior knowledge in terms of logical rules to the learning\\nalgorithm as a prior knowledge to speed up and bias learning (Kunapuli et al., 2010; Towell\\nand Shavlik, 1994).\\nAs discussed in \\\\ Memes Divide and Conquer Hypothesis \\\\ societies can be viewed as a\",\n",
       " 'and Shavlik, 1994).\\nAs discussed in \\\\ Memes Divide and Conquer Hypothesis \\\\ societies can be viewed as a\\ndistributed computational processing systems. In civilized societies knowledge is distributed\\nacross di\\x0berent individuals, this yields a space e\\x0eciency. Moreover computation, i.e. each\\nindividual can specialize on a particular task/topic, is also divided across the individuals in the\\nsociety and hence this will yield a computational e\\x0eciency. Considering the limitations of the\\nhuman brain, the whole processing can not be done just by a single agent in an e\\x0ecient manner.\\nA recent study in paleoantropology states that there is a substantial decline in endocranial\\nvolume of the brain in the last 30000 years Henneberg (1988). The volume of the brain shrunk\\nto 1241 ml from 1502 ml (Henneberg and Steyn, 1993). One of the hypothesis on the reduction\\nof the volume of skull claims that, decline in the volume of the brain might be related to the\\nfunctional changes in brain that arose as a result of cultural development and emergence of\\nsocieties given that this time period overlaps with the transition from hunter-gatherer lifestyle\\nto agricultural societies.\\n3. Experimental Setup\\nSome tasks, which seem reasonably easy for humans to learn4, are nonetheless appearing almost\\nimpossible to learn for current generic state-of-art machine learning algorithms.',\n",
       " \"3. Experimental Setup\\nSome tasks, which seem reasonably easy for humans to learn4, are nonetheless appearing almost\\nimpossible to learn for current generic state-of-art machine learning algorithms.\\nHere we study more closely such a task, which becomes learnable if one provides hints to\\nthe learner about appropriate intermediate concepts. Interestingly, the task we used in our\\nexperiments is not only hard for deep neural networks but also for non-parametric machine\\nlearning algorithms such as SVM's, boosting and decision trees.\\nThe result of the experiments for varying size of dataset with several o\\x0b-the-shelf black box\\nmachine learning algorithms and some popular deep learning algorithms are provided in Table\\n1. The detailed explanations about the algorithms and the hyperparameters used for those\\nalgorithms are given in the Appendix Section 5.2. We also provide some explanations about\\nthe methodologies conducted for the experiments at Section 3.2.\\n3.1 Pentomino Dataset\\nIn order to test our hypothesis, an arti\\x0ccial dataset for object recognition using 64 \\x0264 binary\\nimages is designed5. If the task is two tiered (i.e., with guidance provided), the task in the\\n\\x0crst part is to recognize and locate each Pentomino object class6in the image. The second\\n4. keeping in mind that humans can exploit prior knowledge, either from previous learning or innate knowledge.\",\n",
       " \"4. keeping in mind that humans can exploit prior knowledge, either from previous learning or innate knowledge.\\n5. The source code for the script that generates the arti\\x0ccial Pentomino datasets (Arcade-Universe) is available\\nat:https://github.com/caglar/Arcade-Universe . This implementation is based on Olivier Breuleux's\\nbugland dataset generator.\\n6. A human learner does not seem to need to be taught the shape categories of each Pentomino sprite in order\\nto solve the task. On the other hand, humans have lots of previously learned knowledge about the notion of\\nshape and how central it is in de\\x0cning categories.\\n6\\n(a) sprites, not all same type\\n (b) sprites, all of same type\\nFigure 1: Left (a): An example image from the dataset which has a di\\x0berent sprite type in it.\\nRight (b): An example image from the dataset that has only one type of Pentomino\\nobject in it, but with di\\x0berent orientations and scales.\\npart/\\x0cnal binary classi\\x0ccation task is to \\x0cgure out if all the Pentominos in the image are of\\nthe same shape class or not. If a neural network learned to detect the categories of each object\\nat each location in an image, the remaining task becomes an XOR-like operation between the\",\n",
       " 'the same shape class or not. If a neural network learned to detect the categories of each object\\nat each location in an image, the remaining task becomes an XOR-like operation between the\\ndetected object categories. The types of Pentomino objects that is used for generating the\\ndataset are as follows:\\nPentomino sprites N, P, F, Y, J, and Q, along with the Pentomino N2 sprite (mirror of\\n\\\\Pentomino N\" sprite), the Pentomino F2 sprite (mirror of \\\\Pentomino F\" sprite), and the\\nPentomino Y2 sprite (mirror of \\\\Pentomino Y\" sprite).\\nFigure 2: Di\\x0berent classes of Pentomino shapes used in our dataset.\\nAs shown in Figures 1(a) and 1(b), the synthesized images are fairly simple and do not\\nhave any texture. Foreground pixels are \\\\1\" and background pixels are \\\\0\". Images of the\\ntraining and test sets are generated iid. For notational convenience, assume that the domain\\nof raw input images is X, the set of sprites is S, the set of intermediate object categories is Y\\nfor each possible location in the image and the set of \\x0cnal binary task outcomes is Z=f0;1g.',\n",
       " 'for each possible location in the image and the set of \\x0cnal binary task outcomes is Z=f0;1g.\\nTwo di\\x0berent types of rigid body transformation is performed: sprite rotation rot(X;\\r) where\\n\\x00 =f\\r: (\\r= 90\\x02\\x1e)^[(\\x1e2N);(0\\x14\\x1e\\x143)]gand scaling scale (X;\\x0b) where\\x0b2f1;2gis the\\nscaling factor. The data generating procedure is summarized below.\\nSprite transformations: Before placing the sprites in an empty image, for each image x2X,\\na value for z2Zis randomly sampled which is to have (or not) the same three sprite\\nshapes in the image. Conditioned on the constraint given by z, three sprites are randomly\\n7\\nselectedsijfromSwithout replacement. Using a uniform probability distribution over\\nall possible scales, a scale is chosen and accordingly each sprite image is scaled. Then\\nrotate each sprite is randomly rotated by a multiple of 90 degrees.\\nSprite placement: Upon completion of sprite transformations, a 64 \\x0264 uniform grid is generated which is divided into 8 \\x028 blocks, each block being of size 8 \\x028 pixels, and randomly\\nselect three di\\x0berent blocks from the 64=8 \\x028 on the grid and place the transformed',\n",
       " 'select three di\\x0berent blocks from the 64=8 \\x028 on the grid and place the transformed\\nobjects into di\\x0berent blocks (so they cannot overlap, by construction).\\nEach sprite is centered in the block in which it is located. Thus there is no object translation\\ninside the blocks. The only translation invariance is due to the location of the block inside the\\nimage.\\nA Pentomino sprite is guaranteed to not over\\row the block in which it is located, and there\\nare no collisions or overlaps between sprites, making the task simpler. The largest possible\\nPentomino sprite can be \\x0ct into an 8 \\x024 mask.\\n3.2 Learning Algorithms Evaluated\\nInitially the models are cross-validated by using 5-fold cross-validation. With 40,000 examples,\\nthis gives 32,000 examples for training and 8,000 examples for testing. For neural network\\nalgorithms, stochastic gradient descent (SGD) is used for training. The following standard\\nlearning algorithms were \\x0crst evaluated: decision trees, SVMs with Gaussian kernel, ordinary\\nfully-connected Multi-Layer Perceptrons, Random Forests, k-Nearest Neighbors, Convolutional\\nNeural Networks, and Stacked Denoising Auto-Encoders with supervised \\x0cne-tuning. More',\n",
       " 'Neural Networks, and Stacked Denoising Auto-Encoders with supervised \\x0cne-tuning. More\\ndetails of the con\\x0cgurations and hyper-parameters for each of them are given in Appendix\\nSection 5.2. The only better than chance results were obtained with variations of the Structured\\nMulti-Layer Perceptron described below.\\n3.2.1 Structured Multi-Layer Perceptron (SMLP)\\nThe neural network architecture that is used to solve this task is called the SMLP (Structured\\nMulti-Layer Perceptron), a deep neural network with two parts as illustrated in Figure 5 and\\n7:\\nThe lower part, P1NN ( Part 1 Neural Network , as it is called in the rest of the paper),\\nhas shared weights and local connectivity, with one identical MLP instance of the P1NN for\\neach patch of the image, and typically an 11-element output vector per patch (unless otherwise\\nnoted). The idea is that these 11 outputs per patch could represent the detection of the sprite\\nshape category (or the absence of sprite in the patch). The upper part, P2NN ( Part 2 Neural\\nNetwork ) is a fully connected one hidden layer MLP that takes the concatenation of the outputs',\n",
       " 'Network ) is a fully connected one hidden layer MLP that takes the concatenation of the outputs\\nof all patch-wise P1NNs as input. Note that the \\x0crst layer of P1NN is similar to a convolutional\\nlayer but where the stride equals the kernel size, so that windows do not overlap, i.e., P1NN can\\nbe decomposed into separate networks sharing the same parameters but applied on di\\x0berent\\npatches of the input image, so that each network can actually be trained patch-wise in the case\\nwhere a target is provided for the P1NN outputs. The P1NN output for patch piwhich is\\nextracted from the image xis computed as follows:\\nf\\x12(pi) =g2(Vg1(Upi+b) +c) (1)\\n8\\nwhere pi2Rdis the input patch/receptive \\x0celd extracted from location iof a single image.\\nU2Rdh\\x02dis the weight matrix for the \\x0crst layer of P1NN and b2Rd\\nhis the vector of biases\\nfor the \\x0crst layer of P1NN. g1(\\x01) is the activation function of the \\x0crst layer and g2(\\x01) is the\\nactivation function of the second layer. In many of the experiments, best results were obtained',\n",
       " \"activation function of the second layer. In many of the experiments, best results were obtained\\nwithg1(\\x01) a rectifying non-linearity (a.k.a. as RELU), which is max(0; X) (Jarrett et al.,\\n2009b; Nair and Hinton, 2010; Glorot et al., 2011a; Krizhevsky et al., 2012). V2Rdh\\x02dois the\\nsecond layer's weights matrix, such that and c2Rdoare the biases of the second layer of the\\nP1NN, with doexpected to be smaller than dh.\\nIn this way, g1(Upi+b) is an overcomplete representation of the input patch that can\\npotentially represent all the possible Pentomino shapes for all factors of variations in the patch\\n(rotation, scaling and Pentomino shape type). On the other hand, when trained with hints,\\nf\\x12(pi) is expected to be the lower dimensional representation of a Pentomino shape category\\ninvariant to scaling and rotation in the given patch.\\nIn the experiments with SMLP trained with hints (targets at the output of P1NN), the\\nP1NN is expected to perform classi\\x0ccation of each 8 \\x028 non-overlapping patches of the original\",\n",
       " 'P1NN is expected to perform classi\\x0ccation of each 8 \\x028 non-overlapping patches of the original\\n64\\x0264 input image without having any prior knowledge of whether that speci\\x0cc patch contains\\na Pentomino shape or not. P1NN in SMLP without hints just outputs the local activations for\\neach patch, and gradients on f\\x12(pi) are backpropagated from the upper layers. In both cases\\nP1NN produces the input representation for the Part 2 Neural Net (P2NN). Thus the input\\nrepresentation of P2NN is the concatenated output of P1NN across all the 64 patch locations:\\nho= [f\\x12(p0);:::;f\\x12(pi);:::;f\\x12(pN))] whereNis the number of patches and the ho2Rdi;di=\\ndo\\x02N.hois the concatenated output of the P1NN at each patch.\\nThere is a standardization layer on top of the output of P1NN that centers the activations\\nand performs divisive normalization by dividing by the standard deviation over a minibatch\\nof the activations of that layer. We denote the standardization function z(\\x01). Standardization\\nmakes use of the mean and standard deviation computed for each hidden unit such that each',\n",
       " 'of the activations of that layer. We denote the standardization function z(\\x01). Standardization\\nmakes use of the mean and standard deviation computed for each hidden unit such that each\\nhidden unit of howill have 0 activation and unit standard deviation on average over the\\nminibatch. Xis the set of pentomino images in the minibatch, where X2Rdin\\x02Nis a matrix\\nwithNimages.h(i)\\no(xj) is the vector of activations of the i-th hidden unit of hidden layer\\nho(xj) for thej-th example, with xj2X.\\n\\x16h(i)\\no=1\\nNX\\nxj2Xh(i)\\no(xj) (2)\\n\\x1bh(i)\\no=sPN\\nj(h(i)\\no(xj)\\x00\\x16h(i)\\no)2\\nN+\\x0f (3)\\nz(h(i)\\no(xj)) =h(i)\\no(xj)\\x00\\x16h(i)\\no\\nmax(\\x1bh(i)\\no;\\x0f)(4)\\nwhere\\x0fis a very small constant, that is used to prevent numerical under\\rows in the standard\\ndeviation. P1NN is trained on each 8 \\x028 patches extracted from the image.',\n",
       " 'deviation. P1NN is trained on each 8 \\x028 patches extracted from the image.\\nhois standardized for each training and test sample separately. Di\\x0berent values of \\x0fwere\\nused for SMLP-hints and SMLP-nohints.\\nThe concatenated output of P1NN is fed as an input to the P2NN. P2NN is a feedforward\\nMLP with a sigmoid output layer using a single RELU hidden layer. The task of P2NN is to\\nperform a nonlinear logical operation on the representation provided at the output of P1NN.\\n9\\n3.2.2 Structured Multi Layer Perceptron Trained with Hints (SMLP-hints)\\nThe SMLP-hints architecture exploits a hint about the presence and category of Pentomino\\nobjects, specifying a semantics for the P1NN outputs. P1NN is trained with the intermediate\\ntargetY, specifying the type of Pentomino sprite shape present (if any) at each of the 64 patches\\n(8\\x028 non-overlapping blocks) of the image. Because a possible answer at a given location can\\nbe \\\\none of the object types\" i.e., an empty patch, yp(for patchp) can take one of the 11',\n",
       " 'be \\\\none of the object types\" i.e., an empty patch, yp(for patchp) can take one of the 11\\npossible values, 1 for rejection and the rest is for the Pentomino shape classes, illustrated in\\nFigure 2:\\nyp=(\\n0 if patch pis empty\\ns2Sif the patch pcontains a Pentomino sprite :\\nA similar task has been studied by Fleuret et al. (2011) (at SI appendix Problem 17), who\\ncompared the performance of humans vs computers.\\nThe SMLP-hints architecture takes advantage of dividing the task into two subtasks during\\ntraining with prior information about intermediate-level relevant factors. Because the sum of\\nthe training losses decomposes into the loss on each patch, the P1NN can be pre-trained patchwise. Each patch-speci\\x0cc component of the P1NN is a fully connected MLP with 8 \\x028 inputs\\nand 11 outputs with a softmax output layer. SMLP-hints uses the the standardization given\\nin Equation 3 but with \\x0f= 0.\\nThe standardization is a crucial step for training the SMLP on the Pentomino dataset, and\\nyields much sparser outputs, as seen on Figures 3 and 4. If the standardization is not used,',\n",
       " 'yields much sparser outputs, as seen on Figures 3 and 4. If the standardization is not used,\\neven SMLP-hints could not solve the Pentomino task. In general, the standardization step\\ndampens the small activations and augments larger ones(reducing the noise). Centering the\\nactivations of each feature detector in a neural network has been studied in (Raiko et al., 2012)\\nand (Vatanen et al., 2013). They proposed that transforming the outputs of each hidden neuron\\nin a multi-layer perceptron network to have zero output and zero slope on average makes \\x0crst\\norder optimization methods closer to the second order techniques.\\nBy default, the SMLP uses recti\\x0cer hidden units as activation function, we found a signi\\x0ccant boost by using recti\\x0ccation compared to hyperbolic tangent and sigmoid activation\\nfunctions. The P1NN has a highly overcomplete architecture with 1024 hidden units per patch,\\nand L1 and L2 weight decay regularization coe\\x0ecients on the weights (not the biases) are respectively 1e-6 and 1e-5. The learning rate for the P1NN is 0.75. 1 training epoch was enough',\n",
       " 'for the P1NN to learn the features of Pentomino shapes perfectly on the 40000 training examples. The P2NN has 2048 hidden units. L1 and L2 penalty coe\\x0ecients for the P2NN are\\n1e-6, and the learning rate is 0.1. These were selected by trial and error based on validation\\nset error. Both P1NN (for each patch) and P2NN are fully-connected neural networks, even\\nthough P1NN globally is a special kind of convolutional neural network.\\nFilters of the \\x0crst layer of SMLP are shown in Figure 6. These are the examples of the\\n\\x0clters obtained with the SLMP-hints trained with 40k examples, whose results are given in\\nTable 1. Those \\x0clters look very noisy but they work perfectly on the Pentomino task.\\n3.2.3 Deep and Structured Supervised MLP without Hints (SMLP-nohints)\\nSMLP-nohints uses the same connectivity pattern (and deep architecture) that is also used\\nin the SMLP-hints architecture, but without using the intermediate targets ( Y). It directly\\npredicts the \\x0cnal outcome of the task ( Z), using the same number of hidden units, the same\\n10',\n",
       " 'predicts the \\x0cnal outcome of the task ( Z), using the same number of hidden units, the same\\n10\\nFigure 3: Bar chart of concatenated softmax output activations hoof P1NN (11\\x0264=704 outputs) in SMLP-hints before standardization, for a selected example. There are very\\nlarge spikes at each location for one of the possible 11 outcome (1 of K representation).\\n11\\nFigure 4: Softmax output activations hoof P1NN at SMLP-hints before standardization. There\\nare positive spiked outputs at the locations where there is a Pentomino shape. Positive and negative spikes arise because most of the outputs are near an average value.\\nActivations are higher at the locations where there is a pentomino shape.\\n12\\nStructured MLP \\nArchitecture with \\nHints\\nFinal Binary task \\nlabels\\nIntermediate level \\ntargets .\\nSecond \\nLevel \\nNeural \\nNetwork\\nFirst \\nLevel \\nNeural \\nNetwork\\nFigure 5: Structured MLP architecture, used with hints (trained in two phases, \\x0crst P1NN,\\nbottom two layers, then P2NN, top two layers). In SMLP-hints, P1NN is trained',\n",
       " 'bottom two layers, then P2NN, top two layers). In SMLP-hints, P1NN is trained\\non each 8x8 patch extracted from the image and the softmax output probabilities of\\nall 64 patches are concatenated into a 64 \\x0211 vector that forms the input of P2NN.\\nOnlyUandVare learned in the P1NN and its output on each patch is fed into\\nP2NN. The \\x0crst level and the second level neural networks are trained separately,\\nnot jointly.\\nFigure 6: Filters of Structured MLP architecture, trained with hints on 40k examples.\\n13\\nconnectivity and the same activation function for the hidden units as SMLP-hints. 120 hyperparameter values have been evaluated by randomly selecting the number of hidden units\\nfrom [64;128;256;512;1024;1200;2048] and randomly sampling 20 learning rates uniformly in\\nthe log-domain within the interval of [0 :008;0:8]. Two fully connected hidden layers with 1024\\nhidden units (same as P1NN) per patch is used and 2048 (same as P2NN) for the last hidden layer, with twenty training epochs. For this network the best results are obtained with a\\nlearning rate of 0.05.7\\nStructured MLP \\nArchitecture without \\nHints\\nFinal Binary task',\n",
       " 'learning rate of 0.05.7\\nStructured MLP \\nArchitecture without \\nHints\\nFinal Binary task \\nlabels\\nSecond \\nLevel \\nNeural \\nNetwork\\nFirst \\nLevel \\nNeural \\nNetwork\\nFigure 7: Structured MLP architecture, used without hints (SMLP-nohints). It is the same\\narchitecture as SMLP-hints (Figure 5) but with both parts (P1NN and P2NN) trained\\njointly with respect to the \\x0cnal binary classi\\x0ccation task.\\nWe chose to experiment with various SMLP-nohint architectures and optimization procedures, trying unsuccessfully to achieve as good results with SMLP-nohint as with SMLP-hints.\\nRecti\\x0cer Non-Linearity A recti\\x0cer nonlinearity is used for the activations of MLP hidden\\nlayers. We observed that using piecewise linear nonlinearity activation function such as the\\nrecti\\x0cer can make the optimization more tractable.\\n7. The source code of the structured MLP is available at the github repository: https://github.com/caglar/\\nstructured_mlp\\n14',\n",
       " '7. The source code of the structured MLP is available at the github repository: https://github.com/caglar/\\nstructured_mlp\\n14\\nFigure 8: First layer \\x0clters learned by the Structured MLP architecture, trained without using hints on 447600 examples with online SGD and a sigmoid intermediate layer\\nactivation.\\nIntermediate Layer The output of the P1NN is considered as an intermediate layer of the\\nSMLP. For the SMLP-hints, only softmax output activations have been tried at the intermediate\\nlayer, and that su\\x0eced to learn the task. Since things did not work nearly as well with\\nthe SMLP-nohints, several di\\x0berent activation functions have been tried: softmax( \\x01), tanh(\\x01),\\nsigmoid(\\x01) and linear activation functions.\\nStandardization Layer Normalization at the last layer of the convolutional neural networks\\nhas been used occasionaly to encourage the competition between the hidden units. (Jarrett\\net al., 2009a) used a local contrast normalization layer in their architecture which performs\\nsubtractive and divisive normalization. A local contrast normalization layer enforces a local\\ncompetition between adjacent features in the feature map and between features at the same',\n",
       " 'subtractive and divisive normalization. A local contrast normalization layer enforces a local\\ncompetition between adjacent features in the feature map and between features at the same\\nspatial location in di\\x0berent feature maps. Similarly (Krizhevsky et al., 2012) observed that\\nusing a local response layer that enjoys the bene\\x0ct of using local normalization scheme aids\\ngeneralization.\\nStandardization has been observed to be crucial for both SMLP trained with or without hints. In both SMLP-hints and SMLP-nohints experiments, the neural network was not\\nable to generalize or even learn the training set without using standardization in the SMLP\\nintermediate layer, doing just chance performance. More speci\\x0ccally, in the SMLP-nohints architecture, standardization is part of the computational graph, hence the gradients are being\\nbackpropagated through it. The mean and the standard deviation is computed for each hidden\\nunit separately at the intermediate layer as in Equation 4. But in order to prevent numerical\\nunder\\rows or over\\rows during the backpropagation we have used \\x0f= 1e\\x008 (Equation 3).\\nThe bene\\x0ct of having sparse activations may be speci\\x0ccally important for the ill-conditioned',\n",
       " 'The bene\\x0ct of having sparse activations may be speci\\x0ccally important for the ill-conditioned\\nproblems, for the following reasons. When a hidden unit is \\\\o\\x0b\", its gradient (the derivative of\\nthe loss with respect to its output) is usually close to 0 as well, as seen here. That means that all\\no\\x0b-diagonal second derivatives involving that hidden unit (e.g. its input weights) are also near 0.\\nThis is basically like removing some columns and rows from the Hessian matrix associated with\\na particular example. It has been observed that the condition number of the Hessian matrix\\n(speci\\x0ccally, its largest eigenvalue) increases as the size of the network increases (Dauphin and\\nBengio, 2013), making training considerably slower and ine\\x0ecient (Dauphin and Bengio, 2013).\\nHence one would expect that as sparsity of the gradients (obtained because of sparsity of the\\nactivations) increases, training would become more e\\x0ecient, as if we were training a smaller\\nsub-network for each example, with shared weights across examples, as in dropouts (Hinton\\net al., 2012).\\nIn Figure 9, the activation of each hidden unit in a bar chart is shown: the e\\x0bect of\\nstandardization is signi\\x0ccant, making the activations sparser.',\n",
       " 'In Figure 9, the activation of each hidden unit in a bar chart is shown: the e\\x0bect of\\nstandardization is signi\\x0ccant, making the activations sparser.\\n15\\n(a) Before standardization.\\n (b) After standardization.\\nFigure 9: Activations of the intermediate-level hidden units of an SLMP-nohints for a particular\\nexamples (x-axis: hidden unit number, y-axis: activation value). Left (a): before\\nstandardization. Right (b): after standardization.\\nIn Figure 10, one can see the activation histogram of the SMLP-nohints intermediate layer,\\nshowing the distribution of activation values, before and after standardization. Again the\\nsparsifying e\\x0bect of standardization is very apparent.\\n(a) Before standardization.\\n (b) After standardization.\\nFigure 10: Distribution histogram of activation values of SMLP-nohints intermediate layer.\\nLeft (a): before standardization. Right (b): after standardization.\\nIn Figures 10 and 9, the intermediate level activations of SMLP-nohints are shown before\\nand after standardization. These are for the same SMLP-nohints architecture whose results\\nare presented on Table 1. For that same SMLP, the Adadelta (Zeiler, 2012) adaptive learning\\n16',\n",
       " 'are presented on Table 1. For that same SMLP, the Adadelta (Zeiler, 2012) adaptive learning\\n16\\nrate scheme has been used, with 512 hidden units for the hidden layer of P1NN and recti\\x0cer\\nactivation function. For the output of the P1NN, 11 sigmoidal units have been used while\\nP2NN had 1200 hidden units with recti\\x0cer activation function. The output nonlinearity of the\\nP2NN is a sigmoid and the training objective is the binary crossentropy.\\nAdaptive Learning Rates We have experimented with several di\\x0berent adaptive learning\\nrate algorithms. We tried rmsprop8, Adadelta (Zeiler, 2012), Adagrad (Duchi et al., 2010) and\\na linearly (1/t) decaying learning rate (Bengio, 2013b). For the SMLP-nohints with sigmoid\\nactivation function we have found Adadelta(Zeiler, 2012) converging faster to an e\\x0bective local\\nminima and usually yielding better generalization error compared to the others.\\n3.2.4 Deep and Structured MLP with Unsupervised Pre-Training\\nSeveral experiments have been conducted using an architecture similar to the SMLP-nohints,',\n",
       " '3.2.4 Deep and Structured MLP with Unsupervised Pre-Training\\nSeveral experiments have been conducted using an architecture similar to the SMLP-nohints,\\nbut by using unsupervised pre-training of P1NN, with Denoising Auto-Encoder (DAE) and/or\\nContractive Auto-Encoders (CAE). Supervised \\x0cne-tuning proceeds as in the deep and structured MLP without hints. Because an unsupervised learner may not focus the representation\\njust on the shapes, a larger number of intermediate-level units at the output of P1NN has been\\nexplored: previous work on unsupervised pre-training generally found that larger hidden layers\\nwere optimal when using unsupervised pre-training, because not all unsupervised features will\\nbe relevant to the task at hand. Instead of limiting to 11 units per patch, we experimented\\nwith networks with up to 20 hidden (i.e., code) units per patch in the second-layer patch-wise\\nauto-encoder.\\nIn Appendix 5.1 we also provided the result of some experiments with binary-binary RBMs\\ntrained on 8\\x028 patches from the 40k training dataset.\\nIn unsupervised pretraining experiments in this paper, both contractive auto-encoder (CAE)',\n",
       " 'trained on 8\\x028 patches from the 40k training dataset.\\nIn unsupervised pretraining experiments in this paper, both contractive auto-encoder (CAE)\\nwith sigmoid nonlinearity and binary cross entropy cost function and denoising auto-encoder (DAE)\\nhave been used. In the second layer, experiments were performed with a DAE with recti\\x0cer\\nhidden units utilizing L1 sparsity and weight decay on the weights of the auto-encoder. Greedy\\nlayerwise unsupervised training procedure is used to train the deep auto-encoder architecture\\n(Bengio et al., 2007). In unsupervised pretraining experiments, tied weights have been used.\\nDi\\x0berent combinations of CAE and DAE for unsupervised pretraining have been tested, but\\nnone of the con\\x0cgurations tested managed to learn the Pentomino task, as shown in Table 1.\\n3.3 Experiments with 1 of K representation\\nTo explore the e\\x0bect of changing the complexity of the input representation on the di\\x0eculty\\nof the task, a set of experiments have been designed with symbolic representations of the\\ninformation in each patch. In all cases an empty patch is represented with a 0 vector. These\\nrepresentation can be seen as an alternative input for a P2NN-like network, i.e., they were fed',\n",
       " 'representation can be seen as an alternative input for a P2NN-like network, i.e., they were fed\\nas input to an MLP or another black-box classi\\x0cer.\\nThe following four experiments have been conducted, each one using one using a di\\x0berent\\ninput representation for each patch:\\n8. This is learning rate scaling method that is discussed by G. Hinton in his Video Lecture 6.5 - rmsprop: Divide\\nthe gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine\\nLearning, 2012.\\n17\\nAlgorithm 20k dataset 40k dataset 80k dataset\\nTraining Test Training Test Training Test\\nError Error Error Error Error Error\\nSVM RBF 26.2 50.2 28.2 50.2 30.2 49.6\\nK Nearest Neighbors 24.7 50.0 25.3 49.5 25.6 49.0\\nDecision Tree 5.8 48.6 6.3 49.4 6.9 49.9\\nRandomized Trees 3.2 49.8 3.4 50.5 3.5 49.1\\nMLP 26.5 49.3 33.2 49.9 27.2 50.1\\nConvnet/Lenet5 50.6 49.8 49.4 49.8 50.2 49.8',\n",
       " 'Convnet/Lenet5 50.6 49.8 49.4 49.8 50.2 49.8\\nMaxout Convnet 14.5 49.5 0.0 50.1 0.0 44.6\\n2 layer sDA 49.4 50.3 50.2 50.3 49.7 50.3\\nStruct. Supervised MLP w/o hints 0.0 48.6 0.0 36.0 0.0 12.4\\nStruct. MLP+CAE Supervised Finetuning 50.5 49.7 49.8 49.7 50.3 49.7\\nStruct. MLP+CAE+DAE, Supervised Finetuning 49.1 49.7 49.4 49.7 50.1 49.7\\nStruct. MLP+DAE+DAE, Supervised Finetuning 49.5 50.3 49.7 49.8 50.3 49.7\\nStruct. MLP with Hints 0.21 30.7 0 3.1 0 0.01\\nTable 1: The error percentages with di\\x0berent learning algorithms on Pentomino dataset with\\ndi\\x0berent number of training examples.',\n",
       " 'Table 1: The error percentages with di\\x0berent learning algorithms on Pentomino dataset with\\ndi\\x0berent number of training examples.\\nExperiment 1-Onehot representation without transformations: In this experiment several trials have been done with a 10-input one-hot vector per patch. Each input corresponds to an object category given in clear, i.e., the ideal input for P2NN if a supervised\\nP1NN perfectly did its job.\\nExperiment 2-Disentangled representations: In this experiment, we did trials with 16\\nbinary inputs per patch, 10 one-hot bits for representing each object category, 4 for\\nrotations and 2 for scaling, i.e., the whole information about the input is given, but it is\\nperfectly disentangled. This would be the ideal input for P2NN if an unsupervised P1NN\\nperfectly did its job.\\nExperiment 3-Onehot representation with transformations: For each of the ten object\\ntypes there are 8 = 4 \\x022 possible transformations. Two objects in two di\\x0berent patches\\nare the considered \\\\the same\" (for the \\x0cnal task) if their category is the same regardless\\nof the transformations. The one-hot representation of a patch corresponds to the crossproduct between the 10 object shape classes and the 4 \\x022 transformations, i.e., one out of',\n",
       " \"of the transformations. The one-hot representation of a patch corresponds to the crossproduct between the 10 object shape classes and the 4 \\x022 transformations, i.e., one out of\\n80=10\\x024\\x022 possibilities represented in an 80-bit one-hot vector. This also contains all\\nthe information about the input image patch, but spread out in a kind of non-parametric\\nand non-informative (not disentangled) way, like a perfect memory-based unsupervised\\nlearner (like clustering) could produce. Nevertheless, the shape class would be easier to\\nread out from this representation than from the image representation (it would be an OR\\nover 8 of the bits).\\nExperiment 4-Onehot representation with 80 choices: This representation has the same\\n1 of 80 one-hot representation per patch but the target task is de\\x0cned di\\x0berently. Two objects in two di\\x0berent patches are considered the same i\\x0b they have exactly the same 80-bit\\nonehot representation (i.e., are of the same object category with the same transformation\\napplied).\\nThe \\x0crst experiment is a sanity check. It was conducted with single hidden-layered MLP's\\nwith recti\\x0cer and tanh nonlinearity, and the task was learned perfectly (0 error on both training\\nand test dataset) with very few training epochs.\\n18\",\n",
       " 'with recti\\x0cer and tanh nonlinearity, and the task was learned perfectly (0 error on both training\\nand test dataset) with very few training epochs.\\n18\\n0 100 200 300 400 500 600 700 800 9000.00.10.20.30.40.50.6\\nTraining Error Rate\\nTest Error Rate(a) Training and Test Errors for Experiment 4\\n0 100 200 300 400 500 600 700 8000.00.10.20.30.40.50.6\\nTraining Error Rate\\nTest Error Rate (b) Training and Test Errors for Experiment 3\\nFigure 11: Tanh MLP training curves. Left (a): The training and test errors of Experiment\\n3 over 800 training epochs with 100k training examples using Tanh MLP. Right\\n(b):The training and test errors of Experiment 4 over 700 training epochs with 100k\\ntraining examples using Tanh MLP.\\nThe results of Experiment 2 are given in Table 2. To improve results, we experimented\\nwith the Maxout non-linearity in a feedforward MLP (Goodfellow et al., 2013) with two hidden\\nlayers. Unlike the typical Maxout network mentioned in the original paper, regularizers have\\nbeen deliberately avoided in order to focus on the optimization issue, i.e: no weight decay, norm\\nconstraint on the weights, or dropout. Although learning from a disentangled representation is',\n",
       " 'constraint on the weights, or dropout. Although learning from a disentangled representation is\\nmore di\\x0ecult than learning from perfect object detectors, it is feasible with some architectures\\nsuch as the Maxout network. Note that this representation is the kind of representation that\\none could hope an unsupervised learning algorithm could discover, at best, as argued in Bengio\\net al. (2012).\\nThe only results obtained on the validation set for Experiment 3 and Experiment 4 are\\nshown respectively in Table 3 and Table 4. In these experiments a tanh MLP with two hidden\\nlayers have been tested with the same hyperparameters. In experiment 3 the complexity of the\\nproblem comes from the transformations (8=4 \\x022) and the number of object types.\\nBut in experiment 4, the only source of complexity of the task comes from the number of\\ndi\\x0berent object types. These results are in between the complete failure and complete success\\nobserved with other experiments, suggesting that the task could become solvable with better\\ntraining or more training examples. Figure 11 illustrates the progress of training a tanh MLP,\\non both the training and test error, for Experiments 3 and 4. Clearly, something has been\\nlearned, but the task is not nailed yet. On experiment 3 for both maxout and tanh the maxout\\nthere was a long plateau where the training error and objective stays almost same. Maxout',\n",
       " 'there was a long plateau where the training error and objective stays almost same. Maxout\\ndid just chance on the experiment for about 120 iterations on the training and the test set.\\nBut after 120th iteration the training and test error started decline and eventually it was able\\nto solve the task. Moreover as seen from the curves in Figure 11(a) and 11(b), the training\\nand test error curves are almost the same for both tasks. This implies that for onehot inputs,\\nwhether you increase the number of possible transformations for each object or the number of\\n19\\nLearning Algorithm Training Error Test Error\\nSVM 0.0 35.6\\nRandom Forests 1.29 40.475\\nTanh MLP 0.0 0.0\\nMaxout MLP 0.0 0.0\\nTable 2: Performance of di\\x0berent learning algorithms on disentangled representation in Experiment 2.\\nLearning Algorithm Training Error Test Error\\nSVM 11.212 32.37\\nRandom Forests 24.839 48.915\\nTanh MLP 0.0 22.475\\nMaxout MLP 0.0 0.0\\nTable 3: Performance of di\\x0berent learning algorithms using a dataset with onehot vector and\\n80 inputs as discussed for Experiment 3.\\nobject categories, as soon as the number of possible con\\x0cgurations is same, the complexity of\\nthe problem is almost the same for the MLP.',\n",
       " 'object categories, as soon as the number of possible con\\x0cgurations is same, the complexity of\\nthe problem is almost the same for the MLP.\\n3.4 Does the E\\x0bect Persist with Larger Training Set Sizes?\\nThe results shown in this section indicate that the problem in the Pentomino task clearly\\nis not just a regularization problem, but rather basically hinges on an optimization problem.\\nOtherwise, we would expect test error to decrease as the number of training examples increases.\\nThis is shown \\x0crst by studying the online case and then by studying the ordinary training\\ncase with a \\x0cxed size training set but considering increasing training set sizes. In the online\\nminibatch setting, parameter updates are performed as follows:\\n\\x12t+1=\\x12t\\x00\\x01\\x12t (5)\\n\\x01\\x12t=\\x0fPN\\nir\\x12tL(xt;\\x12t)\\nN(6)\\nwhereL(xt;\\x12t) is the loss incurred on example xtwith parameters \\x12t, wheret2Z+and\\x0f\\nis the learning rate.\\nOrdinary batch algorithms converge linearly to the optimum \\x12\\x03, however the noisy gradient\\nestimates in the online SGD will cause parameter \\x12to \\ructuate near the local optima. However,\\nonline SGD directly optimizes the expected risk, because the examples are drawn iid from the',\n",
       " 'online SGD directly optimizes the expected risk, because the examples are drawn iid from the\\nground-truth distribution (Bottou, 2010). Thus:\\nL1=E[L(x;\\x12)] =Z\\nxL(x;\\x12)p(x)dx (7)\\n20\\nLearning Algorithm Training Error Test Error\\nSVM 4.346 40.545\\nRandom Forests 23.456 47.345\\nTanh MLP 0 25.8\\nTable 4: Performance of di\\x0berent algorithms using a dataset with onehot vector and 80 binary\\ninputs as discussed in Experiment 4.\\nwhereL1is the generalization error. Therefore online SGD is trying to minimize the\\nexpected risk with noisy updates. Those noisy updates have the e\\x0bect of regularizer:\\n\\x01\\x12t=\\x0fPN\\nir\\x12tL(xt;\\x12t)\\nN=\\x0fr\\x12tL(x;\\x12t) +\\x0f\\x18t (8)\\nwherer\\x12tL(x;\\x12t) is the true gradient and \\x18tis the zero-mean stochastic gradient \\\\noise\"\\ndue to computing the gradient over a \\x0cnite-size minibatch sample.\\nWe would like to know if the problem with the Pentomino dataset is more a regularization',\n",
       " 'due to computing the gradient over a \\x0cnite-size minibatch sample.\\nWe would like to know if the problem with the Pentomino dataset is more a regularization\\nor an optimization problem. An SMLP-nohints model was trained by online SGD with the randomly generated online Pentomino stream. The learning rate was adaptive, with the Adadelta\\nprocedure (Zeiler, 2012) on minibatches of 100 examples. In the online SGD experiments, two\\nSMLP-nohints that is trained with and without standardization at the intermediate layer with\\nexactly the same hyperparameters are tested. The SMLP-nohints P1NN patch-wise submodel\\nhas 2048 hidden units and the SMLP intermediate layer has 1152 = 64 \\x0218 hidden units. The\\nnonlinearity that is used for the intermediate layer is the sigmoid. P2NN has 2048 hidden units.\\nSMLP-nohints has been trained either with or without standardization on top of the output\\nunits of the P1NN. The experiments illustrated in Figures 12 and 13 are with the same SMLP\\nwithout hints architecture for which results are given in Table 1. In those graphs only the results\\nfor the training on the randomly generated 545400 Pentomino samples have been presented. As',\n",
       " \"without hints architecture for which results are given in Table 1. In those graphs only the results\\nfor the training on the randomly generated 545400 Pentomino samples have been presented. As\\nshown in the plots SMLP-nohints was not able to generalize without standardization. Although\\nwithout standardization the training loss seems to decrease initially, it eventually gets stuck in\\na plateau where training loss doesn't change much.\\nTraining of SMLP-nohints online minibatch SGD is performed using standardization in the\\nintermediate layer and Adadelta learning rate adaptation, on 1046000 training examples from\\nthe randomly generated Pentomino stream. At the end of the training, test error is down to\\n27.5%, which is much better than chance but from from the score obtained with SMLP-hints\\nof near 0 error.\\nIn another SMLP-nohints experiment without standardization the model is trained with the\\n1580000 Pentomino examples using online minibatch SGD. P1NN has 2048 hidden units and 16\\nsigmoidal outputs per patch. for the P1NN hidden layer. P2NN has 1024 hidden units for the\\nhidden layer. Adadelta is used to adapt the learning rate. At the end of training this SMLP,\\nthe test error remained stuck, at 50.1%.\\n21\\n0 100 200 300 400 500 600 700 800\",\n",
       " 'the test error remained stuck, at 50.1%.\\n21\\n0 100 200 300 400 500 600 700 800\\nBatch no0.200.250.300.350.400.450.500.55Test Error\\nSMLP with standardization\\nSMLP without standardizationFigure 12: Test errors of SMLP-nohints with and without standardization in the intermediate\\nlayer. Sigmoid as an intermediate layer activation has been used. Each tick (batch\\nno) in the x-axis represents 400 examples.\\n22\\n0 100 200 300 400 500 600 700\\nBatch no01234567Training LossSMLP with standardization\\nSMLP without standardizationFigure 13: Training errors of SMLP-nohints with and without standardization in the intermediate layer. Sigmoid nonlinearity has been used as an intermediate layer activation\\nfunction. The x-axis is in units of blocks of 400 examples in the training set.\\n23\\n3.4.1 Experiments with Increased Training Set Size\\nHere we consider the e\\x0bect of training di\\x0berent learners with di\\x0berent numbers of training\\nexamples. For the experimental results shown in Table 1, 3 training set sizes (20k, 40k and 80k\\nexamples) had been used. Each dataset was generated with di\\x0berent random seeds (so they',\n",
       " 'examples) had been used. Each dataset was generated with di\\x0berent random seeds (so they\\ndo not overlap). Figure 14 also shows the error bars for an ordinary MLP with three hidden\\nlayers, for a larger range of training set sizes, between 40k and 320k examples. The number of\\ntraining epochs is 8 (more did not help), and there are three hidden layers with 2048 feature\\ndetectors. The learning rate we used in our experiments is 0.01. The activation function of the\\nMLP is a tanh nonlinearity, while the L1, L2 penalty coe\\x0ecients are both 1e-6.\\nTable 1 shows that, without guiding hints, none of the state-of-art learning algorithms could\\nperform noticeably better than a random predictor on the test set. This shows the importance\\nof intermediate hints introduced in the SMLP. The decision trees and SVMs can over\\x0ct the\\ntraining set but they could not generalize on the test set. Note that the numbers reported in the\\ntable are for hyper-parameters selected based on validation set error, hence lower training errors\\nare possible if avoiding all regularization and taking large enough models. On the training set,\\nthe MLP with two large hidden layers (several thousands) could reach nearly 0% training error,\\nbut still did not manage to achieve good test error.',\n",
       " 'the MLP with two large hidden layers (several thousands) could reach nearly 0% training error,\\nbut still did not manage to achieve good test error.\\nIn the experiment results shown in Figure 14, we evaluate the impact of adding more training\\ndata for the fully-connected MLP. As mentioned before for these experiments we have used a\\nMLP with three hidden layers where each layer has 2048 hidden units. The tanh( \\x01) activation\\nfunction is used with 0.05 learning rate and minibatches of size 200.\\nAs can be seen from the \\x0cgure, adding more training examples did not help either training\\nor test error (both are near 50%, with training error slightly lower and test error slightly\\nhigher), reinforcing the hypothesis that the di\\x0ecult encountered is one of optimization, not of\\nregularization.\\nFigure 14: Training and test error bar charts for a regular MLP with 3 hidden layers. There\\nis no signi\\x0ccant improvement on the generalization error of the MLP as the new\\ntraining examples are introduced.\\n24\\n3.5 Experiments on E\\x0bect of Initializing with Hints\\nInitialization of the parameters in a neural network can have a big impact on the learning\\nand generalization (Glorot and Bengio, 2010). Previously Erhan et al. (2010) showed that',\n",
       " 'and generalization (Glorot and Bengio, 2010). Previously Erhan et al. (2010) showed that\\ninitializing the parameters of a neural network with unsupervised pretraining guides the learning towards basins of attraction of local minima that provides better generalization from the\\ntraining dataset. In this section we analyze the e\\x0bect of initializing the SMLP with hints and\\nthen continuing without hints at the rest of the training. For experimental analysis of hints\\nbased initialization, SMLP is trained for 1 training epoch using the hints and for 60 epochs it\\nis trained without hints on the 40k examples training set. We also compared the same architecture with the same hyperparameters, against to SMLP-nohints trained for 61 iterations on\\nthe same dataset. After one iteration of hint-based training SMLP obtained 9% training error\\nand 39% test error. Following the hint based training, SMLP is trained without hints for 60\\nepochs, but at epoch 18, it already got 0% training and 0% test error. The hyperparameters\\nfor this experiment and the experiment that the results shown for the SMLP-hints in Table 1\\nare the same. The test results for initialization with and without hints are shown on Figure 15.\\nThis \\x0cgure suggests that initializing with hints can give the same generalization performance\\nbut training takes longer.',\n",
       " 'This \\x0cgure suggests that initializing with hints can give the same generalization performance\\nbut training takes longer.\\nFigure 15: Plots showing the test error of SMLP with random initialization vs initializing with\\nhint based training.\\n3.5.1 Further Experiments on Optimization for Pentomino Dataset\\nWith extensive hyperparameter optimization and using standardization in the intermediate\\nlevel of the SMLP with softmax nonlinearity, SMLP-nohints was able to get 5.3% training and\\n25\\n6.7% test error on the 80k Pentomino training dataset. We used the 2050 hidden units for the\\nhidden layer of P1NN and 11 softmax output per patch. For the P2NN, we used 1024 hidden\\nunits with sigmoid and learning rate 0.1 without using any adaptive learning rate method. This\\nSMLP uses a recti\\x0cer nonlinearity for hidden layers of both P1NN and P2NN. Considering that\\narchitecture uses softmax as the intermediate activation function of SMLP-nohints. It is very\\nlikely that P1NN is trying to learn the presence of speci\\x0cc Pentomino shape in a given patch.\\nThis architecture has a very large capacity in the P1NN, that probably provides it enough',\n",
       " 'This architecture has a very large capacity in the P1NN, that probably provides it enough\\ncapacity to learn the presence of Pentomino shapes at each patch e\\x0bortlessly.\\nAn MLP with 2 hidden layers, each 1024 recti\\x0cer units, was trained using LBFGS (the\\nimplementation from the scipy.optimize library) on 40k training examples, with gradients computed on batches of 10000 examples at each iteration. However, after convergence of training,\\nthe MLP was still doing chance on the test dataset.\\nWe also observed that using linear units for the intermediate layer yields better generalization error without standardization compared to using activation functions such as sigmoid,\\ntanh and RELU for the intermediate layer. SMLP-nohints was able to get 25% generalization error with linear units without standardization whereas all the other activation functions\\nthat has been tested failed to generalize with the same number of training iterations without\\nstandardization and hints. This suggests that using non-linear intermediate-level activation\\nfunctions without standardization introduces an optimization di\\x0eculty for the SMLP-nohints,\\nmaybe because the intermediate level acts like a bottleneck in this architecture.\\n4. Conclusion and Discussion\\nIn this paper we have shown an example of task which seems almost impossible to solve by\\nstandard black-box machine learning algorithms, but can be almost perfectly solved when',\n",
       " '4. Conclusion and Discussion\\nIn this paper we have shown an example of task which seems almost impossible to solve by\\nstandard black-box machine learning algorithms, but can be almost perfectly solved when\\none encourages a semantics for the intermediate-level representation that is guided by prior\\nknowledge. The task has the particularity that it is de\\x0cned by the composition of two nonlinear sub-tasks (object detection on one hand, and a non-linear logical operation similar to\\nXOR on the other hand).\\nWhat is interesting is that in the case of the neural network, we can compare two networks\\nwith exactly the same architecture but a di\\x0berent pre-training, one of which uses the known\\nintermediate concepts to teach an intermediate representation to the network. With enough\\ncapacity and training time they can over\\x0ct but did not not capture the essence of the task, as\\nseen by test set performance.\\nWe know that a structured deep network can learn the task, if it is initialized in the right\\nplace, and do it from very few training examples. Furthermore we have shown that if one\\npre-trains SMLP with hints for only one epoch, it can nail the task. But the exactly same\\narchitecture which started training from random initialization, failed to generalize.\\nConsider the fact that even SMLP-nohints with standardization after being trained using',\n",
       " \"architecture which started training from random initialization, failed to generalize.\\nConsider the fact that even SMLP-nohints with standardization after being trained using\\nonline SGD on 1046000 generated examples and still gets 27.5% test error. This is an indication\\nthat the problem is not a regularization problem but possibly an inability to \\x0cnd a good e\\x0bective\\nlocal minima of generalization error .\\nWhat we hypothesize is that for most initializations and architectures (in particular the\\nfully-connected ones), although it is possible to \\x0cnd a good e\\x0bective local minimum of training\\nerror when enough capacity is provided, it is di\\x0ecult (without the proper initialization) to \\x0cnd a\\ngood local minimum of generalization error. On the other hand, when the network architecture\\n26\\nis constrained enough but still allows it to represent a good solution (such as the structured\\nMLP of our experiments), it seems that the optimization problem can still be di\\x0ecult and even\\ntraining error remains stuck high if the standardization isn't used. Standardization obviously\\nmakes the training objective of the SMLP easier to optimize and helps it to \\x0cnd at least a\\nbetter e\\x0bective local minimum of training error . This \\x0cnding suggests that by using speci\\x0cc\\narchitectural constraints and sometimes domain speci\\x0cc knowledge about the problem, one can\",\n",
       " 'architectural constraints and sometimes domain speci\\x0cc knowledge about the problem, one can\\nalleviate the optimization di\\x0eculty that generic neural network architectures face.\\nIt could be that the combination of the network architecture and training procedure produces a training dynamics that tends to yield into these minima that are poor from the point of\\nview of generalization error, even when they manage to nail training error by providing enough\\ncapacity. Of course, as the number of examples increases, we would expect this discrepancy to\\ndecrease, but then the optimization problem could still make the task unfeasible in practice.\\nNote however that our preliminary experiments with increasing the training set size (8-fold) for\\nMLPs did not reveal signs of potential improvements in test error yet, as shown in Figure 14.\\nEven using online training on 545400 Pentomino examples, the SMLP-nohints architecture was\\nstill doing far from perfect in terms of generalization error (Figure 12).\\nThese \\x0cndings bring supporting evidence to the \\\\Guided Learning Hypothesis\" and \\\\Deeper\\nHarder Hypothesis\" from Bengio (2013a): higher level abstractions, which are expressed by\\ncomposing simpler concepts, are more di\\x0ecult to learn (with the learner often getting in an\\ne\\x0bective local minimum ), but that di\\x0eculty can be overcome if another agent provides hints',\n",
       " 'e\\x0bective local minimum ), but that di\\x0eculty can be overcome if another agent provides hints\\nof the importance of learning other, intermediate-level abstractions which are relevant to the\\ntask.\\nMany interesting questions remain open. Would a network without any guiding hint eventually \\x0cnd the solution with a enough training time and/or with alternate parametrizations?\\nTo what extent is ill-conditioning a core issue? The results with LBFGS were disappointing but\\nchanges in the architectures (such as standardization of the intermediate level) seem to make\\ntraining much easier. Clearly, one can reach good solutions from an appropriate initialization,\\npointing in the direction of an issue with local minima, but it may be that good solutions are\\nalso reachable from other initializations, albeit going through a tortuous ill-conditioned path\\nin parameter space. Why did our attempts at learning the intermediate concepts in an unsupervised way fail? Are these results speci\\x0cc to the task we are testing or a limitation of the\\nunsupervised feature learning algorithm tested? Trying with many more unsupervised variants and exploring explanatory hypotheses for the observed failures could help us answer that.\\nFinally, and most ambitious, can we solve these kinds of problems if we allow a community\\nof learners to collaborate and collectively discover and combine partial solutions in order to\\nobtain solutions to more abstract tasks like the one presented here? Indeed, we would like to',\n",
       " \"of learners to collaborate and collectively discover and combine partial solutions in order to\\nobtain solutions to more abstract tasks like the one presented here? Indeed, we would like to\\ndiscover learning algorithms that can solve such tasks without the use of prior knowledge as\\nspeci\\x0cc and strong as the one used in the SMLP here. These experiments could be inspired by\\nand inform us about potential mechanisms for collective learning through cultural evolutions\\nin human societies.\\nAcknowledgments\\nWe would like to thank to the ICLR 2013 reviewers for their insightful comments, and NSERC,\\nCIFAR, Compute Canada and Canada Research Chairs for funding.\\n27\\nReferences\\nA. Ben-Hur and J. Weston. A user's guide to support vector machines. Methods in Molecular\\nBiology , 609:223{239, 2010.\\nY. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep\\nnetworks. In NIPS'2006 , 2007.\\nYoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine\\nLearning , 2(1):1{127, 2009. Also published as a book. Now Publishers, 2009.\\nYoshua Bengio. Evolving culture vs local minima. In Growing Adaptive Machines: Integrating\",\n",
       " \"Yoshua Bengio. Evolving culture vs local minima. In Growing Adaptive Machines: Integrating\\nDevelopment and Learning in Arti\\x0ccial Neural Networks , number also as ArXiv 1203.2990v1,\\npages T. Kowaliw, N. Bredeche & R. Doursat, eds. Springer-Verlag, March 2013a. URL\\nhttp://arxiv.org/abs/1203.2990 .\\nYoshua Bengio. Practical recommendations for gradient-based training of deep architectures.\\nIn K.-R. M\\x7f uller, G. Montavon, and G. B. Orr, editors, Neural Networks: Tricks of the Trade .\\nSpringer, 2013b.\\nYoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.\\nIn L\\x13 eon Bottou and Michael Littman, editors, Proceedings of the Twenty-sixth International\\nConference on Machine Learning (ICML'09) . ACM, 2009a.\\nYoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.\\nInICML'09 , 2009b.\\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Unsupervised feature learning and deep\",\n",
       " \"InICML'09 , 2009b.\\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Unsupervised feature learning and deep\\nlearning: A review and new perspectives. Technical Report arXiv:1206.5538, U. Montreal,\\n2012. URL http://arxiv.org/abs/1206.5538 .\\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Unsupervised feature learning and deep\\nlearning: A review and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI) , 2013.\\nJames Bergstra, Olivier Breuleux, Fr\\x13 ed\\x13 eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU\\nand GPU math expression compiler. In Proceedings of the Python for Scienti\\x0cc Computing\\nConference (SciPy) , 2010.\\nL\\x13 eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of\\nCOMPSTAT'2010 , pages 177{186. Springer, 2010.\\nLeo Breiman. Random forests. Machine Learning , 45(1):5{32, 2001.\",\n",
       " \"COMPSTAT'2010 , pages 177{186. Springer, 2010.\\nLeo Breiman. Random forests. Machine Learning , 45(1):5{32, 2001.\\nD. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets\\nfor handwritten digit recognition. Neural Computation , 22:1{14, 2010.\\nYann Dauphin and Yoshua Bengio. Big neural networks waste capacity. Technical Report\\narXiv:1301.3583, Universite de Montreal, 2013.\\nRichard Dawkins. The Sel\\x0csh Gene . Oxford University Press, 1976.\\n28\\nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\\nstochastic optimization. Journal of Machine Learning Research , 12:2121{2159, 2010.\\nDumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent,\\nand Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of\\nMachine Learning Research , 11:625{660, February 2010.\\nFran\\x18 cois Fleuret, Ting Li, Charles Dubout, Emma K Wampler, Steven Yantis, and Donald\",\n",
       " 'Fran\\x18 cois Fleuret, Ting Li, Charles Dubout, Emma K Wampler, Steven Yantis, and Donald\\nGeman. Comparing machines and humans on a visual categorization test. Proceedings of the\\nNational Academy of Sciences , 108(43):17621{17625, 2011.\\nX. Glorot, A. Bordes, and Y. Bengio. Deep sparse recti\\x0cer neural networks. In AISTATS ,\\n2011a.\\nXavier Glorot and Yoshua Bengio. Understanding the di\\x0eculty of training deep feedforward\\nneural networks. In JMLR W&CP: Proceedings of the Thirteenth International Conference\\non Arti\\x0ccial Intelligence and Statistics (AISTATS 2010) , volume 9, pages 249{256, May\\n2010.\\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse recti\\x0cer neural networks. In\\nJMLR W&CP: Proceedings of the Fourteenth International Conference on Arti\\x0ccial Intelligence and Statistics (AISTATS 2011) , April 2011b.\\nIan J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\\nMaxout networks. In ICML , 2013.',\n",
       " 'Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\\nMaxout networks. In ICML , 2013.\\nMaciej Henneberg. Decrease of human skull size in the holocene. Human biology , pages 395{405,\\n1988.\\nMaciej Henneberg and Maryna Steyn. Trends in cranial capacity and cranial index in subsaharan africa during the holocene. American journal of human biology , 5(4):473{479, 1993.\\nJ. Henrich and R. McElreath. The evolution of cultural evolution. Evolutionary Anthropology:\\nIssues, News, and Reviews , 12(3):123{135, 2003.\\nGeo\\x0brey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep\\nbelief nets. Neural Computation , 18:1527{1554, 2006.\\nGeo\\x0brey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. Technical\\nreport, arXiv:1207.0580, 2012.',\n",
       " \"report, arXiv:1207.0580, 2012.\\nC.W. Hsu, C.C. Chang, C.J. Lin, et al. A practical guide to support vector classi\\x0ccation, 2003.\\nKevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato, and Yann LeCun. What is the\\nbest multi-stage architecture for object recognition? In Proc. International Conference on\\nComputer Vision (ICCV'09) , pages 2146{2153. IEEE, 2009a.\\nKevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato, and Yann LeCun. What is the best\\nmulti-stage architecture for object recognition? In ICCV'09 , 2009b.\\n29\\nFaisal Khan, Xiaojin Zhu, and Bilge Mutlu. How do humans teach: On curriculum learning and\\nteaching dimension. In Advances in Neural Information Processing Systems 24 (NIPS'11) ,\\npages 1449{1457, 2011.\\nAlex Krizhevsky, Ilya Sutskever, and Geo\\x0brey Hinton. ImageNet classi\\x0ccation with deep\\nconvolutional neural networks. In Advances in Neural Information Processing Systems 25\\n(NIPS'2012) . 2012.\",\n",
       " \"convolutional neural networks. In Advances in Neural Information Processing Systems 25\\n(NIPS'2012) . 2012.\\nKai A. Krueger and Peter Dayan. Flexible shaping: how learning in small steps helps. Cognition ,\\n110:380{394, 2009.\\nG. Kunapuli, K.P. Bennett, R. Maclin, and J.W. Shavlik. The adviceptron: Giving advice to\\nthe perceptron. Proceedings of the Conference on Arti\\x0ccial Neural Networks In Engineering\\n(ANNIE 2010) , 2010.\\nHugo Larochelle, Yoshua Bengio, Jerome Louradour, and Pascal Lamblin. Exploring strategies\\nfor training deep neural networks. Journal of Machine Learning Research , 10:1{40, 2009.\\nY. LeCun, L. Bottou, Y. Bengio, and P. Ha\\x0bner. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE , 86(11):2278{2324, 1998.\\nT.M. Mitchell. The need for biases in learning generalizations . Department of Computer\\nScience, Laboratory for Computer Science Research, Rutgers Univ., 1980.\\nT.M. Mitchell and S.B. Thrun. Explanation-based neural network learning for robot control.\\nAdvances in Neural information processing systems , pages 287{287, 1993.\",\n",
       " \"T.M. Mitchell and S.B. Thrun. Explanation-based neural network learning for robot control.\\nAdvances in Neural information processing systems , pages 287{287, 1993.\\nR. Montague. Universal grammar. Theoria , 36(3):373{398, 1970.\\nV. Nair and G. E Hinton. Recti\\x0ced linear units improve restricted Boltzmann machines. In\\nICML'10 , 2010.\\nL.B.J.H.F.R.A. Olshen and C.J. Stone. Classi\\x0ccation and regression trees. Belmont, Calif.:\\nWadsworth , 1984.\\nJoseph O'Sullivan. Integrating initialization bias and search bias in neural network learning,\\n1996.\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\\nP. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. The\\nJournal of Machine Learning Research , 12:2825{2830, 2011.\\nGail B. Peterson. A day of great illumination: B. F. Skinner's discovery of shaping. Journal of\\nthe Experimental Analysis of Behavior , 82(3):317{328, 2004.\",\n",
       " \"the Experimental Analysis of Behavior , 82(3):317{328, 2004.\\nTapani Raiko, Harri Valpola, and Yann LeCun. Deep learning made easier by linear transformations in perceptrons. In International Conference on Arti\\x0ccial Intelligence and Statistics ,\\npages 924{932, 2012.\\nSalah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive\\nauto-encoders: Explicit invariance during feature extraction. In ICML'2011 , 2011.\\n30\\nSalah Rifai, Yoshua Bengio, Yann Dauphin, and Pascal Vincent. A generative process for sampling contractive auto-encoders. In Proceedings of the Twenty-nine International Conference\\non Machine Learning (ICML'12) . ACM, 2012. URL http://icml.cc/discuss/2012/590.\\nhtml .\\nR. Salakhutdinov and G.E. Hinton. Deep Boltzmann machines. In Proceedings of the Twelfth\\nInternational Conference on Arti\\x0ccial Intelligence and Statistics (AISTATS 2009) , volume 8,\\n2009.\\nBurrhus F. Skinner. Reinforcement today. American Psychologist , 13:94{99, 1958.\",\n",
       " '2009.\\nBurrhus F. Skinner. Reinforcement today. American Psychologist , 13:94{99, 1958.\\nR.J. Solomono\\x0b. A system for incremental learning based on algorithmic probability. In Proceedings of the Sixth Israeli Conference on Arti\\x0ccial Intelligence, Computer Vision and Pattern\\nRecognition , pages 515{527. Citeseer, 1989.\\nG.G. Towell and J.W. Shavlik. Knowledge-based arti\\x0ccial neural networks. Arti\\x0ccial intelligence , 70(1):119{165, 1994.\\nTommi Vatanen, Tapani Raiko, Harri Valpola, and Yann LeCun. Pushing stochastic gradient\\ntowards second-order methods{backpropagation learning with transformations in nonlinearities. arXiv preprint arXiv:1301.3476 , 2013.\\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\\nStacked denoising autoencoders: Learning useful representations in a deep network with a\\nlocal denoising criterion. Journal of Machine Learning Research , 11:3371{3408, December\\n2010.\\nLuis Von Ahn, Manuel Blum, Nicholas J Hopper, and John Langford. Captcha: Using hard',\n",
       " \"2010.\\nLuis Von Ahn, Manuel Blum, Nicholas J Hopper, and John Langford. Captcha: Using hard\\nai problems for security. In Advances in CryptologyEUROCRYPT 2003 , pages 294{311.\\nSpringer, 2003.\\nJason Weston, Fr\\x13 ed\\x13 eric Ratle, and Ronan Collobert. Deep learning via semi-supervised embedding. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Proceedings of the Twenty-\\x0cfth International Conference on Machine Learning (ICML'08) ,\\npages 1168{1175, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4. doi:\\n10.1145/1390156.1390303.\\nMatthew D Zeiler. Adadelta: An adaptive learning rate method. arXiv preprint\\narXiv:1212.5701 , 2012.\\n5. Appendix\\n5.1 Binary-Binary RBMs on Pentomino Dataset\\nWe trained binary-binary RBMs (both visible and hidden are binary) on 8 \\x028 patches extracted\\nfrom the Pentomino Dataset using PCD (stochastic maximum likelihood), a weight decay of\\n.0001 and a sparsity penalty9. We used 256 hidden units and trained by SGD with a batch\",\n",
       " '.0001 and a sparsity penalty9. We used 256 hidden units and trained by SGD with a batch\\nsize of 32 and a annealing learning rate (Bengio, 2013b) starting from 1e-3 with annealing rate\\n9. implemented as TorontoSparsity in pylearn2, see the yaml \\x0cle in the repository for more details\\n31\\n1.000015. The RBM is trained with momentum starting from 0.5. The biases are initialized to\\n-2 in order to get a sparse representation. The RBM is trained for 120 epochs (approximately\\n50 million updates).\\nAfter pretraining the RBM, its parameters are used to initialize the \\x0crst layer of an SMLPnohints network. As in the usual architecture of the SMLP-nohints on top of P1NN, there is\\nan intermediate layer. Both P1NN and the intermediate layer have a sigmoid nonlinearity, and\\nthe intermediate layer has 11 units per location. This SMLP-nohints is trained with Adadelta\\nand standardization at the intermediate layer10.\\n0 5 10 15 20 25 30 35 40 45\\nEpoch0.00.10.20.30.40.5Error percentageRBM Test and Training Errors\\nTraining Error\\nTest Error',\n",
       " '0 5 10 15 20 25 30 35 40 45\\nEpoch0.00.10.20.30.40.5Error percentageRBM Test and Training Errors\\nTraining Error\\nTest Error\\nFigure 16: Training and test errors of an SMLP-nohints network whose \\x0crst layer is pre-trained\\nas an RBM. Training error reduces to 0% at epoch 42, but test error is still chance.\\n5.2 Experimental Setup and Hyper-parameters\\n5.2.1 Decision Trees\\nWe used the decision tree implementation in the scikit-learn (Pedregosa et al., 2011) python\\npackage which is an implementation of the CART (Regression Trees) algorithm. The CART\\nalgorithm constructs the decision tree recursively and partitions the input space such that the\\nsamples belonging to the same category are grouped together (Olshen and Stone, 1984). We\\nused The Gini index as the impurity criteria. We evaluated the hyper-parameter con\\x0cgurations with a grid-search. We cross-validated the maximum depth ( maxdepth ) of the tree (for\\npreventing the algorithm to severely over\\x0ct the training set) and minimum number of samples\\n10. In our auto-encoder experiments we directly fed features to P2NN without standardization and Adadelta.\\n32',\n",
       " '10. In our auto-encoder experiments we directly fed features to P2NN without standardization and Adadelta.\\n32\\nFigure 17: Filters learned by the binary-binary RBM after training on the 40k examples. The\\nRBM did learn the edge structure of Pentomino shapes.\\nFigure 18: 100 samples generated from trained RBM. All the generated samples are valid Pentomino shapes.\\n33\\nrequired to create a split ( minsplit). 20 di\\x0berent con\\x0cgurations of hyper-parameter values were\\nevaluated. We obtained the best validation error with maxdepth = 300 andminsplit = 8.\\n5.2.2 Support Vector Machines\\nWe used the \\\\Support Vector Classi\\x0cer (SVC)\" implementation from the scikit-learn package\\nwhich in turn uses the libsvm\\'s Support Vector Machine (SVM) implementation. Kernelbased SVMs are non-parametric models that map the data into a high dimensional space and\\nseparate di\\x0berent classes with hyperplane(s) such that the support vectors for each category\\nwill be separated by a large margin. We cross-validated three hyper-parameters of the model\\nusing grid-search: C,\\rand the type of kernel( kerneltype).Cis the penalty term (weight',\n",
       " 'using grid-search: C,\\rand the type of kernel( kerneltype).Cis the penalty term (weight\\ndecay) for the SVM and \\ris a hyper-parameter that controls the width of the Gaussian for\\nthe RBF kernel. For the polynomial kernel, \\rcontrols the \\rexibility of the classi\\x0cer (degree\\nof the polynomial) as the number of parameters increases (Hsu et al., 2003; Ben-Hur and\\nWeston, 2010). We evaluated forty-two hyper-parameter con\\x0cgurations. That includes, two\\nkernel types:fRBF; Polynomial g; three gammas:f1e\\x002;1e\\x003;1e\\x004gfor the RBF kernel,\\nf1;2;5gfor the polynomial kernel, and seven Cvalues among:f0:1;1;2;4;8;10;16g. As a\\nresult of the grid search and cross-validation, we have obtained the best test error by using the\\nRBF kernel, with C= 2 and\\r= 1.\\n5.2.3 Multi Layer Perceptron\\nWe have our own implementation of Multi Layer Perceptron based on the Theano (Bergstra\\net al., 2010) machine learning libraries. We have selected 2 hidden layers, the recti\\x0cer activation',\n",
       " 'et al., 2010) machine learning libraries. We have selected 2 hidden layers, the recti\\x0cer activation\\nfunction, and 2048 hidden units per layer. We cross-validated three hyper-parameters of the\\nmodel using random-search, sampling the learning rates \\x0fin log-domain, and selecting L1\\nandL2 regularization penalty coe\\x0ecients in sets of \\x0cxed values, evaluating 64 hyperparameter\\nvalues. The range of the hyperparameter values are \\x0f2[0:0001;1],L12f0:;1e\\x006;1e\\x005;1e\\x004g\\nandL22f0;1e\\x006;1e\\x005g. As a result, the following were selected: L1 = 1e\\x006,L2 = 1e\\x005\\nand\\x0f= 0:05.\\n5.2.4 Random Forests\\nWe used scikit-learn\\'s implementation of \\\\Random Forests\" decision tree learning. The Random Forests algorithm creates an ensemble of decision trees by randomly selecting for each tree\\na subset of features and applying bagging to combine the individual decision trees (Breiman,\\n2001). We have used grid-search and cross-validated the maxdepth ,minsplit, and number',\n",
       " '2001). We have used grid-search and cross-validated the maxdepth ,minsplit, and number\\nof trees (nestimators ). We have done the grid-search on the following hyperparameter values,nestimators2 f5;10;15;25;50g,maxdepth2 f100;300;600;900g, andminsplits2\\nf1;4;16g. We obtained the best validation error with maxdepth = 300,minsplit = 4 and\\nnestimators = 10.\\n5.2.5 k-Nearest Neighbors\\nWe used scikit-learn\\'s implementation of k-Nearest Neighbors (k-NN). k-NN is an instancebased, lazy learning algorithm that selects the training examples closest in Euclidean distance\\nto the input query. It assigns a class label to the test example based on the categories of the\\nkclosest neighbors. The hyper-parameters we have evaluated in the cross-validation are the\\nnumber of neighbors ( k) andweights . Theweights hyper-parameter can be either \\\\uniform\" or\\n34\\n\\\\distance\". With \\\\uniform\", the value assigned to the query point is computed by the majority\\nvote of the nearest neighbors. With \\\\distance\", each value assigned to the query point is\\ncomputed by weighted majority votes where the weights are computed with the inverse distance',\n",
       " 'vote of the nearest neighbors. With \\\\distance\", each value assigned to the query point is\\ncomputed by weighted majority votes where the weights are computed with the inverse distance\\nbetween the query point and the neighbors. We have used nneighbours2f1;2;4;6;8;12gand\\nweights2f\"uniform \";\"distance \"gfor hyper-parameter search. As a result of cross-validation\\nand grid search, we obtained the best validation error with k= 2 andweights =\\\\uniform\".\\n5.2.6 Convolutional Neural Nets\\nWe used a Theano (Bergstra et al., 2010) implementation of Convolutional Neural Networks\\n(CNN) from the deep learning tutorial at deeplearning.net , which is based on a vanilla\\nversion of a CNN LeCun et al. (1998). Our CNN has two convolutional layers. Following\\neach convolutional layer, we have a max-pooling layer. On top of the convolution-poolingconvolution-pooling layers there is an MLP with one hidden layer. In the cross-validation we\\nhave sampled 36 learning rates in log-domain in the range [0 :0001;1] and the number of \\x0clters',\n",
       " 'have sampled 36 learning rates in log-domain in the range [0 :0001;1] and the number of \\x0clters\\nfrom the range [10 ;20;30;40;50;60] uniformly. For the \\x0crst convolutional layer we used 9 \\x029\\nreceptive \\x0celds in order to guarantee that each object \\x0cts inside the receptive \\x0celd. As a result\\nof random hyperparameter search and doing manual hyperparameter search on the validation\\ndataset, the following values were selected:\\n\\x0fThe number of features used for the \\x0crst layer is 30 and the second layer is 60.\\n\\x0fFor the second convolutional layer, 7 \\x027 receptive \\x0celds. The stride for both convolutional\\nlayers is 1.\\n\\x0fConvolved images are downsampled by a factor of 2 \\x022 at each pooling operation.\\n\\x0fThe learning rate for CNN is 0.01 and it was trained for 8 epochs.\\n5.2.7 Maxout Convolutional Neural Nets\\nWe used the pylearn2 ( https://github.com/lisa-lab/pylearn2 ) implementation of maxout\\nconvolutional networks (Goodfellow et al., 2013). There are two convolutional layers in the',\n",
       " \"convolutional networks (Goodfellow et al., 2013). There are two convolutional layers in the\\nselected architecture, without any pooling. In the last convolutional layer, there is a maxout\\nnon-linearity. The following were selected by cross-validation: learning rate, number of channels\\nfor the both convolution layers, number of kernels for the second layer and number of units and\\npieces per maxout unit in the last layer, a linearly decaying learning rate, momentum starting\\nfrom 0.5 and saturating to 0.8 at the 200'th epoch. Random search for the hyperparameters\\nwas used to evaluate 48 di\\x0berent hyperparameter con\\x0cgurations on the validation dataset. For\\nthe \\x0crst convolutional layer, 8 \\x028 kernels were selected to make sure that each Pentomino shape\\n\\x0cts into the kernel. Early stopping was used and test error on the model that has the best\\nvalidation error is reported. Using norm constraint on the fan-in of the \\x0cnal softmax units\\nyields slightly better result on the validation dataset.\\nAs a result of cross-validation and manually tuning the hyperparameters we used the following hyperparameters:\\n\\x0f16 channels per convolutional layer. 600 hidden units for the maxout layer.\\n\\x0f6x6 kernels for the second convolutional layer.\\n35\",\n",
       " '\\x0f16 channels per convolutional layer. 600 hidden units for the maxout layer.\\n\\x0f6x6 kernels for the second convolutional layer.\\n35\\n\\x0f5 pieces for the convolution layers and 4 pieces for the maxout layer per maxout units.\\n\\x0fWe decayed the learning rate by the factor of 0.001 and the initial learning rate is 0.026367.\\nBut we scaled the learning rate of the second convolutional layer by a constant factor of\\n0.6.\\n\\x0fThe norm constraint (on the incoming weights of each unit) is 1.9365.\\nFigure 19 shows the \\x0crst layer \\x0clters of the maxout convolutional net, after being trained\\non the 80k training set for 85 epochs.\\nFigure 19: Maxout convolutional net \\x0crst layer \\x0clters. Most of the \\x0clters were able to learn\\nthe basic edge structure of the Pentomino shapes.\\n5.2.8 Stacked Denoising Auto-Encoders\\nDenoising Auto-Encoders (DAE) are a form of regularized auto-encoder (Bengio et al., 2013).\\nThe DAE forces the hidden layer to discover more robust features and prevents it from simply\\nlearning the identity by reconstructing the input from a corrupted version of it (Vincent et al.,',\n",
       " \"The DAE forces the hidden layer to discover more robust features and prevents it from simply\\nlearning the identity by reconstructing the input from a corrupted version of it (Vincent et al.,\\n2010). Two DAEs were stacked, resulting in an unsupervised transformation with two hidden\\nlayers of 1024 units each. Parameters of all layers are then \\x0cne-tuned with supervised \\x0cnetuning using logistic regression as the classi\\x0cer and SGD as the gradient-based optimization\\nalgorithm. The stochastic corruption process is binomial (0 or 1 replacing each input value,\\nwith probability 0.2). The selected learning rate is \\x0f0= 0:01 for the DAe and \\x0f1= 0:1 for\\nsupervised \\x0cne-tuning. Both L1 and L2 penalty for the DAEs and for the logistic regression\\nlayer are set to 1e-6.\\nCAE+MLP with Supervised Finetuning: A regularized auto-encoder which sometimes\\noutperforms the DAE is the Contractive Auto-Encoder (CAE), (Rifai et al., 2012), which\\npenalizes the Frobenius norm of the Jacobian matrix of derivatives of the hidden units with\\nrespect to the CAE's inputs. The CAE serves as pre-training for an MLP, and in the supervised\",\n",
       " \"respect to the CAE's inputs. The CAE serves as pre-training for an MLP, and in the supervised\\n\\x0cne-tuning state, the Adagrad method was used to automatically tune the learning rate (Duchi\\net al., 2010).\\nAfter training a CAE with 100 sigmoidal units patch-wise, the features extracted on each\\npatch are concatenated and fed as input to an MLP. The selected Jacobian penalty coe\\x0ecient\\nis 2, the learning rate for pre-training is 0.082 with batch size of 200 and 200 epochs of unsupervised learning are performed on the training set. For supervised \\x0cnetuning, the learning\\nrate is 0.12 over 100 epochs, L1 and L2 regularization penalty terms respectively are 1e-4 and\\n1e-6, and the top-level MLP has 6400 hidden units.\\n36\\nGreedy Layerwise CAE+DAE Supervised Finetuning: For this experiment we stack a\\nCAE with sigmoid non-linearities and then a DAE with recti\\x0cer non-linearities during the pretraining phase. As recommended by Glorot et al. (2011b) we have used a softplus nonlinearity\\nfor reconstruction, softplus (x) =log(1 +ex). We used an L1 penalty on the recti\\x0cer outputs\",\n",
       " 'for reconstruction, softplus (x) =log(1 +ex). We used an L1 penalty on the recti\\x0cer outputs\\nto obtain a sparser representation with recti\\x0cer non-linearity and L2 regularization to keep the\\nnon-zero weights small.\\nThe main di\\x0berence between the DAE and CAE is that the DAE yields more robust reconstruction whereas the CAE obtains more robust features (Rifai et al., 2011).\\nAs seen on Figure 7 the weights U and V are shared on each patch and we concatenate the\\noutputs of the last auto-encoder on each patch to feed it as an input to an MLP with a large\\nhidden layer.\\nWe used 400 hidden units for the CAE and 100 hidden units for DAE. The learning rate\\nused for the CAE is 0.82 and for DAE it is 9*1e-3. The corruption level for the DAE (binomial\\nnoise) is 0.25 and the contraction level for the CAE is 2.0. The L1 regularization penalty for\\nthe DAE is 2.25*1e-4 and the L2 penalty is 9.5*1e-5. For the supervised \\x0cnetuning phase the',\n",
       " 'the DAE is 2.25*1e-4 and the L2 penalty is 9.5*1e-5. For the supervised \\x0cnetuning phase the\\nlearning rate used is 4*1e-4 with L1 and L2 penalties respectively 1e-5 and 1e-6. The top-level\\nMLP has 6400 hidden units. The auto-encoders are each trained for 150 epochs while the whole\\nMLP is \\x0cne-tuned for 50 epochs.\\nGreedy Layerwise DAE+DAE Supervised Finetuning: For this architecture, we have\\ntrained two layers of denoising auto-encoders greedily and performed supervised \\x0cnetuning\\nafter unsupervised pre-training. The motivation for using two denoising auto-encoders is the\\nfact that recti\\x0cer nonlinearities work well with the deep networks but it is di\\x0ecult to train\\nCAEs with the recti\\x0cer non-linearity. We have used the same type of denoising auto-encoder\\nthat is used for the greedy layerwise CAE+DAE supervised \\x0cnetuning experiment.\\nIn this experiment we have used 400 hidden units for the \\x0crst layer DAE and 100 hidden\\nunits for the second layer DAE. The other hyperparameters for DAE and supervised \\x0cnetuning',\n",
       " 'units for the second layer DAE. The other hyperparameters for DAE and supervised \\x0cnetuning\\nare the same as with the CAE+DAE MLP Supervised Finetuning experiment.\\n37',\n",
       " 'DeepPose: Human Pose Estimation via Deep Neural Networks\\nAlexander Toshev Christian Szegedy\\nGoogle\\n1600 Amphitheatre Pkwy\\nMountain View, CA 94043\\ntoshev,szegedy@google.com\\nFigure 1. Besides extreme variability in articulations, many of the\\njoints are barely visible. We can guess the location of the right\\narm in the left image only because we see the rest of the pose and\\nanticipate the motion or activity of the person. Similarly, the left\\nbody half of the person on the right is not visible at all. These\\nare examples of the need for holistic reasoning . We believe that\\nDNNs can naturally provide such type of reasoning.\\nAbstract\\nWe propose a method for human pose estimation based\\non Deep Neural Networks (DNNs). The pose estimation\\nis formulated as a DNN-based regression problem towards\\nbody joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The\\napproach has the advantage of reasoning about pose in a\\nholistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-ofart or better performance on four academic benchmarks of\\ndiverse real-world images.\\n1. Introduction\\nThe problem of human pose estimation, deﬁned as the',\n",
       " 'diverse real-world images.\\n1. Introduction\\nThe problem of human pose estimation, deﬁned as the\\nproblem of localization of human joints, has enjoyed substantial attention in the computer vision community. In\\nFig. 1, one can see some of the challenges of this problem – strong articulations, small and barely visible joints,\\nocclusions and the need to capture the context.\\nThe main stream of work in this ﬁeld has been motivatedmainly by the ﬁrst challenge, the need to search in the large\\nspace of all possible articulated poses. Part-based models\\nlend themselves naturally to model articulations ([16, 8])\\nand in the recent years a variety of models with efﬁcient\\ninference have been proposed ([6, 19]).\\nThe above efﬁciency, however, is achieved at the cost of\\nlimited expressiveness – the use of local detectors, which\\nreason in many cases about a single part, and most importantly by modeling only a small subset of all interactions\\nbetween body parts. These limitations, as exempliﬁed in\\nFig. 1, have been recognized and methods reasoning about\\npose in a holistic manner have been proposed [15, 21] but\\nwith limited success in real-world problems.\\nIn this work we ascribe to this holistic view of human\\npose estimation. We capitalize on recent developments of',\n",
       " 'with limited success in real-world problems.\\nIn this work we ascribe to this holistic view of human\\npose estimation. We capitalize on recent developments of\\ndeep learning and propose a novel algorithm based on a\\nDeep Neural Network (DNN). DNNs have shown outstanding performance on visual classiﬁcation tasks [14] and more\\nrecently on object localization [23, 9]. However, the question of applying DNNs for precise localization of articulated\\nobjects has largely remained unanswered. In this paper we\\nattempt to cast a light on this question and present a simple\\nand yet powerful formulation of holistic human pose estimation as a DNN.\\nWe formulate the pose estimation as a joint regression\\nproblem and show how to successfully cast it in DNN settings. The location of each body joint is regressed to using\\nas an input the full image and a 7-layered generic convolutional DNN. There are two advantages of this formulation.\\nFirst, the DNN is capable of capturing the full context of\\neach body joint – each joint regressor uses the full image\\nas a signal. Second, the approach is substantially simpler\\nto formulate than methods based on graphical models – no\\nneed to explicitly design feature representations and detectors for parts; no need to explicitly design a model topology\\nand interactions between joints. Instead, we show that a\\ngeneric convolutional DNN can be learned for this problem.',\n",
       " 'and interactions between joints. Instead, we show that a\\ngeneric convolutional DNN can be learned for this problem.\\nFurther, we propose a cascade of DNN-based pose predictors. Such a cascade allows for increased precision of\\n1arXiv:1312.4659v3  [cs.CV]  20 Aug 2014\\njoint localization. Starting with an initial pose estimation,\\nbased on the full image, we learn DNN-based regressors\\nwhich reﬁnes the joint predictions by using higher resolution sub-images.\\nWe show state-of-art results or better than state-of-art on\\nfour widely used benchmarks against all reported results.\\nWe show that our approach performs well on images of people which exhibit strong variation in appearance as well as\\narticulations. Finally, we show generalization performance\\nby cross-dataset evaluation.\\n2. Related Work\\nThe idea of representing articulated objects in general,\\nand human pose in particular, as a graph of parts has been\\nadvocated from the early days of computer vision [16]. The\\nso called Pictorial Strictures (PSs), introduced by Fishler\\nand Elschlager [8], were made tractable and practical by\\nFelzenszwalb and Huttenlocher [6] using the distance transform trick. As a result, a wide variety of PS-based models',\n",
       " 'Felzenszwalb and Huttenlocher [6] using the distance transform trick. As a result, a wide variety of PS-based models\\nwith practical signiﬁcance were subsequently developed.\\nThe above tractability, however, comes with the limitation of having a tree-based pose models with simple binary\\npotential not depending on image data. As a result, research\\nhas focused on enriching the representational power of the\\nmodels while maintaining tractability. Earlier attempts to\\nachieve this were based on richer part detectors [19, 1, 4].\\nMore recently, a wide variety of models expressing complex\\njoint relationships were proposed. Yang and Ramanan [27]\\nuse a mixture model of parts. Mixture models on the full\\nmodel scale, by having mixture of PSs, have been studied\\nby Johnson and Everingham [13]. Richer higher-order spatial relationships were captured in a hierarchical model by\\nTian et al. [25]. A different approach to capture higherorder relationship is through image-dependent PS models,\\nwhich can be estimated via a global classiﬁer [26, 20, 18].\\nApproaches which ascribe to our philosophy of reasoning about pose in a holistic manner have shown limited\\npracticality. Mori and Malik [15] try to ﬁnd for each test\\nimage the closest exemplar from a set of labeled images',\n",
       " 'practicality. Mori and Malik [15] try to ﬁnd for each test\\nimage the closest exemplar from a set of labeled images\\nand transfer the joint locations. A similar nearest neighbor\\nsetup is employed by Shakhnarovich et al. [21], who however use locality sensitive hashing. More recently, Gkioxari\\net al. [10] propose a semi-global classiﬁer for part conﬁguration. This formulation has shown very good results on\\nreal-world data, however, it is based on linear classiﬁers\\nwith less expressive representation than ours and is tested\\non arms only. Finally, the idea of pose regression has been\\nemployed by Ionescu et al. [11], however they reason about\\n3D pose.\\nThe closest work to ours uses convolution NNs together\\nwith Neighborhood Component Analysis to regress toward\\na point in an embedding representing pose [24]. However,\\nthis work does not employ a cascade of networks. Cascadesof DNN regressors have been used for localization, however\\nof facial points [22]. On the related problem of face pose\\nestimation, Osadchy et al. [17] employ a NN-based pose\\nembedding trained with a contrastive loss.\\n3. Deep Learning Model for Pose Estimation',\n",
       " 'estimation, Osadchy et al. [17] employ a NN-based pose\\nembedding trained with a contrastive loss.\\n3. Deep Learning Model for Pose Estimation\\nWe use the following notation. To express a pose, we encode the locations of all kbody joints in pose vector deﬁned\\nasy= (:::;yT\\ni;:::)T;i2f1;:::;kg, where yicontains\\nthexandycoordinates of the ithjoint. A labeled image is\\ndenoted by (x;y)wherexstands for the image data and y\\nis the ground truth pose vector.\\nFurther, since the joint coordinates are in absolute image\\ncoordinates, it proves beneﬁcial to normalize them w. r. t. a\\nboxbbounding the human body or parts of it. In a trivial\\ncase, the box can denote the full image. Such a box is deﬁned by its center bc2R2as well as width bwand height\\nbh:b= (bc;bw;bh). Then the joint yican be translated by\\nthe box center and scaled by the box size which we refer to\\nas normalization by b:\\nN(yi;b) =\\x121=bw 0\\n0 1=bh\\x13\\n(yi\\x00bc) (1)',\n",
       " 'as normalization by b:\\nN(yi;b) =\\x121=bw 0\\n0 1=bh\\x13\\n(yi\\x00bc) (1)\\nFurther, we can apply the same normalization to the elements of pose vector N(y;b) = (:::;N (yi;b)T;:::)Tresulting in a normalized pose vector . Finally, with a slight\\nabuse of notation, we use N(x;b)to denote a crop of the\\nimagexby the bounding box b, which de facto normalizes\\nthe image by the box. For brevity we denote by N(\\x01)normalization with bbeing the full image box.\\n3.1. Pose Estimation as DNN-based Regression\\nIn this work, we treat the problem of pose estimation as\\nregression, where the we train and use a function  (x;\\x12)2\\nR2kwhich for an image xregresses to a normalized pose\\nvector, where \\x12denotes the parameters of the model. Thus,\\nusing the normalization transformation from Eq. (1) the\\npose prediction y\\x03in absolute image coordinates reads\\ny\\x03=N\\x001( (N(x);\\x12)) (2)\\nDespite its simple formulation, the power and complexity of the method is in  , which is based on a convolutional',\n",
       " 'Despite its simple formulation, the power and complexity of the method is in  , which is based on a convolutional\\nDeep Neural Network (DNN). Such a convolutional network consists of several layers – each being a linear transformation followed by a non-linear one. The ﬁrst layer takes\\nas input an image of predeﬁned size and has a size equal to\\nthe number of pixels times three color channels. The last\\nlayer outputs the target values of the regression, in our case\\n2kjoint coordinates.\\nWe base the architecture of the  on the work by\\nKrizhevsky et al. [14] for image classiﬁcation since it has\\nshown outstanding results on object localization as well\\n(xi, yi)\\n(x(s-1)i, y (s-1) i)xsi - x(s-1)iysi - y(s-1)iInitial stageStage s\\nsend reﬁned values to next stage220 x 220\\nDNN-based regressor\\n27 x 27 x 128\\n13 x 13 x 192\\n13 x 13 x192\\n13 x 13 x192\\n4096 \\n4096 \\n55 x 55 x 48xiyi...DNN-based reﬁner\\n27 x 27 x 128\\n13 x 13 x 192\\n13 x 13 x192',\n",
       " '4096 \\n55 x 55 x 48xiyi...DNN-based reﬁner\\n27 x 27 x 128\\n13 x 13 x 192\\n13 x 13 x192\\n13 x 13 x192\\n4096 \\n4096 \\n55 x 55 x 48Figure 2. Left: schematic view of the DNN-based pose regression. We visualize the network layers with their corresponding dimensions,\\nwhere convolutional layers are in blue, while fully connected ones are in green. We do not show the parameter free layers. Right: at stage\\ns, a reﬁning regressor is applied on a sub image to reﬁne a prediction from the previous stage.\\n[23]. In a nutshell, the network consists of 7layers (see\\nFig. 2 left). Denote by Ca convolutional layer, by LRN\\na local response normalization layer, Pa pooling layer\\nand byFa fully connected layer. Only CandFlayers\\ncontain learnable parameters, while the rest are parameter free. Both CandFlayers consist of a linear transformation followed by a nonlinear one, which in our case\\nis a rectiﬁed linear unit. For Clayers, the size is deﬁned as width\\x02height\\x02depth, where the ﬁrst two dimensions have a spatial meaning while the depth deﬁnes',\n",
       " 'the number of ﬁlters. If we write the size of each layer in\\nparentheses, then the network can be described concisely\\nasC(55\\x0255\\x0296)\\x00LRN\\x00P\\x00C(27\\x0227\\x02256)\\x00\\nLRN\\x00P\\x00C(13\\x0213\\x02384)\\x00C(13\\x0213\\x02384)\\x00\\nC(13\\x0213\\x02256)\\x00P\\x00F(4096)\\x00F(4096) . The ﬁlter\\nsize for the ﬁrst two Clayers is 11\\x0211and5\\x025and for\\nthe remaining three is 3\\x023. Pooling is applied after three\\nlayers and contributes to increased performance despite the\\nreduction of resolution. The input to the net is an image\\nof220\\x02220which via stride of 4is fed into the network.\\nThe total number of parameters in the above model is about\\n40M. For further details, we refer the reader to [14].\\nThe use of a generic DNN architecture is motivated by\\nits outstanding results on both classiﬁcation and localization\\nproblems. In the experimental section we show that such a\\ngeneric architecture can be used to learn a model resulting\\nin state-of-art or better performance on pose estimation as\\nwell. Further, such a model is a truly holistic one — the',\n",
       " 'generic architecture can be used to learn a model resulting\\nin state-of-art or better performance on pose estimation as\\nwell. Further, such a model is a truly holistic one — the\\nﬁnal joint location estimate is based on a complex nonlinear\\ntransformation of the full image.\\nAdditionally, the use of a DNN obviates the need to design a domain speciﬁc pose model. Instead such a model\\nand the features are learned from the data. Although the regression loss does not model explicit interactions between\\njoints, such are implicitly captured by all of the 7hidden\\nlayers – all the internal features are shared by all joint regressors.\\nTraining The difference to [14] is the loss. Instead of a\\nclassiﬁcation loss, we train a linear regression on top of thelast network layer to predict a pose vector by minimizing\\nL2distance between the prediction and the true pose vector. Since the ground truth pose vector is deﬁned in absolute image coordinates and poses vary in size from image to\\nimage, we normalize our training set Dusing the normalization from Eq. (1):\\nDN=f(N(x);N(y))j(x;y)2Dg (3)\\nThen theL2loss for obtaining optimal network parameters\\nreads:\\narg min\\n\\x12X',\n",
       " 'Then theL2loss for obtaining optimal network parameters\\nreads:\\narg min\\n\\x12X\\n(x;y)2DNkX\\ni=1jjyi\\x00 i(x;\\x12)jj2\\n2 (4)\\nFor clarity we write out the optimization over individual\\njoints. It should be noted, that the above objective can\\nbe used even if for some images not all joints are labeled.\\nIn this case, the corresponding terms in the sum would be\\nomitted.\\nThe above parameters \\x12are optimized for using Backpropagation in a distributed online implementation. For\\neach mini-batch of size 128, adaptive gradient updates are\\ncomputed [3]. The learning rate, as the most important parameter, is set to 0:0005 . Since the model has large number\\nof parameters and the used datasets are of relatively small\\nsize, we augment the data using large number of randomly\\ntranslated image crops (see Sec. 3.2), left/right ﬂips as well\\nas DropOut regularization for the Flayers set to 0:6.\\n3.2. Cascade of Pose Regressors\\nThe pose formulation from the previous section has the\\nadvantage that the joint estimation is based on the full image and thus relies on context. However, due to its ﬁxed\\ninput size of 220\\x02220, the network has limited capacity',\n",
       " 'input size of 220\\x02220, the network has limited capacity\\nto look at detail – it learns ﬁlters capturing pose properties\\nat coarse scale. These are necessary to estimate rough pose\\nbut insufﬁcient to always precisely localize the body joints.\\nNote that we cannot easily increase the input size since this\\nwill increase the already large number of parameters. In order to achieve better precision, we propose to train a cascade\\nof pose regressors. At the ﬁrst stage, the cascade starts off\\nby estimating an initial pose as outlined in the previous section. At subsequent stages, additional DNN regressors are\\ntrained to predict a displacement of the joint locations from\\nprevious stage to the true location. Thus, each subsequent\\nstage can be thought of as a reﬁnement of the currently predicted pose, as shown in Fig. 2.\\nFurther, each subsequent stage uses the predicted joint\\nlocations to focus on the relevant parts of the image – subimages are cropped around the predicted joint location from\\nprevious stage and the pose displacement regressor for this\\njoint is applied on this sub-image. In this way, subsequent\\npose regressors see higher resolution images and thus learn\\nfeatures for ﬁner scales which ultimately leads to higher\\nprecision.\\nWe use the same network architecture for all stages of\\nthe cascade but learn different network parameters. For',\n",
       " 'features for ﬁner scales which ultimately leads to higher\\nprecision.\\nWe use the same network architecture for all stages of\\nthe cascade but learn different network parameters. For\\nstages2 f1;:::;Sgof totalScascade stages, we denote by\\x12sthe learned network parameters. Thus, the\\npose displacement regressor reads  (x;\\x12s). To reﬁne a\\ngiven joint location yiwe will consider a joint bounding\\nboxbicapturing the sub-image around yi:bi(y;\\x1b) =\\n(yi;\\x1bdiam(y);\\x1bdiam(y))having as center the i-th joint\\nand as dimension the pose diameter scaled by \\x1b. The diameter diam (y)of the pose is deﬁned as the distance between\\nopposing joints on the human torso, such as left shoulder\\nand right hip, and depends on the concrete pose deﬁnition\\nand dataset.\\nUsing the above notation, at the stage s= 1we start with\\na bounding box b0which either encloses the full image or\\nis obtained by a person detector. We obtain an initial pose:\\nStage 1 : y1 N\\x001( (N(x;b0);\\x121);b0)(5)',\n",
       " 'is obtained by a person detector. We obtain an initial pose:\\nStage 1 : y1 N\\x001( (N(x;b0);\\x121);b0)(5)\\nAt each subsequent stage s\\x152, for all joints i2f1;:::;kg\\nwe regress ﬁrst towards a reﬁnement displacement ys\\ni\\x00\\ny(s\\x001)\\ni by applying a regressor on the sub image deﬁned\\nbyb(s\\x001)\\ni from previous stage (s\\x001). Then, we estimate\\nnew joint boxes bs\\ni:\\nStages:ys\\ni y(s\\x001)\\ni +N\\x001( i(N(x;b);\\x12s);b)(6)\\nforb=b(s\\x001)\\ni\\nbs\\ni (ys\\ni;\\x1bdiam(ys);\\x1bdiam(ys)) (7)\\nWe apply the cascade for a ﬁxed number of stages S,\\nwhich is determined as explained in Sec. 4.1.\\nTraining The network parameters \\x121are trained as\\noutlined in Sec. 3.1, Eq. (4). At subsequent stages',\n",
       " 'Training The network parameters \\x121are trained as\\noutlined in Sec. 3.1, Eq. (4). At subsequent stages\\ns\\x152, the training is done identically with one important difference. Each joint ifrom a training example(x;y)is normalized using a different bounding box(y(s\\x001)\\ni;\\x1bdiam(y(s\\x001));\\x1bdiam(y(s\\x001)))– the one centered at the prediction for the same joint obtained from previous stage – so that we condition the training of the stage\\nbased on the model from previous stage.\\nSince deep learning methods have large capacity, we\\naugment the training data by using multiple normalizations\\nfor each image and joint. Instead of using the prediction\\nfrom previous stage only, we generate simulated predictions. This is done by randomly displacing the ground truth\\nlocation for joint iby a vector sampled at random from a\\n2-dimensional Normal distribution N(s\\x001)\\ni with mean and\\nvariance equal to the mean and variance of the observed displacements (y(s\\x001)\\ni\\x00yi)across all examples in the training data. The full augmented training data can be deﬁned\\nby ﬁrst sampling an example and a joint from the original\\ndata at uniform and then generating a simulated prediction',\n",
       " 'by ﬁrst sampling an example and a joint from the original\\ndata at uniform and then generating a simulated prediction\\nbased on a sampled displacement \\x0efromN(s\\x001)\\ni :\\nDs\\nA=f(N(x;b);N(yi;b))j\\n(x;yi)\\x18D;\\x0e\\x18N(s\\x001)\\ni;\\nb= (yi+\\x0e;\\x1bdiam(y))g\\nThe training objective for cascade stage sis done as in\\nEq. (4) by taking extra care to use the correct normalization\\nfor each joint:\\n\\x12s= arg min\\n\\x12X\\n(x;yi)2Ds\\nAjjyi\\x00 i(x;\\x12)jj2\\n2 (8)\\n4. Empirical Evaluation\\n4.1. Setup\\nDatasets There is a wide variety of benchmarks for human pose estimation. In this work we use datasets, which\\nhave large number of training examples sufﬁcient to train a\\nlarge model such as the proposed DNN, as well as are realistic and challenging.\\nThe ﬁrst dataset we use is Frames Labeled In Cinema\\n(FLIC), introduced by [20], which consists of 4000 training and 1000 test images obtained from popular Hollywood\\nmovies. The images contain people in diverse poses and especially diverse clothing. For each labeled human, 10upper',\n",
       " 'movies. The images contain people in diverse poses and especially diverse clothing. For each labeled human, 10upper\\nbody joints are labeled.\\nThe second dataset we use is Leeds Sports Dataset [12]\\nand its extension [13], which we will jointly denote by LSP.\\nCombined they contain 11000 training and 1000 testing images. These are images from sports activities and as such\\nare quite challenging in terms of appearance and especially\\narticulations. In addition, the majority of people have 150\\npixel height which makes the pose estimation even more\\nchallenging. In this dataset, for each person the full body is\\nlabeled with total 14joints.\\nFor all of the above datasets, we deﬁne the diameter of a\\nposeyto be the distance between a shoulder and hip from\\nopposing sides and denote it by diam (y). It should be noted,\\nthat the joints in all datasets are arranged in a tree kinematically mimicking the human body. This allows for a deﬁnition of a limb being a pair of neighboring joints in the pose\\ntree.\\nMetrics In order to be able to compare with published results we will use two widely accepted evaluation metrics.\\nPercentage of Correct Parts (PCP) measures detection rate\\nof limbs, where a limb is considered detected if the distance\\nbetween the two predicted joint locations and the true limb',\n",
       " 'Percentage of Correct Parts (PCP) measures detection rate\\nof limbs, where a limb is considered detected if the distance\\nbetween the two predicted joint locations and the true limb\\njoint locations is at most half of the limb length [5]. PCP\\nwas the initially preferred metric for evaluation, however it\\nhas the drawback of penalizing shorter limbs, such as lower\\narms, which are usually harder to detect.\\nTo address this drawback, recently detection rates of\\njoints are being reported using a different detection criterion – a joint is considered detected if the distance between\\nthe predicted and the true joint is within a certain fraction of\\nthe torso diameter. By varying this fraction, detection rates\\nare obtained for varying degrees of localization precision.\\nThis metric alleviates the drawback of PCP since the detection criteria for all joints are based on the same distance\\nthreshold. We refer to this metric as Percent of Detected\\nJoints (PDJ).\\nExperimental Details For all the experiments we use the\\nsame network architecture. Inspired by [7], we use a body\\ndetector on FLIC to obtain initially a rough estimate of the\\nhuman body bounding box. It is based on a face detector –\\nthe detected face rectangle is enlarged by a ﬁxed scaler. This\\nscaler is determined on the training data such that it contains\\nall labeled joints. This face-based body detector results in',\n",
       " 'scaler is determined on the training data such that it contains\\nall labeled joints. This face-based body detector results in\\na rough estimate, which however presents a good starting\\npoint for our approach. For LSP we use the full image as\\ninitial bounding box since the humans are relatively tightly\\ncropped by design.\\nUsing a small held-out set of 50 images for both datasets\\nto determine the algorithm hyperparameters. To measure\\noptimality of the parameters we used average over PDJ at\\n0:2across all joints. The scaler \\x1b, which deﬁnes the size of\\nthe reﬁnement joint bounding box as a fraction of the pose\\nsize, is determined as follows: for FLIC we chose \\x1b= 1:0\\nafter exploring values f0:8;1:0;1:2g, for LSP we use \\x1b=\\n2:0after tryingf1:5;1:7;2:0;2:3g. The number of cascade\\nstagesSis determined by training stages until the algorithm\\nstopped improving on the held-out set. For both FLIC and\\nLSP we arrived at S= 3.\\nTo improve generalization, for each cascade stage starting ats= 2 we augment the training data by sampling 40\\nrandomly translated crop boxes for each joint as explained',\n",
       " 'To improve generalization, for each cascade stage starting ats= 2 we augment the training data by sampling 40\\nrandomly translated crop boxes for each joint as explained\\nin Sec. 3.2. Thus, for LSP with 14joints and after mirroring the images and sampling the number training examples\\nis11000\\x0240\\x022\\x0214 = 12M, which is essential for\\ntraining a large network as ours.The presented algorithm allows for an efﬁcient implementation. The running time is approx. 0:1sper image,\\nas measured on a 12core CPU. This compares favorably\\nto other approaches, as some of the current state-of-art approaches have higher complexity: [20] runs in approx. 4s,\\nwhile [27] runs in 1:5s. The training complexity, however,\\nis higher. The initial stage was trained within 3days on\\napprox. 100 workers, most of the ﬁnal performance was\\nachieved after 12hours though. Each reﬁnement stage was\\ntrained for 7days since the amount of data was 40\\x02larger\\nthan the one for the initial stage due to the data augmentation in Sec. 3.2. Note that using more data led to increased\\nperformance.\\n4.2. Results and Discussion',\n",
       " 'than the one for the initial stage due to the data augmentation in Sec. 3.2. Note that using more data led to increased\\nperformance.\\n4.2. Results and Discussion\\nComparisons We present comparative results to other approaches. We compare on LSP using PCP metric in Fig. 1.\\nWe show results for the four most challenging limbs – lower\\nand upper arms and legs – as well as the average value\\nacross these limbs for all compared algorithms. We clearly\\noutperform all other approaches, especially achieving better estimation for legs. For example, for upper legs we obtain0:78up from 0:74for the next best performing method.\\nIt is worth noting that while the other approaches exhibit\\nstrengths for particular limbs, none of the other dataset consistently dominates across all limbs. In contrary, DeepPose\\nshows strong results for all challenging limbs.\\nUsing the PDJ metric allows us to vary the threshold for\\nthe distance between prediction and ground truth, which deﬁnes a detection. This threshold can be thought of as a\\nlocalization precision at which detection rates are plotted.\\nThus one could compare approaches across different desired precisions. We present results on FLIC in Fig. 3 comparing against additional four methods as well is on LSP in\\nFig. 4. For each dataset we train and test according the protocol for each dataset. Similarly to previous experiment we',\n",
       " 'Fig. 4. For each dataset we train and test according the protocol for each dataset. Similarly to previous experiment we\\noutperform all ﬁve algorithms. Our gains are bigger in the\\nlow precision domain, in the cases where we detect rough\\npose without precisely localizing the joints. On FLIC, at\\nnormalized distance 0:2we obtain a an increase of detection\\nrates by 0:15and0:2for elbow and wrists against the next\\nbest performing method. On LSP, at normalized distance\\n0:5we get an absolute increase of 0:1. At low precision\\nregime of normalized distance of 0:2for LSP we show comparable performance for legs and slightly worse arms. This\\ncan be attributed to the fact that the DNN-based approach\\ncomputes joint coordinates using 7 layers of transformation,\\nsome of which contain max pooling.\\nAnother observation is that our approach works well for\\nboth appearance heavy movie data as well as string articulation such as the sports images in LSP.\\nEffects of cascade-based reﬁnement A single DNNbased joint regressor gives rough joint location. However,\\n0 0.05 0.1 0.15 0.20.10.30.50.70.9Elbows\\nNormalized distance to true jointDetection rate',\n",
       " '0 0.05 0.1 0.15 0.20.10.30.50.70.9Elbows\\nNormalized distance to true jointDetection rate\\n  \\nDeepPoseMODECEichner et al.Yang et al.Sapp et al.\\n0 0.05 0.1 0.15 0.20.10.30.50.70.9Wrists\\nNormalized distance to true jointDetection rateFigure 3. Percentage of detected joints (PDJ) on FLIC for two joints: elbow and wrist. We compare DeepPose, after two cascade stages,\\nwith four other approaches.\\nMethodArm LegAve.Upper Lower Upper Lower\\nDeepPose-st1 0.5 0.27 0.74 0.65 0.54\\nDeepPose-st2 0.56 0.36 0.78 0.70 0.60\\nDeepPose-st3 0.56 0.38 0.77 0.71 0.61\\nDantone et al. [2] 0.45 0.25 0.65 0.61 0.49\\nTian et al. [25]\\x030.52 0.33 0.70 0.60 0.56\\nJohnson et al. [13] 0.54 0.38 0.75 0.66 0.58',\n",
       " 'Johnson et al. [13] 0.54 0.38 0.75 0.66 0.58\\nWang et al. [26]\\x030.565 0.37 0.76 0.68 0.59\\nPishchulin [18]\\x0e0.49 0.32 0.74 0.70 0.56\\nTable 1. Percentage of Correct Parts (PCP) at 0.5 on LSP for DeepPose as well as ﬁve state-of-art approaches.\\x03The authors use a\\nslightly looser version of PCP in which for each limb the average\\ndistance from predicted limb joints belonging to the corresponding true joints is used to determine whether the limb is correctly\\ndetected.\\x0eThe authors use person-centric joint annotations.\\n0 0.1 0.2 0.3 0.4 0.50.10.30.50.70.9Arms\\nNormalized distance to true jointDetection rate\\n  \\nDeepPose − wristsDeepPose − elbowsDantone et al. − wristsDantone et al. − elbows\\n0 0.1 0.2 0.3 0.4 0.50.10.30.50.70.9Legs\\nNormalized distance to true jointDetection rate\\n  \\nDeepPose − ankleDeepPose − kneeDantone et al. − ankleDantone et al. − knee',\n",
       " 'Normalized distance to true jointDetection rate\\n  \\nDeepPose − ankleDeepPose − kneeDantone et al. − ankleDantone et al. − knee\\nFigure 4. Percentage of detected joints (PDJ) on LSP for four\\nlimbs for DeepPose and Dantone et al. [2] over an extended range\\nof distances to true joint: [0;0:5]of the torso diameter. Results of\\nDeepPose are plotted with solid lines while all the results by [2]\\nare plotted in dashed lines. Results for the same joint from both\\nalgorithms are colored with same color.\\nto obtain higher precision the subsequent stages of the cascade, which serve as a reﬁnement of the initial prediction,\\nare of paramount importance. To see this, in Fig. 5 we\\npresent the joint detections at different precisions for the initial prediction as well as two subsequent cascade stages. As\\nexpected, we can see that the major gains of the reﬁnement\\n00.05 0.10.15 0.20.25 0.30.35 0.40.10.30.50.70.9Wrists\\nNormalized distance to true jointDetection rate\\n  \\nDeepPose − initial stage 1DeepPose − stage 2DeepPose − stage 3',\n",
       " 'Normalized distance to true jointDetection rate\\n  \\nDeepPose − initial stage 1DeepPose − stage 2DeepPose − stage 3\\n0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.40.10.30.50.70.9Elbows\\nNormalized distance to true jointDetection rateFigure 5. Percent of detected joints (PDJ) on FLIC or the ﬁrst three\\nstages of the DNN cascade. We present results over larger spectrum of normalized distances between prediction and ground truth.\\nprocedure are at high-precision regime of at normalized distances of [0:15;0:2]. Further, the major gains are achieved\\nafter one stage of reﬁnement. The reason being that subsequent stages end up using smaller sub-images around each\\njoint. And although the subsequent stages look at higher\\nresolution inputs, they have more limited context.\\nExamples of cases, where reﬁnement helps, are visualized in Fig. 6. The initial stage is usually successful at estimating a roughly correct pose, however, this pose is not\\n”snapped” to the correct one. For example, in row three the\\npose has the right shape but incorrect scale. In the second\\nrow, the predicted pose is translated north from the ideal',\n",
       " 'pose has the right shape but incorrect scale. In the second\\nrow, the predicted pose is translated north from the ideal\\none. In most cases, the second stage of the cascade resolves\\nthis snapping problem and better aligns the joints. In more\\nrare cases, such as in ﬁrst row, further facade stages improve\\non individual joints.\\nCross-dataset Generalization To evaluate the generalization properties of our algorithm, we used the trained\\nmodels on LSP and FLIC on two related datasets. The fullbody model trained on LSP is tested on the test portion of\\nthe Image Parse dataset [19] with results presented in Table 2. The ImageParse dataset is similar to LSP as it contains people doing sports, however it contains a lot of people from personal photo collections involved in other activities. Further, the upper-body model trained on FLIC was\\napplied on the whole Buffy dataset [7]. We can see that our\\nInitial stage 1stage 2stage 3Figure 6. Predicted poses in red and ground truth poses in green\\nfor the ﬁrst three stages of a cascade for three examples.\\n0 0.05 0.1 0.15 0.20.10.30.50.70.9Elbows\\nNormalized distance to true jointDetection rate\\n  \\nEichner et al.Yang et al.Sapp et al.MODECDeepPose',\n",
       " 'Normalized distance to true jointDetection rate\\n  \\nEichner et al.Yang et al.Sapp et al.MODECDeepPose\\n0 0.05 0.1 0.15 0.20.10.30.50.70.9Wrists\\nNormalized distance to true jointDetection rate\\nFigure 7. Percentage of detected joints (PDJ) on Buffy dataset\\nfor two joints: elbow and wrist. The models have been trained on\\nFLIC. We compare DeepPose, after two cascade stages, with four\\nother approaches.\\nMethodArm LegAve.Upper Lower Upper Lower\\nDeepPose 0.8 0.75 0.71 0.5 0.69\\nPishchulin [18] 0.80 0.70 0.59 037 0.62\\nJohnson et al. [13] 0.75 0.67 0.67 0.46 0.64\\nYang et al. [27] 0.69 0.64 0.55 0.35 0.56\\nTable 2. Percentage of Correct Parts (PCP) at 0.5 on Image Parse\\ndataset for DeepPose as well as two state-of-art approaches on Image Parse dataset. Results obtained from [18].\\napproach can retain state-of-art performance compared to\\nother approaches. This shows good generalization abilities.',\n",
       " 'approach can retain state-of-art performance compared to\\nother approaches. This shows good generalization abilities.\\nExample poses To get a better idea of the performance of\\nour algorithm, we visualize a sample of estimated poses on\\nimages from LSP in Fig. 8. We can see that our algorithm is\\nable to get correct pose for most of the joints under variety\\nof conditions: upside-down people (row 1, column 1), severe foreshortening (row1, column 3), unusual poses (row\\n3, column 5), occluded limbs as the occluded arms in row\\n3, columns 2 and 6, unusual illumination conditions (row 3,\\ncolumn 3). In most of the cases, when the estimated pose isnot precise, it still has a correct shape. For example, in the\\nlast row some of the predicted limbs are not aligned with\\nthe true locations, however the overall shape of the pose is\\ncorrect. A common failure mode is confusing left with right\\nside when the person was photographed from the back (row\\n6, column 6). Results on FLIC (see Fig. 9) are usually better\\nwith occasional visible mistakes on lower arms.\\n5. Conclusion\\nWe present, to our knowledge, the ﬁrst application of\\nDeep Neural Networks (DNNs) to human pose estimation.\\nOur formulation of the problem as DNN-based regression to',\n",
       " 'Deep Neural Networks (DNNs) to human pose estimation.\\nOur formulation of the problem as DNN-based regression to\\njoint coordinates and the presented cascade of such regressors has the advantage of capturing context and reasoning\\nabout pose in a holistic manner. As a result, we are able to\\nachieve state-of-art or better results on several challenging\\nacademic datasets.\\nFurther, we show that using a generic convolutional neural network, which was originally designed for classiﬁcation tasks, can be applied to the different task of localization. In future, we plan to investigate novel architectures\\nwhich could be potentially better tailored towards localization problems in general, and in pose estimation in particular.\\nAcknowledgements I would like to thank Luca Bertelli,\\nBen Sapp and Tianli Yu for assistance with data and fruitful\\ndiscussions.\\nReferences\\n[1] M. Andriluka, S. Roth, and B. Schiele. Pictorial structures\\nrevisited: People detection and articulated pose estimation.\\nInCVPR , 2009.\\n[2] M. Dantone, J. Gall, C. Leistner, and L. Van Gool. Human\\npose estimation using body parts dependent joint regressors.\\nInCVPR , 2013.\\n[3] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient',\n",
       " 'pose estimation using body parts dependent joint regressors.\\nInCVPR , 2013.\\n[3] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient\\nmethods for online learning and stochastic optimization. In\\nCOLT . ACL, 2010.\\n[4] M. Eichner and V . Ferrari. Better appearance models for\\npictorial structures. 2009.\\n[5] M. Eichner, M. Marin-Jimenez, A. Zisserman, and V . Ferrari.\\nArticulated human pose estimation and search in (almost)\\nunconstrained still images. ETH Zurich, D-ITET, BIWI,\\nTechnical Report No , 272, 2010.\\n[6] P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial structures for object recognition. International Journal of Computer Vision , 61(1):55–79, 2005.\\n[7] V . Ferrari, M. Marin-Jimenez, and A. Zisserman. Progressive\\nsearch space reduction for human pose estimation. In CVPR ,\\n2008.\\n[8] M. A. Fischler and R. A. Elschlager. The representation and\\nmatching of pictorial structures. Computers, IEEE Transactions on , 100(1):67–92, 1973.',\n",
       " 'matching of pictorial structures. Computers, IEEE Transactions on , 100(1):67–92, 1973.\\nFigure 8. Visualization of pose results on images from LSP. Each pose is represented as a stick ﬁgure, inferred from predicted joints.\\nDifferent limbs in the same image are colored differently, same limb across different images has the same color.\\nFigure 9. Visualization of pose results on images from FLIC. Meaning of stick ﬁgures is the same as in Fig. 8 above.\\n[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic\\nsegmentation. In CVPR , 2014.\\n[10] G. Gkioxari, P. Arbel ´aez, L. Bourdev, and J. Malik. Articulated pose estimation using discriminative armlet classiﬁers.\\nInCVPR , 2013.\\n[11] C. Ionescu, F. Li, and C. Sminchisescu. Latent structured\\nmodels for human pose estimation. In ICCV , 2011.\\n[12] S. Johnson and M. Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In BMVC ,\\n2010.\\n[13] S. Johnson and M. Everingham. Learning effective human',\n",
       " '2010.\\n[13] S. Johnson and M. Everingham. Learning effective human\\npose estimation from inaccurate annotation. In CVPR , 2011.\\n[14] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In NIPS ,\\n2012.\\n[15] G. Mori and J. Malik. Estimating human body conﬁgurations\\nusing shape context matching. In ECCV , 2002.[16] R. Nevatia and T. O. Binford. Description and recognition of\\ncurved objects. Artiﬁcial Intelligence , 8(1):77–98, 1977.\\n[17] M. Osadchy, Y . LeCun, and M. L. Miller. Synergistic face\\ndetection and pose estimation with energy-based models.\\nThe Journal of Machine Learning Research , 8:1197–1215,\\n2007.\\n[18] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele. Poselet conditioned pictorial structures. In CVPR , 2013.\\n[19] D. Ramanan. Learning to parse images of articulated bodies.\\nInNIPS , 2006.',\n",
       " '[19] D. Ramanan. Learning to parse images of articulated bodies.\\nInNIPS , 2006.\\n[20] B. Sapp and B. Taskar. Modec: Multimodal decomposable\\nmodels for human pose estimation. In CVPR , 2013.\\n[21] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose estimation with parameter-sensitive hashing. In CVPR , 2003.\\n[22] Y . Sun, X. Wang, and X. Tang. Deep convolutional network cascade for facial point detection. In Computer Vision\\nand Pattern Recognition (CVPR), 2013 IEEE Conference on ,\\npages 3476–3483. IEEE, 2013.\\n[23] C. Szegedy, A. Toshev, and D. Erhan. Object detection via\\ndeep neural networks. In NIPS 26 , 2013.\\n[24] G. W. Taylor, R. Fergus, G. Williams, I. Spiro, and C. Bregler. Pose-sensitive embedding by nonlinear nca regression.\\nInNIPS , 2010.\\n[25] Y . Tian, C. L. Zitnick, and S. G. Narasimhan. Exploring the\\nspatial hierarchy of mixture models for human pose estimation. In ECCV , 2012.',\n",
       " 'spatial hierarchy of mixture models for human pose estimation. In ECCV , 2012.\\n[26] F. Wang and Y . Li. Beyond physical connections: Tree models in human pose estimation. In CVPR , 2013.\\n[27] Y . Yang and D. Ramanan. Articulated pose estimation with\\nﬂexible mixtures-of-parts. In CVPR , 2011.',\n",
       " '1\\nReturn of the Devil in the Details:\\nDelving Deep into Convolutional Nets\\nKen Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman\\nVisual Geometry Group, Department of Engineering Science, University of Oxford\\nfken,karen,vedaldi,az g@robots.ox.ac.uk\\nAbstract —The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, signiﬁcantly raising the interest of the community in\\nthese methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous\\nstate-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper\\nconducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on\\na common ground, identifying and disclosing important implementation details. We identify several useful properties\\nof CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced\\nsigniﬁcantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods\\nthat can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to\\nCNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source',\n",
       " 'CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source\\ncode and models to reproduce the experiments in the paper is made publicly available.\\nF\\n1 I NTRODUCTION\\nPERHAPS the single most important design\\nchoice in current state-of-the-art image classiﬁcation and object recognition systems is the choice\\nof visual features, or image representation. In fact,\\nmost of the quantitative improvements to image\\nunderstanding obtained in the past dozen years can\\nbe ascribed to the introduction of improved representations, from the Bag-of-Visual-Words (BoVW) [1],\\n[2] to the (Improved) Fisher Vector (IFV) [3]. A\\ncommon characteristic of these methods is that\\nthey are largely handcrafted . They are also relatively\\nsimple, comprising dense sampling of local image\\npatches, describing them by means of visual descriptors such as SIFT, encoding them into a highdimensional representation, and then pooling over\\nthe image. Recently, these handcrafted approaches\\nhave been substantially outperformed by the introduction of the latest generation of Convolutional\\nNeural Networks (CNNs) [4] to the computer vision\\nﬁeld. These networks have a substantially more\\nsophisticated structure than standard representations, comprising several layers of non-linear feature extractors, and are therefore said to be deep',\n",
       " 'ﬁeld. These networks have a substantially more\\nsophisticated structure than standard representations, comprising several layers of non-linear feature extractors, and are therefore said to be deep\\n(in contrast, classical representation will be referred\\nto as shallow ). Furthermore, while their structureis handcrafted, they contain a very large number\\nof parameters learnt from data. When applied to\\nstandard image classiﬁcation and object detection\\nbenchmark datasets such as ImageNet ILSVRC [5]\\nand PASCAL VOC [6] such networks have demonstrated excellent performance [7], [8], [9], [10], [11],\\nsigniﬁcantly better than standard image encodings [12].\\nDespite these impressive results, it remains unclear how different deep architectures compare to\\neach other and to shallow computer vision methods such as IFV . Most papers did not test these\\nrepresentations extensively on a common ground,\\nso a systematic evaluation of the effect of different design and implementation choices remains\\nlargely missing. As noted in our previous work [12],\\nwhich compared the performance of various shallow visual encodings, the performance of computer\\nvision systems depends signiﬁcantly on implementation\\ndetails . For example, state-of-the-art methods such\\nas [13] not only involve the use of a CNN, but\\nalso include other improvements such as the use',\n",
       " 'details . For example, state-of-the-art methods such\\nas [13] not only involve the use of a CNN, but\\nalso include other improvements such as the use\\nof very large scale datasets, GPU computation, and\\ndata augmentation (also known as data jittering or\\nvirtual sampling). These improvements could also\\ntransfer to shallow representations such as the IFV ,\\npotentially explaining a part of the performancearXiv:1405.3531v4  [cs.CV]  5 Nov 2014\\n2\\ngap [14].\\nIn this study we analyse and empirically clarify\\nthese issues, conducting a large set of rigorous\\nexperiments (Sect. 4), in many ways picking up\\nthe story where it last ended in [12] with the\\ncomparison of shallow encoders. We focus on methods to construct image representations ,i.e. encoding functions \\x1emapping an image Ito a vector\\n\\x1e(I)2Rdsuitable for analysis with a linear classiﬁer, such as an SVM. We consider three scenarios\\n(Sect. 2, Sect. 3): shallow image representations,\\ndeep representations pre-trained on outside data,\\nand deep representation pre-trained and then ﬁnetuned on the target dataset. As part of our tests,\\nwe explore generally-applicable best practices that',\n",
       " 'and deep representation pre-trained and then ﬁnetuned on the target dataset. As part of our tests,\\nwe explore generally-applicable best practices that\\nare nevertheless more often found in combination with CNNs [13] or, alternatively, with shallow\\nencoders [12], porting them with mutual beneﬁt.\\nThese are (Sect. 2): the use of colour information ,\\nfeature normalisation , and, most importantly, the\\nuse of substantial data augmentation . We also determine scenario-speciﬁc best-practices , improving\\nthe ones in [12], [15] and others, including dimensionality reduction for deep features. Finally, we\\nachieve performance competitive with the state\\nof the art [16], [17] on PASCAL VOC classiﬁcation\\nusing less additional training data and signiﬁcantly\\nsimpler techniques. As in [12], the source code and\\nmodels to reproduce all experiments in this paper\\nis available on the project website1.\\n2 S CENARIOS\\nThis section introduces the three types of image\\nrepresentation \\x1e(I)considered in this paper, describing them within the context of three different\\nscenarios. Having outlined details speciﬁc to each,\\ngeneral methodologies which apply to all three\\nscenarios are reviewed, such as data augmentation',\n",
       " 'scenarios. Having outlined details speciﬁc to each,\\ngeneral methodologies which apply to all three\\nscenarios are reviewed, such as data augmentation\\nand feature normalisation, together with the linear\\nclassiﬁer (trained with a standard hinge loss). We\\nalso specify here the benchmark datasets used in\\nthe evaluation.\\n2.1 Scenario 1: Shallow representation (IFV)\\nOur reference shallow image representation is the\\nIFV [3]. Our choice is motivated by the fact that\\nIFV usually outperforms related encoding methods\\nsuch as BoVW, LLC [12], and VLAD [18]. Given an\\nimageI, the IFV\\x1eFV(I)is obtained by extracting\\n1. http://www.robots.ox.ac.uk/\\x18vgg/research/deep eval/a dense collection of patches and corresponding\\nlocal descriptors xi2RD(e.g. SIFT) from the image\\nat multiple scales. Each descriptor xiis then softquantized using a Gaussian Mixture Model with\\nKcomponents. First and second order differences\\nbetween each descriptor xiand its Gaussian cluster\\nmean\\x16kare accumulated in corresponding blocks\\nuk,vkin the vector \\x1eFV(I)2R2KD, appropriately\\nweighed by the Gaussian soft-assignments and',\n",
       " 'uk,vkin the vector \\x1eFV(I)2R2KD, appropriately\\nweighed by the Gaussian soft-assignments and\\ncovariance, leading to a 2KD-dimensional image\\nrepresentation \\x1eFV(I) = [u>\\n1;v>\\n1;:::u>\\nK;v>\\nK]>:The\\nimproved version of the Fisher vector involves postprocessing \\x1eFVby computing the signed squareroot of its scalar components and normalising the\\nresult to a unit `2norm. The details of this construction can be found in [3]; here we follow the\\nnotation of [12].\\n2.2 Scenario 2: Deep representation (CNN) with\\npre-training\\nOur deep representations are inspired by the success of the CNN of Krizhevsky et al. [13]. As shown\\nin [7], [19], the vector of activities \\x1eCNN(I)of the\\npenultimate layer of a deep CNN, learnt on a large\\ndataset such as ImageNet [5], can be used as a powerful image descriptor applicable to other datasets.\\nNumerous CNN architectures that improve the\\nprevious state of the art obtained using shallow\\nrepresentations have been proposed, but choosing',\n",
       " 'Numerous CNN architectures that improve the\\nprevious state of the art obtained using shallow\\nrepresentations have been proposed, but choosing\\nthe best one remains an open question. Many are inspired by [13]: DeCAF [7], [11], Caffe [20], Oquab et\\nal. [8]. Others use larger networks with a smaller\\nstride of the ﬁrst convolutional layer: Zeiler and\\nFergus [19] and OverFeat [10], [9]. Other differences\\ninclude the CNN pre-training protocols. Here we\\nadopt a single learning framework and experiment\\nwith architectures of different complexity exploring\\ntheir performance-speed trade-off.\\n2.3 Scenario 3: Deep representation (CNN) with\\npre-training and ﬁne-tuning\\nIn Scenario 2 features are trained on one (large)\\ndataset and applied to another (usually smaller).\\nHowever, it was demonstrated [11] that ﬁne-tuning\\na pre-trained CNN on the target data can signiﬁcantly improve the performance. We consider\\nthis scenario separately from that of Scenario 2, as\\nthe image features become dataset-speciﬁc after the\\nﬁne-tuning.\\n3\\n2.4 Commonalities\\nWe now turn to what is in common across the\\nscenarios.',\n",
       " 'ﬁne-tuning.\\n3\\n2.4 Commonalities\\nWe now turn to what is in common across the\\nscenarios.\\n2.4.1 Data augmentation\\nData augmentation is a method applicable to shallow and deep representations, but that has been\\nso far mostly applied to the latter [13], [19]. By\\naugmentation we mean perturbing an image Iby\\ntransformations that leave the underlying class unchanged ( e.g. cropping and ﬂipping) in order to\\ngenerate additional examples of the class. Augmentation can be applied at training time, at test time,\\nor both. The augmented samples can either be taken\\nas-is or combined to form a single feature, e.g. using\\nsum/max-pooling or stacking.\\n2.4.2 Linear predictors\\nAll the representations \\x1e(I)in the three scenarios\\nare used to construct linear predictorshw;\\x1e(I)ifor\\neach class to be recognized. These predictors are\\nlearnt using Support Vector Machines (SVM) by\\nﬁtting wto the available training data by minimizing an objective function balancing a quadratic\\nregularizer and the hinge-loss. The parameter C\\nin the SVM, trading-off regularizer and loss, is\\ndetermined using an held-off validation subset of',\n",
       " 'regularizer and the hinge-loss. The parameter C\\nin the SVM, trading-off regularizer and loss, is\\ndetermined using an held-off validation subset of\\nthe data. Here we use the same learning framework\\nwith all representations. It is common experience\\nthat linear classiﬁers are particularly sensitive to\\nthenormalisation of the data and that, in particular,\\nSVMs tend to beneﬁt from `2normalisation [3] (an\\ninterpretation is that after normalisation the inner\\nproduct corresponds to the cosine similarly).\\n2.5 Benchmark data\\nAs reference benchmark we use the PASCAL\\nVOC [6] data as already done in [12]. The VOC2007 edition contains about 10,000 images split into\\ntrain, validation, and test sets, and labelled with\\ntwenty object classes. A one-vs-rest SVM classiﬁer\\nfor each class is learnt and evaluated independently\\nand the performance is measured as mean Average\\nPrecision (mAP) across all classes. The VOC-2012\\nedition contains roughly twice as many images\\nand does not include test labels; instead, evaluation\\nuses the ofﬁcial PASCAL Evaluation Server. To train\\ndeep representations we use the ILSVRC-2012 challenge dataset. This contains 1,000 object categories',\n",
       " 'uses the ofﬁcial PASCAL Evaluation Server. To train\\ndeep representations we use the ILSVRC-2012 challenge dataset. This contains 1,000 object categories\\nfrom ImageNet [5] with roughly 1.2M training images, 50,000 validation images, and 100,000 testimages. Performance is evaluated using the top-5\\nclassiﬁcation error. Finally, we also evaluate over\\ntheCaltech-101 and Caltech-256 image classiﬁcation benchmarks [21], [22]. For Caltech-101, we\\nfollowed the protocol of [12], and considered three\\nrandom splits into training and testing data, each\\nof which comprises 30 training and up to 30 testing\\nimages per class. For Caltech-256, two random\\nsplits were generated, each of which contains 60\\ntraining images per class, and the rest are used for\\ntesting. On both Caltech datasets, performance is\\nmeasured using mean class accuracy.\\n3 D ETAILS\\nThis section gives the implementation details of the\\nmethods introduced in Sect. 2.\\n3.1 Improved Fisher Vector details\\nOur IFV representation uses a slightly improved\\nsetting compared to the best result of [12].\\nComputation starts by upscaling the image Iby a\\nfactor of 2 [23], followed by SIFT features extraction',\n",
       " 'setting compared to the best result of [12].\\nComputation starts by upscaling the image Iby a\\nfactor of 2 [23], followed by SIFT features extraction\\nwith a stride of 3 pixels at 7 different scales withp\\n2\\nscale increments. These features are square-rooted\\nas suggested by [24], and decorrelated and reduced\\nin dimension from 128Dto80Dusing PCA. A\\nGMM with K= 256 components is learnt from features sampled from the training images. Hence the\\nFisher Vector \\x1eFV(I)has dimension 2KD= 40;960.\\nBefore use in classiﬁcation, the vector is signedsquare-rooted and l2-normalised (square rooting\\ncorrespond to the Hellinger’s kernel map [25]). As\\nin [12], square-rooting is applied twice, once to the\\nraw encodings, and once again after sum pooling\\nand normalisation. In order to capture weak geometrical information, the IFV representation is used\\nin a spatial pyramid [26]. As in [12], the image is\\ndivided into 1\\x021,3\\x021, and 2\\x022spatial subdivisions\\nand corresponding IFVs are computed and stacked\\nwith an overall dimension of 8\\x022KD= 327;680\\nelements.',\n",
       " 'and corresponding IFVs are computed and stacked\\nwith an overall dimension of 8\\x022KD= 327;680\\nelements.\\nIn addition to this standard formulation, we experiment with a few modiﬁcations. The ﬁrst one\\nis the use of intra-normalisation of the descriptor\\nblocks, an idea recently proposed for the VLAD\\ndescriptor [27]. In this case, the `2normalisation\\nis applied to the individual sub-blocks (uk;vk)of\\nthe vector \\x1eFV(I), which helps to alleviate the\\nlocal feature burstiness [28]. In the case of the\\nimproved intra-normalised features, it was found\\n4\\nthat applying the square-rooting only once to the\\nﬁnal encoding produced the best results.\\nThe second modiﬁcation is the use of spatiallyextended local descriptors [23] instead of a spatial\\npyramid. Here descriptors xiare appended with\\ntheir image location (xi;yi)before quantization\\nwith the GMM. Formally, xiis extended, after PCA\\nprojection, with its normalised spatial coordinates:\\n[x>\\ni;xi=W\\x000:5;yi=H\\x000:5]>, whereW\\x02Hare the\\ndimensions of the image. Since the GMM quantizes',\n",
       " 'i;xi=W\\x000:5;yi=H\\x000:5]>, whereW\\x02Hare the\\ndimensions of the image. Since the GMM quantizes\\nboth appearance and location, this allows for spatial\\ninformation to be captured directly by the softquantization process. This method is signiﬁcantly\\nmore memory-efﬁcient than using a spatial pyramid. Speciﬁcally, the PCA-reduced SIFT features\\nare spatially augmented by appending (x;y)yieldingD= 82 dimensional descriptors pooled in a\\n2KD= 41;984dimensional IFV .\\nThe third modiﬁcation is the use of colour features in addition to SIFT descriptors. While colour\\ninformation is used in CNNs [13] and by the original FV paper [3], it was not explored in our previous comparison [12]. We do so here by adopting\\nthe same Local Colour Statistics (LCS) features as\\nused by [3]. LCS is computed by dividing an input\\npatch into a 4\\x024spatial grid (akin to SIFT), and\\ncomputing the mean and variance of each of the\\nLab colour channels for each cell of the grid. The\\nLCS dimensionality is thus 4\\x024\\x022\\x023 = 96 . This\\nis then encoded in a similar manner to SIFT.',\n",
       " 'Lab colour channels for each cell of the grid. The\\nLCS dimensionality is thus 4\\x024\\x022\\x023 = 96 . This\\nis then encoded in a similar manner to SIFT.\\n3.2 Convolutional neural networks details\\nThe CNN-based features are based on three CNN\\narchitectures representative of the state of the art\\n(shown in Table 1) each exploring a different accuracy/speed trade-off. To ensure a fair comparison\\nbetween them, these networks are trained using the\\nsame training protocol and the same implementation, which we developed based on the opensource Caffe framework [20]. `2-normalising the\\nCNN features \\x1eCNN(I)before use in the SVM was\\nfound to be important for performance.\\nOur Fast (CNN-F) architecture is similar to the\\none used by Krizhevsky et al . [13]. It comprises\\n8 learnable layers, 5 of which are convolutional,\\nand the last 3 are fully-connected. The input image\\nsize is 224\\x02224. Fast processing is ensured by the\\n4 pixel stride in the ﬁrst convolutional layer. The\\nmain differences between our architecture and that\\nof [13] are the reduced number of convolutional\\nlayers and the dense connectivity between convolu-tional layers ([13] used sparse connections to enable\\ntraining on two GPUs).',\n",
       " 'of [13] are the reduced number of convolutional\\nlayers and the dense connectivity between convolu-tional layers ([13] used sparse connections to enable\\ntraining on two GPUs).\\nOur Medium (CNN-M) architecture is similar\\nto the one used by Zeiler and Fergus [19]. It is\\ncharacterised by the decreased stride and smaller\\nreceptive ﬁeld of the ﬁrst convolutional layer, which\\nwas shown to be beneﬁcial on the ILSVRC dataset.\\nAt the same time, conv2 uses larger stride (2 instead\\nof 1) to keep the computation time reasonable. The\\nmain difference between our net and that of [19] is\\nwe use less ﬁlters in the conv4 layer (512 vs. 1024).\\nOur Slow (CNN-S) architecture is related to the\\n‘accurate’ network from the OverFeat package [10].\\nIt also uses 7\\x027ﬁlters with stride 2in conv1.\\nUnlike CNN-M and [19], the stride in conv2 is\\nsmaller (1 pixel), but the max-pooling window in\\nconv1 and conv5 is larger ( 3\\x023) to compensate for\\nthe increased spatial resolution. Compared to [10],\\nwe use 5 convolutional layers as in the previous',\n",
       " 'conv1 and conv5 is larger ( 3\\x023) to compensate for\\nthe increased spatial resolution. Compared to [10],\\nwe use 5 convolutional layers as in the previous\\narchitectures ([10] used 6), and less ﬁlters in conv5\\n(512 instead of 1024); we also incorporate an LRN\\nlayer after conv1 ([10] did not use contrast normalisation).\\n3.2.1 CNN training\\nIn general, our CNN training procedure follows that of [13], learning on ILSVRC-2012 using gradient descent with momentum. The hyperparameters are the same as used by [13]: momentum0:9; weight decay 5\\x0110\\x004; initial learning rate\\n10\\x002, which is decreased by a factor of 10, when\\nthe validation error stop decreasing. The layers are\\ninitialised from a Gaussian distribution with a zero\\nmean and variance equal to 10\\x002. We also employ\\nsimilar data augmentation in the form of random\\ncrops, horizontal ﬂips, and RGB colour jittering.\\nTest time crop sampling is discussed in Sect. 3.3;\\nat training time, 224\\x02224 crops are sampled randomly, rather than deterministically. Thus, the only\\nnotable difference to [13] is that the crops are taken\\nfrom the whole training image P\\x02256;P\\x15256,',\n",
       " 'notable difference to [13] is that the crops are taken\\nfrom the whole training image P\\x02256;P\\x15256,\\nrather than its 256\\x02256 centre. Training was performed on a single NVIDIA GTX Titan GPU and\\nthe training time varied from 5 days for CNN-F to\\n3 weeks for CNN-S.\\n3.2.2 CNN ﬁne-tuning on the target dataset\\nIn our experiments, we ﬁne-tuned CNN-S using\\nVOC-2007, VOC-2012, or Caltech-101 as the target\\ndata. Fine-tuning was carried out using the same\\nframework (and the same data augmentation), as\\nwe used for CNN training on ILSVRC. The last\\n5\\nArch. conv1 conv2 conv3 conv4 conv5 full6 full7 full8\\nCNN-F64x11x11 256x5x5 256x3x3 256x3x3 256x3x3 4096 4096 1000\\nst. 4, pad 0 st. 1, pad 2 st. 1, pad 1 st. 1, pad 1 st. 1, pad 1 drop- drop- softLRN, x2 pool LRN, x2 pool - - x2 pool out out max',\n",
       " 'CNN-M96x7x7 256x5x5 512x3x3 512x3x3 512x3x3 4096 4096 1000\\nst. 2, pad 0 st. 2, pad 1 st. 1, pad 1 st. 1, pad 1 st. 1, pad 1 drop- drop- softLRN, x2 pool LRN, x2 pool - - x2 pool out out max\\nCNN-S96x7x7 256x5x5 512x3x3 512x3x3 512x3x3 4096 4096 1000\\nst. 2, pad 0 st. 1, pad 1 st. 1, pad 1 st. 1, pad 1 st. 1, pad 1 drop- drop- softLRN, x3 pool x2 pool - - x3 pool out out max\\nTABLE 1\\nCNN architectures. Each architecture contains 5 convolutional layers (conv 1–5) and three\\nfully-connected layers (full 1–3). The details of each of the convolutional layers are given in three sub-rows:\\nthe ﬁrst speciﬁes the number of convolution ﬁlters and their receptive ﬁeld size as “num x size x size”; the\\nsecond indicates the convolution stride (“st.”) and spatial padding (“pad”); the third indicates if Local',\n",
       " 'second indicates the convolution stride (“st.”) and spatial padding (“pad”); the third indicates if Local\\nResponse Normalisation (LRN) [13] is applied, and the max-pooling downsampling factor. For full 1–3, we\\nspecify their dimensionality, which is the same for all three architectures. Full6 and full7 are regularised\\nusing dropout [13], while the last layer acts as a multi-way soft-max classiﬁer. The activation function for all\\nweight layers (except for full8) is the REctiﬁcation Linear Unit (RELU) [13].\\nfully-connected layer (conv8) has output dimensionality equal to the number of classes, which\\ndiffers between datasets, so we initialised it from\\na Gaussian distribution (as used for CNN training\\nabove). Now we turn to dataset-speciﬁc ﬁne-tuning\\ndetails.\\nVOC-2007 and VOC-2012. Considering that PASCAL VOC is a multi-label dataset ( i.e. a single\\nimage might have multiple labels), we replaced the\\nsoftmax regression loss with a more appropriate\\nloss function, for which we considered two options:\\none-vs-rest classiﬁcation hinge loss (the same loss',\n",
       " 'softmax regression loss with a more appropriate\\nloss function, for which we considered two options:\\none-vs-rest classiﬁcation hinge loss (the same loss\\nas used in the SVM experiments) and ranking hinge\\nloss. Both losses deﬁne constraints on the scores of\\npositive (Ipos) and negative ( Ineg) images for each\\nclass:wc\\x1e(Ipos)>1\\x00\\x18;wc\\x1e(Ineg)<\\x001 +\\x18for the\\nclassiﬁcation loss, wc\\x1e(Ipos)> w c\\x1e(Ineg) + 1\\x00\\x18\\nfor the ranking loss ( wcis thec-th row of the\\nlast fully-connected layer, which can be seen as a\\nlinear classiﬁer on deep features \\x1e(I);\\x18is a slack\\nvariable). Our ﬁne-tuned networks are denoted as\\n“CNN S TUNE-CLS” (for the classiﬁcation loss)\\nand “CNN S TUNE-RNK” (for the ranking loss).\\nIn the case of both VOC datasets, the training\\nand validation subsets were combined to form a\\nsingle training set. Given the smaller size of the\\ntraining data when compared to ILSVRC-2012, we',\n",
       " 'and validation subsets were combined to form a\\nsingle training set. Given the smaller size of the\\ntraining data when compared to ILSVRC-2012, we\\ncontrolled for over-ﬁtting by using lower initial\\nlearning rates for the ﬁne-tuned hidden layers.The learning rate schedule for the last layer /\\nhidden layers was: 10\\x002=10\\x004!10\\x003=10\\x004!\\n10\\x004=10\\x004!10\\x005=10\\x005.\\nCaltech-101 dataset contains a single class label\\nper image, so ﬁne-tuning was performed using the\\nsoftmax regression loss. Other settings (including\\nthe learning rate schedule) were the same as used\\nfor the VOC ﬁne-tuning experiments.\\n3.2.3 Low-dimensional CNN feature training\\nOur baseline networks (Table 1) have the same\\ndimensionality of the last hidden layer (full7): 4096.\\nThis design choice is in accordance with the stateof-the-art architectures [13], [19], [10], and leads to\\na 4096-D dimensional image representation, which\\nis already rather compact compared to IFV . We\\nfurther trained three modiﬁcations of the CNN-M\\nnetwork, with lower dimensional full7 layers of:\\n2048, 1024, and 128 dimensions respectively. The',\n",
       " 'further trained three modiﬁcations of the CNN-M\\nnetwork, with lower dimensional full7 layers of:\\n2048, 1024, and 128 dimensions respectively. The\\nnetworks were learnt on ILSVRC-2012. To speedup training, all layers aside from full7/full8 were\\nset to those of the CNN-M net and a lower initial\\nlearning rate of 10\\x003was used. The initial learning\\nrate of full7/full8 was set to 10\\x002.\\n3.3 Data augmentation details\\nWe explore three data augmentation strategies. The\\nﬁrst strategy is to use no augmentation . In contrast to IFV , however, CNNs require images to\\n6\\nbe transformed to a ﬁxed size ( 224\\x02224) even\\nwhen no augmentation is used. Hence the image is\\ndownsized so that the smallest dimension is equal\\nto224 pixels and a 224\\x02224 crop is extracted\\nfrom the centre.2The second strategy is to use\\nﬂip augmentation , mirroring images about the yaxis producing two samples from each image. The\\nthird strategy, termed C+F augmentation , combines\\ncropping and ﬂipping. For CNN-based representations, the image is downsized so that the smallest\\ndimension is equal to 256 pixels. Then 224\\x02224',\n",
       " 'cropping and ﬂipping. For CNN-based representations, the image is downsized so that the smallest\\ndimension is equal to 256 pixels. Then 224\\x02224\\ncrops are extracted from the four corners and the\\ncentre of the image. Note that the crops are sampled\\nfrom the whole image, rather than its 256\\x02256\\ncentre, as done by [13]. These crops are then ﬂipped\\nabout they-axis, producing 10perturbed samples\\nper input image. In the case of the IFV encoding, the\\nsame crops are extracted, but at the original image\\nresolution.\\n4 A NALYSIS\\nThis section describes the experimental results,\\ncomparing different features and data augmentation schemes. The results are given in Table 2 for\\nVOC-2007 and analysed next, starting from generally applicable methods such as augmentation and\\nthen discussing the speciﬁcs of each scenario. We\\nthen move onto other datasets and the state of the\\nart in Sect. 4.7.\\n4.1 Data augmentation\\nWe experiment with no data augmentation (denoted Image Aug=– in Tab. 2), ﬂip augmentation ( Image Aug=F ), and C+F augmentation ( Image\\nAug=C ). Augmented images are used as standalone samples ( f), or by fusing the corresponding',\n",
       " 'Aug=C ). Augmented images are used as standalone samples ( f), or by fusing the corresponding\\ndescriptors using sum ( s) or max ( m) pooling or\\nstacking ( t). So for example Image Aug=(C) f s in\\nrow [f] of Tab. 2 means that C+F augmentation is\\nused to generate additional samples in training ( f),\\nand is combined with sum-pooling in testing ( s).\\nAugmentation consistently improves performance by\\x183%for both IFV ( e.g. [d] vs. [f]) and\\nCNN ( e.g. [o] vs. [p]). Using additional samples for\\ntraining and sum-pooling for testing works best\\n([p]) followed by sum-pooling [r], max pooling [q],\\nand stacking [s]. In terms of the choice of transformations, ﬂipping improves only marginally ([o] vs.\\n2. Extracting a 224\\x02224 centre crop from a 256\\x02256\\nimage [13] resulted in worse performance.[u]), but using the more expensive C+F sampling\\nimproves, as seen, by about 2\\x183%([o]vs. [p]). We\\nexperimented with sampling more transformations,\\ntaking a higher density of crops from the centre of\\nthe image, but observed no beneﬁt.\\n4.2 Colour',\n",
       " 'experimented with sampling more transformations,\\ntaking a higher density of crops from the centre of\\nthe image, but observed no beneﬁt.\\n4.2 Colour\\nColour information can be added and subtracted in\\nCNN and IFV . In IFV replacing SIFT with the colour\\ndescriptors of [3] (denoted COL inMethod ) yields\\nsigniﬁcantly worse performance ([j] vs. [h]). However, when SIFT and colour descriptors are combined by stacking the corresponding IFVs ( COL+ )\\nthere is a small but signiﬁcant improvement of\\naround\\x181%in the non-augmented case ( e.g. [h]\\nvs. [k]) but little impact in the augmented case\\n(e.g. [i] vs. [l]). For CNNs, retraining the network\\nafter converting all the input images to grayscale\\n(denoted GSinMethods ) has a more signiﬁcant\\nimpact, resulting in a performance drop of \\x183%\\n([w] vs. [p], [v] vs. [o]).\\n4.3 Scenario 1: Shallow representation (IFV)\\nThe baseline IFV encoding using a spatial pyramid\\n[a] performs slightly better than the results [I] taken',\n",
       " '4.3 Scenario 1: Shallow representation (IFV)\\nThe baseline IFV encoding using a spatial pyramid\\n[a] performs slightly better than the results [I] taken\\nfrom Chatﬁeld et al. [12], primarily due to a larger\\nnumber of spatial scales being used during SIFT\\nfeature extraction, and the resultant SIFT features\\nbeing square-rooted. Intra-normalisation , denoted as\\nINin the Method column of the table, improves the\\nperformance by \\x181%(e.g. [c] vs. [d]). More interestingly, switching from spatial pooling (denoted\\nspm in the SPool column) to feature spatial augmentation ( SPool=(x,y) ) has either little effect on the\\nperformance or results in a marginal increase ([a] vs.\\n[c], [b] vs. [d]), whilst resulting in a representation\\nwhich is over 10 \\x02smaller. We also experimented\\nwith augmenting with scale in addition to position\\nas in [23] but observed no improvement. Finally,\\nwe investigate pushing the parameters of the representation setting K= 512 (rows [h]-[l]). Increasing\\nthe number of GMM centres in the model from\\nK= 256 to512 results in a further performance\\nincrease (e.g. [h] vs. [d]), but at the expense of',\n",
       " 'K= 256 to512 results in a further performance\\nincrease (e.g. [h] vs. [d]), but at the expense of\\nhigher-dimensional codes (125K dimensional).\\n4.4 Scenario 2: Deep representation (CNN) with\\npre-training\\nCNN-based methods consistently outperform the\\nshallow encodings, even after the improvements\\n7\\nMethod SPool Image Aug. Dim mAP\\n(I) FK BL spm – 327K 61.69 79.0 67.4 51.9 70.9 30.8 72.2\\n(II) DECAF – (C) t t 327K 73.41 87.4 79.3 84.1 78.4 42.3 73.7\\n(a) FK spm – 327K 63.66 83.4 68.8 59.6 74.1 35.7 71.2\\n(b) FK IN spm – 327K 64.18 82.1 69.7 59.7 75.2 35.7 71.3\\n(c) FK (x,y) – 42K 63.51 83.2 69.4 60.6 73.9 36.3 68.6\\n(d) FK IN (x,y) – 42K 64.36 83.1 70.4 62.4 75.2 37.1 69.1',\n",
       " '(d) FK IN (x,y) – 42K 64.36 83.1 70.4 62.4 75.2 37.1 69.1\\n(e) FK IN (x,y) (F) f - 42K 64.35 83.1 70.5 62.3 75.4 37.1 69.1\\n(f) FK IN (x,y) (C) f s 42K 67.17 85.5 71.6 64.6 77.2 39.0 70.8\\n(g) FK IN (x,y) (C) s s 42K 66.68 84.9 70.1 64.7 76.3 39.2 69.8\\n(h) FK IN 512 (x,y) – 84K 65.36 84.1 70.4 65.0 76.7 37.2 71.3\\n(i) FK IN 512 (x,y) (C) f s 84K 68.02 85.9 71.8 67.1 77.1 38.8 72.3\\n(j) FK IN COL 512 – – 82K 52.18 69.5 52.1 47.5 64.0 24.6 49.8',\n",
       " '(j) FK IN COL 512 – – 82K 52.18 69.5 52.1 47.5 64.0 24.6 49.8\\n(k) FK IN 512 COL+ (x,y) – 166K 66.37 82.9 70.1 67.0 77.0 36.1 70.0\\n(l) FK IN 512 COL+ (x,y) (C) f s 166K 67.93 85.1 70.5 67.5 77.4 35.7 71.2\\n(m) CNN F – (C) f s 4K 77.38 88.7 83.9 87.0 84.7 46.9 77.5\\n(n) CNN S – (C) f s 4K 79.74 90.7 85.7 88.9 86.6 50.5 80.1\\n(o) CNN M – – 4K 76.97 89.5 84.3 88.8 83.2 48.4 77.0\\n(p) CNN M – (C) f s 4K 79.89 91.7 85.4 89.5 86.6 51.6 79.3\\n(q) CNN M – (C) f m 4K 79.50 90.9 84.6 89.4 85.8 50.3 78.4',\n",
       " '(q) CNN M – (C) f m 4K 79.50 90.9 84.6 89.4 85.8 50.3 78.4\\n(r) CNN M – (C) s s 4K 79.44 91.4 85.2 89.1 86.1 52.1 78.0\\n(s) CNN M – (C) t t 41K 78.77 90.7 85.0 89.2 85.8 51.0 77.8\\n(t) CNN M – (C) f - 4K 77.78 90.5 84.3 88.8 84.5 47.9 78.0\\n(u) CNN M – (F) f - 4K 76.99 90.1 84.2 89.0 83.5 48.1 77.2\\n(v) CNN M GS – – 4K 73.59 87.4 80.8 82.4 82.1 44.5 73.5\\n(w) CNN M GS – (C) f s 4K 77.00 89.4 83.8 85.1 84.4 49.4 77.6\\n(x) CNN M 2048 – (C) f s 2K 80.10 91.3 85.8 89.9 86.7 52.4 79.7',\n",
       " '(x) CNN M 2048 – (C) f s 2K 80.10 91.3 85.8 89.9 86.7 52.4 79.7\\n(y) CNN M 1024 – (C) f s 1K 79.91 91.4 86.9 89.3 85.8 53.3 79.8\\n(z) CNN M 128 – (C) f s 128 78.60 91.3 83.9 89.2 86.9 52.1 81.0\\n(a) FK+CNN F (x,y) (C) f s 88K 77.95 89.6 83.1 87.1 84.5 48.0 79.4\\n(b) FK+CNN M 2048 (x,y) (C) f s 86K 80.14 90.9 85.9 88.8 85.5 52.3 81.4\\n(g) CNN S TUNE-RNK – (C) f s 4K 82.42 95.3 90.4 92.5 89.6 54.4 81.9\\nTABLE 2\\nVOC 2007 results (continued overleaf) . See Sect. 4 for details.\\ndiscussed above, by a large \\x1810% mAP margin\\n([i]vs. [p]). Our small architecture CNN-F, which is',\n",
       " 'discussed above, by a large \\x1810% mAP margin\\n([i]vs. [p]). Our small architecture CNN-F, which is\\nsimilar to DeCAF [7], performs signiﬁcantly better\\nthan the latter ([II] vs. [s]), validating our implementation. Both medium CNN-M [m] and slow\\nCNN-S [p] outperform the fast CNN-F [m] by a\\nsigniﬁcant 2\\x183% margin. Since the accuracy\\nof CNN-S and CNN-M is nearly the same, we\\nfocus on the latter as it is simpler and marginally\\n(\\x1825%) faster. Remarkably, these good networks\\nwork very well even with no augmentation [o].\\nAnother advantage of CNNs compared to IFV is\\nthe small dimensionality of the output features,although IFV can be compressed to an extent. We\\nexplored retraining the CNNs such that the ﬁnal\\nlayer was of a lower dimensionality, and reducing\\nfrom 4096 to 2048 actually resulted in a marginal\\nperformance boost ([x] vs. [p]). What is surprising\\nis that we can reduce the output dimensionality\\nfurther to 1024D [y] and even 128D [z] with only\\na drop of\\x182%for codes that are 32\\x02smaller',\n",
       " 'further to 1024D [y] and even 128D [z] with only\\na drop of\\x182%for codes that are 32\\x02smaller\\n(\\x18650\\x02smaller than our best performing IFV [i]).\\nNote,`2-normalising the features accounted for up\\nto\\x185%of their performance over VOC 2007; it\\nshould be applied before input to the SVM and\\nafter pooling the augmented descriptors (where\\n8\\n(I) 79.9 61.4 56.0 49.6 58.4 44.8 78.8 70.8 85.0 31.7 51.0 56.4 80.2 57.5\\n(II) 83.7 83.7 54.3 61.9 70.2 79.5 85.3 77.2 90.9 51.1 73.8 57.0 86.4 68.0\\n(a) 80.7 64.4 53.8 53.8 60.2 47.8 79.9 68.9 86.1 37.3 51.1 55.8 83.7 56.9\\n(b) 80.6 64.8 53.9 54.9 60.7 50.5 80.4 69.5 86.2 38.3 54.4 56.3 82.7 56.7',\n",
       " '(c) 81.1 64.2 51.1 53.4 61.9 50.0 80.0 67.5 85.3 35.7 51.9 53.8 83.5 58.9\\n(d) 80.5 66.9 50.9 53.9 62.1 51.5 80.5 68.5 85.9 37.2 55.2 54.3 83.3 59.2\\n(e) 80.5 66.8 51.0 54.1 62.2 51.5 80.4 68.2 86.0 37.3 55.1 54.2 83.3 59.2\\n(f) 82.4 71.6 52.8 62.4 63.4 57.1 81.6 70.9 86.9 41.2 61.2 56.9 85.2 61.5\\n(g) 81.9 71.0 52.8 61.6 62.2 56.8 81.8 70.0 86.5 41.5 61.0 56.5 84.3 60.9\\n(h) 81.1 67.9 52.6 55.4 61.4 51.2 80.5 69.1 86.4 41.2 56.0 56.2 83.7 59.9',\n",
       " '(i) 82.5 73.2 54.7 62.7 64.5 56.6 82.2 71.3 87.5 43.0 62.0 59.3 85.7 62.4\\n(j) 66.1 46.6 42.5 35.8 41.1 45.5 75.4 58.3 83.9 39.8 47.3 35.6 69.2 49.0\\n(k) 80.0 65.9 52.8 56.1 61.0 56.9 81.4 69.6 88.4 49.0 59.2 56.4 84.7 62.8\\n(l) 81.6 70.8 52.9 59.6 63.1 59.9 82.1 70.5 88.9 50.6 63.7 57.5 86.1 64.1\\n(m) 86.3 85.4 58.6 71.0 72.6 82.0 87.9 80.7 91.8 58.5 77.4 66.3 89.1 71.3\\n(n) 87.8 88.3 61.3 74.8 74.7 87.2 89.0 83.7 92.3 58.8 80.5 69.4 90.5 74.0',\n",
       " '(o) 85.1 87.4 58.1 70.4 73.1 83.5 85.5 80.9 90.8 54.1 78.9 61.1 89.0 70.4\\n(p) 87.7 88.6 60.3 80.1 74.4 85.9 88.2 84.6 92.1 60.3 80.5 66.2 91.3 73.5\\n(q) 87.6 88.6 60.7 78.2 73.6 86.0 87.4 83.8 92.3 59.3 81.0 66.8 91.3 74.0\\n(r) 87.5 88.1 60.4 76.9 74.8 85.8 88.1 84.3 92.2 59.5 79.3 65.8 90.8 73.5\\n(s) 87.3 87.6 60.1 72.3 75.3 85.2 86.9 82.6 91.9 58.5 77.9 66.5 90.5 73.4\\n(t) 85.7 87.9 58.3 74.2 73.9 84.7 86.6 82.0 91.0 55.8 79.2 62.1 89.3 71.0',\n",
       " '(u) 85.3 87.3 58.1 70.0 73.4 83.5 86.0 80.8 90.9 53.9 78.1 61.2 88.8 70.6\\n(v) 85.0 84.9 57.8 65.9 69.8 79.5 82.9 77.4 89.2 42.8 71.7 60.2 86.3 67.8\\n(w) 87.2 86.5 59.5 72.4 74.1 81.7 86.0 82.3 90.8 48.9 73.7 66.8 89.6 71.0\\n(x) 87.6 88.4 60.2 76.9 75.4 85.5 88.0 83.4 92.1 61.1 83.1 68.5 91.9 74.2\\n(y) 87.8 88.6 59.0 77.2 73.1 85.9 88.3 83.5 91.8 59.9 81.4 68.3 93.0 74.1\\n(z) 86.6 87.5 59.1 70.0 72.9 84.6 86.7 83.6 89.4 57.0 81.5 64.8 90.4 73.4',\n",
       " '(a) 86.8 85.6 59.9 72.0 73.4 81.4 88.6 80.5 92.1 60.6 77.3 66.4 89.3 73.3\\n(b) 87.7 88.4 61.2 76.9 76.6 84.9 89.1 82.9 92.4 61.9 80.9 68.7 91.5 75.1\\n(g) 91.5 91.9 64.1 76.3 74.9 89.7 92.2 86.9 95.2 60.7 82.9 68.0 95.5 74.4\\nTABLE 2\\nVOC 2007 results (continued from previous page)\\napplicable).\\n4.5 Scenario 3: Deep representation (CNN) with\\npre-training and ﬁne-tuning\\nWe ﬁne-tuned our CNN-S architecture on VOC2007 using the ranking hinge loss, and achieved\\na signiﬁcant improvement: 2:7%([g]vs. [n]). This\\ndemonstrates that in spite of the small amount\\nof VOC training data (5,011 images), ﬁne-tuning\\nis able to adjust the learnt deep representation to\\nbetter suit the dataset in question.4.6 Combinations\\nFor the CNN-M 2048 representation [x], stacking',\n",
       " 'is able to adjust the learnt deep representation to\\nbetter suit the dataset in question.4.6 Combinations\\nFor the CNN-M 2048 representation [x], stacking\\ndeep and shallow representations to form a higherdimensional descriptor makes little difference ([x]\\nvs. [b]). For the weaker CNN-F it results in a small\\nboost of\\x180:8%([m] vs. [a]).\\n4.7 Comparison with the state of the art\\nIn Table 3 we report our results on ILSVRC-2012,\\nVOC-2007, VOC-2012, Caltech-101, and Caltech-256\\ndatasets, and compare them to the state of the art.\\nFirst, we note that the ILSVRC error rates of our\\nCNN-F, CNN-M, and CNN-S networks are better\\n9\\nILSVRC-2012 VOC-2007 VOC-2012 Caltech-101 Caltech-256\\n(top-5 error) (mAP) (mAP) (accuracy) (accuracy)\\n(a) FK IN 512 - 68.0 – – –\\n(b) CNN F 16.7 77.4 79.9 – –\\n(c) CNN M 13.7 79.9 82.5 87.15 \\x060.80 77.03\\x060.46',\n",
       " '(c) CNN M 13.7 79.9 82.5 87.15 \\x060.80 77.03\\x060.46\\n(d) CNN M 2048 13.5 80.1 82.4 86.64 \\x060.53 76.88\\x060.35\\n(e) CNN S 13.1 79.7 82.9 87.76 \\x060.66 77.61\\x060.12\\n(f) CNN S TUNE-CLS 13.1 - 83.0 88.35\\x060.56 77.33\\x060.56\\n(g) CNN S TUNE-RNK 13.1 82.4 83.2 – –\\n(h) Zeiler & Fergus [19] 16.1 - 79.0 86.5 \\x060.5 74.2\\x060.3\\n(i) Razavian et al. [9], [10] 14.7 77.2 – – –\\n(j) Oquab et al. [8] 18 77.7 78.7 (82.8*) – –\\n(k) Oquab et al. [16] - - 86.3*– –\\n(l) Wei et al. [17] - 81.5 ( 85.2*) 81.7 ( 90.3*) – –',\n",
       " '(l) Wei et al. [17] - 81.5 ( 85.2*) 81.7 ( 90.3*) – –\\n(m) He et al. [29] 13.6 80.1 - 91.4\\x060.7 –\\nTABLE 3\\nComparison with the state of the art on ILSVRC2012, VOC2007, VOC2012, Caltech-101, and\\nCaltech-256. Results marked with * were achieved using models pre-trained on the extended ILSVRC\\ndatasets (1512 classes in [8], [16], 2000 classes in [17]). All other results were achieved using CNNs\\npre-trained on ILSVRC-2012 (1000 classes).\\nthan those reported by [13], [19], and [10] for the\\nrelated conﬁgurations. This validates our implementation, and the difference is likely to be due to\\nthe sampling of image crops from the uncropped\\nimage plane (instead of the centre). When using\\nour CNN features on other datasets, the relative\\nperformance generally follows the same pattern\\nas on ILSVRC, where the nets are trained – the\\nCNN-F architecture exhibits the worst performance,\\nwith CNN-M and CNN-S performing considerably\\nbetter.\\nFurther ﬁne-tuning of CNN-S on the VOC',\n",
       " 'CNN-F architecture exhibits the worst performance,\\nwith CNN-M and CNN-S performing considerably\\nbetter.\\nFurther ﬁne-tuning of CNN-S on the VOC\\ndatasets turns out to be beneﬁcial; on VOC-2012,\\nusing the ranking loss is marginally better than\\nthe classiﬁcation loss ([g] vs. [f]), which can be\\nexplained by the ranking-based VOC evaluation\\ncriterion. Fine-tuning on Caltech-101 also yields a\\nsmall improvement, but no gain is observed over\\nCaltech-256.\\nOur CNN-S net is competitive with recent CNNbased approaches [19], [9], [8], [16], [17], [29] and\\non a number of datasets (VOC-2007, VOC-2012,\\nCaltech-101, Caltech-256) and sets the state of the\\nart on VOC-2007 and VOC-2012 across methods\\npre-trained solely on ILSVRC-2012 dataset. While\\nthe CNN-based methods of [16], [17] achieve better',\n",
       " 'pre-trained solely on ILSVRC-2012 dataset. While\\nthe CNN-based methods of [16], [17] achieve better\\nperformance on VOC (86.3% and 90.3% respectively), they were trained using extended ILSVRCdatasets, enriched with additional categories semantically close to the ones in VOC. Additionally, [17] used a signiﬁcantly more complex classiﬁcation pipeline, driven by bounding box proposals [30], pre-trained on ILSVRC-2013 detection\\ndataset. Their best reported result on VOC-2012\\n(90.3%) was achieved by the late fusion with a\\ncomplex hand-crafted method of [31]; without fusion, they get 84.2%. On Caltech-101, [29] achieves\\nthe state of the art using spatial pyramid pooling\\nof conv5 layer features, while we used full7 layer\\nfeatures consistently across all datasets (for full7\\nfeatures, they report 87:08%).\\nIn addition to achieving performance comparable\\nto the state of the art with a very simple approach\\n(but powerful CNN-based features), with the modiﬁcations outlined in the paper (primarily the use\\nof data augmentation similar to the CNN-based\\nmethods) we are able to improve the performance',\n",
       " 'of data augmentation similar to the CNN-based\\nmethods) we are able to improve the performance\\nof shallow IFV to 68.02% (Table 2, [i]).\\n4.8 Performance Evolution on VOC-2007\\nA comparative plot of the evolution in the performance of the methods evaluated in this paper,\\nalong with a selection from our earlier review of\\nshallow methods [12] is presented in Fig. 1. Classiﬁcation accuracy over PASCAL VOC was 54.48%\\nmAP for the BoVW model in 2008, 61.7% for the\\n10\\n      BOW\\n32K\\n–FK-BL\\n327K\\n–\\n[I]FK\\n327K\\n–\\n[a]FK-IN\\n84K\\nf s\\n[i]DeCAF\\n4K\\nt t\\n[II]CNN-F\\n4K\\nf s\\n[m]CNN-M 2K\\n2K\\nf s\\n[y]CNN-S\\n4K (TN)\\nf s\\n[γ]   \\n                           \\n545658606264666870727476788082mAP 68.02\\n54.4861.6963.6673.4177.1580.132008 2010 2013 2014 ...\\n82.42\\nMethod\\nDim.\\nAug.\\nRef.',\n",
       " '54.4861.6963.6673.4177.1580.132008 2010 2013 2014 ...\\n82.42\\nMethod\\nDim.\\nAug.\\nRef.\\nFig. 1. Evolution of Performance on PASCAL VOC-2007 over the recent years. Please refer to Table 2\\nfor details and references.\\nIFV in 2010 [12], and 73.41% for DeCAF [7] and\\nsimilar [8], [9] CNN-based methods introduced in\\nlate 2013. Our best performing CNN-based method\\n(CNN-S with ﬁne-tuning) achieves 82.42%, comparable to the most recent state-of-the-art.\\n4.9 Timings and dimensionality\\nOne of our best-performing CNN representations\\nCNN-M-2048 [x] is \\x1842\\x02more compact than the\\nbest performing IFV [i] (84K vs. 2K) and CNN-M\\nfeatures are also \\x1850\\x02faster to compute ( \\x18120s\\nvs.\\x182:4sper image with augmentation enabled,\\nover a single CPU core). Non-augmented CNN-M\\nfeatures [o] take around 0:3sper image, compared\\nto\\x180:4sfor CNN-S features and \\x180:13sfor CNNF features.\\n5 C ONCLUSION',\n",
       " 'to\\x180:4sfor CNN-S features and \\x180:13sfor CNNF features.\\n5 C ONCLUSION\\nIn this paper we presented a rigorous empirical evaluation of CNN-based methods for image\\nclassiﬁcation, along with a comparison with more\\ntraditional shallow feature encoding methods. We\\nhave demonstrated that the performance of shallow representations can be signiﬁcantly improved\\nby adopting data augmentation, typically used indeep learning. In spite of this improvement, deep\\narchitectures still outperform the shallow methods\\nby a large margin. We have shown that the performance of deep representations on the ILSVRC\\ndataset is a good indicator of their performance\\non other datasets, and that ﬁne-tuning can further\\nimprove on already very strong results achieved\\nusing the combination of deep representations and\\na linear SVM. Source code and CNN models to\\nreproduce the experiments presented in the paper\\nare available on the project website [32] in the\\nhope that it would provide common ground for\\nfuture comparisons, and good baselines for image\\nrepresentation research.\\nACKNOWLEDGEMENTS\\nThis work was supported by the EPSRC and ERC\\ngrant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the\\ndonation of the GPUs used for this research.\\nREFERENCES',\n",
       " 'grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the\\ndonation of the GPUs used for this research.\\nREFERENCES\\n[1] G. Csurka, C. Bray, C. Dance, and L. Fan, “Visual categorization with bags of keypoints,” in Workshop on Statistical\\nLearning in Computer Vision, ECCV , 2004, pp. 1–22.\\n11\\n[2] J. Sivic and A. Zisserman, “Video Google: A text retrieval\\napproach to object matching in videos,” in Proc. ICCV ,\\nvol. 2, 2003, pp. 1470–1477.\\n[3] F. Perronnin, J. S ´anchez, and T. Mensink, “Improving the\\nFisher kernel for large-scale image classiﬁcation,” in Proc.\\nECCV , 2010.\\n[4] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,” Neural\\nComputation , vol. 1, no. 4, pp. 541–551, 1989.',\n",
       " 'Computation , vol. 1, no. 4, pp. 541–551, 1989.\\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\\n“Imagenet: A large-scale hierarchical image database,” in\\nProc. CVPR , 2009.\\n[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman, “The PASCAL Visual Object Classes\\n(VOC) challenge,” IJCV , vol. 88, no. 2, pp. 303–338, 2010.\\n[7] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell, “Decaf: A deep convolutional\\nactivation feature for generic visual recognition,” CoRR ,\\nvol. abs/1310.1531, 2013.\\n[8] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning\\nand Transferring Mid-Level Image Representations using\\nConvolutional Neural Networks,” in Proc. CVPR , 2014.',\n",
       " 'and Transferring Mid-Level Image Representations using\\nConvolutional Neural Networks,” in Proc. CVPR , 2014.\\n[9] A. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson,\\n“CNN Features off-the-shelf: an Astounding Baseline for\\nRecognition,” CoRR , vol. abs/1403.6382, 2014.\\n[10] P . Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y. LeCun, “OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks,” in\\nProc. ICLR , 2014.\\n[11] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich\\nfeature hierarchies for accurate object detection and semantic segmentation,” in Proc. CVPR , 2014.\\n[12] K. Chatﬁeld, V . Lempitsky, A. Vedaldi, and A. Zisserman,\\n“The devil is in the details: an evaluation of recent feature\\nencoding methods,” in Proc. BMVC. , 2011.',\n",
       " '“The devil is in the details: an evaluation of recent feature\\nencoding methods,” in Proc. BMVC. , 2011.\\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet\\nclassiﬁcation with deep convolutional neural networks,”\\ninNIPS , 2012, pp. 1106–1114.\\n[14] M. Paulin, J. Revaud, Z. Harchaoui, F. Perronnin, and\\nC. Schmid, “Transformation Pursuit for Image Classiﬁcation,” in Proc. CVPR , 2014.\\n[15] F. Perronnin, Z. Akata, Z. Harchaoui, and C. Schmid,\\n“Towards good practice in large-scale learning for image\\nclassiﬁcation,” in Proc. CVPR , 2012, pp. 3482–3489.\\n[16] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Weakly\\nsupervised object recognition with convolutional neural\\nnetworks,” INRIA, Tech. Rep. HAL-01015140, 2014.',\n",
       " 'supervised object recognition with convolutional neural\\nnetworks,” INRIA, Tech. Rep. HAL-01015140, 2014.\\n[17] Y. Wei, W. Xia, J. Huang, B. Ni, J. Dong, Y. Zhao, and\\nS. Yan, “CNN: Single-label to multi-label,” CoRR , vol.\\nabs/1406.5726, 2014.\\n[18] H. J ´egou, F. Perronnin, M. Douze, J. S ´anchez, P . P ´erez, and\\nC. Schmid, “Aggregating local images descriptors into\\ncompact codes,” IEEE P AMI , 2012.\\n[19] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional networks,” CoRR , vol. abs/1311.2901,\\n2013.\\n[20] Y. Jia, “Caffe: An open source convolutional architecture\\nfor fast feature embedding,” http://caffe.berkeleyvision.\\norg/, 2013.\\n[21] L. Fei-Fei, R. Fergus, and P . Perona, “Learning generative',\n",
       " 'org/, 2013.\\n[21] L. Fei-Fei, R. Fergus, and P . Perona, “Learning generative\\nvisual models from few training examples: An incremental bayesian approach tested on 101 object categories,”\\ninIEEE CVPR Workshop of Generative Model Based Vision ,\\n2004.[22] G. Grifﬁn, A. Holub, and P . Perona, “Caltech-256 object\\ncategory dataset,” California Institute of Technology,\\nTech. Rep. 7694, 2007. [Online]. Available: http://\\nauthors.library.caltech.edu/7694\\n[23] J. S ´anchez, F. Perronnin, and T. Em ´ıdio de Campos,\\n“Modeling the spatial layout of images beyond spatial\\npyramids,” Pattern Recognition Letters , vol. 33, no. 16, pp.\\n2216–2223, 2012.\\n[24] R. Arandjelovi ´c and A. Zisserman, “Three things everyone should know to improve object retrieval,” in Proc.\\nCVPR , 2012.\\n[25] A. Vedaldi and A. Zisserman, “Efﬁcient additive kernels\\nvia explicit feature maps,” IEEE P AMI , 2011.',\n",
       " '[25] A. Vedaldi and A. Zisserman, “Efﬁcient additive kernels\\nvia explicit feature maps,” IEEE P AMI , 2011.\\n[26] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond Bags\\nof Features: Spatial Pyramid Matching for Recognizing\\nNatural Scene Categories,” in Proc. CVPR , 2006.\\n[27] R. Arandjelovi ´c and A. Zisserman, “All about VLAD,” in\\nProc. CVPR , 2013.\\n[28] H. J ´egou, M. Douze, and C. Schmid, “On the burstiness\\nof visual elements,” in Proc. CVPR , Jun 2009.\\n[29] K. He, A. Zhang, S. Ren, and J. Sun, “Spatial pyramid\\npooling in deep convolutional networks for visual recognition,” in Proc. ECCV , 2014.\\n[30] M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P . H. S. Torr,\\n“BING: Binarized normed gradients for objectness estimation at 300fps,” in Proc. CVPR , 2014.',\n",
       " '“BING: Binarized normed gradients for objectness estimation at 300fps,” in Proc. CVPR , 2014.\\n[31] S. Yan, J. Dong, Q. Chen, Z. Song, Y. Pan, W. Xia,\\nH. Zhongyang, Y. Hua, and S. Shen, “Generalized hierarchical matching for subcategory aware object classiﬁcation,” in The P ASCAL Visual Object Classes Challenge\\nWorkshop , 2012.\\n[32] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman,\\n“Return of the devil in the details: delving deep into\\nconvolutional nets webpage,” 2014. [Online]. Available:\\nhttp://www.robots.ox.ac.uk/\\x18vgg/research/deep eval',\n",
       " 'Learning Phrase Representations using RNN Encoder–Decoder\\nfor Statistical Machine Translation\\nKyunghyun Cho\\nBart van Merri ¨enboer Caglar Gulcehre\\nUniversit ´e de Montr ´eal\\nfirstname.lastname@umontreal.caDzmitry Bahdanau\\nJacobs University, Germany\\nd.bahdanau@jacobs-university.de\\nFethi Bougares Holger Schwenk\\nUniversit ´e du Maine, France\\nfirstname.lastname@lium.univ-lemans.frYoshua Bengio\\nUniversit ´e de Montr ´eal, CIFAR Senior Fellow\\nfind.me@on.the.web\\nAbstract\\nIn this paper, we propose a novel neural network model called RNN Encoder–\\nDecoder that consists of two recurrent\\nneural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other\\ndecodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly\\ntrained to maximize the conditional probability of a target sequence given a source',\n",
       " 'decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly\\ntrained to maximize the conditional probability of a target sequence given a source\\nsequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an\\nadditional feature in the existing log-linear\\nmodel. Qualitatively, we show that the\\nproposed model learns a semantically and\\nsyntactically meaningful representation of\\nlinguistic phrases.\\n1 Introduction\\nDeep neural networks have shown great success in\\nvarious applications such as objection recognition\\n(see, e.g., (Krizhevsky et al., 2012)) and speech\\nrecognition (see, e.g., (Dahl et al., 2012)). Furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing (NLP).\\nThese include, but are not limited to, language\\nmodeling (Bengio et al., 2003), paraphrase detection (Socher et al., 2011) and word embedding extraction (Mikolov et al., 2013). In the ﬁeld of statistical machine translation (SMT), deep neural\\nnetworks have begun to show promising results.\\n(Schwenk, 2012) summarizes a successful usage\\nof feedforward neural networks in the framework',\n",
       " 'networks have begun to show promising results.\\n(Schwenk, 2012) summarizes a successful usage\\nof feedforward neural networks in the framework\\nof phrase-based SMT system.Along this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as a part\\nof the conventional phrase-based SMT system.\\nThe proposed neural network architecture, which\\nwe will refer to as an RNN Encoder–Decoder , consists of two recurrent neural networks (RNN) that\\nact as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a\\nﬁxed-length vector, and the decoder maps the vector representation back to a variable-length target\\nsequence. The two networks are trained jointly to\\nmaximize the conditional probability of the target\\nsequence given a source sequence. Additionally,\\nwe propose to use a rather sophisticated hidden\\nunit in order to improve both the memory capacity\\nand the ease of training.\\nThe proposed RNN Encoder–Decoder with a\\nnovel hidden unit is empirically evaluated on the\\ntask of translating from English to French. We\\ntrain the model to learn the translation probability of an English phrase to a corresponding French\\nphrase. The model is then used as a part of a standard phrase-based SMT system by scoring each',\n",
       " 'train the model to learn the translation probability of an English phrase to a corresponding French\\nphrase. The model is then used as a part of a standard phrase-based SMT system by scoring each\\nphrase pair in the phrase table. The empirical evaluation reveals that this approach of scoring phrase\\npairs with an RNN Encoder–Decoder improves\\nthe translation performance.\\nWe qualitatively analyze the trained RNN\\nEncoder–Decoder by comparing its phrase scores\\nwith those given by the existing translation model.\\nThe qualitative analysis shows that the RNN\\nEncoder–Decoder is better at capturing the linguistic regularities in the phrase table, indirectly\\nexplaining the quantitative improvements in the\\noverall translation performance. The further analysis of the model reveals that the RNN Encoder–\\nDecoder learns a continuous space representation\\nof a phrase that preserves both the semantic and\\nsyntactic structure of the phrase.arXiv:1406.1078v3  [cs.CL]  3 Sep 2014\\n2 RNN Encoder–Decoder\\n2.1 Preliminary: Recurrent Neural Networks\\nA recurrent neural network (RNN) is a neural network that consists of a hidden state hand an\\noptional output ywhich operates on a variablelength sequence x= (x1;:::;x T). At each time\\nstept, the hidden state hhtiof the RNN is updated\\nby',\n",
       " 'stept, the hidden state hhtiof the RNN is updated\\nby\\nhhti=f\\x00\\nhht\\x001i;xt\\x01\\n; (1)\\nwherefis a non-linear activation function.fmay be as simple as an elementwise logistic sigmoid function and as complex as a long short-term memory (LSTM)\\nunit (Hochreiter and Schmidhuber, 1997).\\nAn RNN can learn a probability distribution\\nover a sequence by being trained to predict the\\nnext symbol in a sequence. In that case, the output\\nat each timestep tis the conditional distribution\\np(xtjxt\\x001;:::;x 1). For example, a multinomial\\ndistribution ( 1-of-Kcoding) can be output using a\\nsoftmax activation function\\np(xt;j= 1jxt\\x001;:::;x 1) =exp\\x00\\nwjhhti\\x01\\nPK\\nj0=1exp\\x00\\nwj0hhti\\x01;\\n(2)\\nfor all possible symbols j= 1;:::;K , where wj\\nare the rows of a weight matrix W. By combining\\nthese probabilities, we can compute the probability of the sequence xusing\\np(x) =TY',\n",
       " \"are the rows of a weight matrix W. By combining\\nthese probabilities, we can compute the probability of the sequence xusing\\np(x) =TY\\nt=1p(xtjxt\\x001;:::;x 1): (3)\\nFrom this learned distribution, it is straightforward to sample a new sequence by iteratively sampling a symbol at each time step.\\n2.2 RNN Encoder–Decoder\\nIn this paper, we propose a novel neural network\\narchitecture that learns to encode a variable-length\\nsequence into a ﬁxed-length vector representation\\nand to decode a given ﬁxed-length vector representation back into a variable-length sequence.\\nFrom a probabilistic perspective, this new model\\nis a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence,\\ne.g.p(y1;:::;y T0jx1;:::;x T), where one\\nx1x2 xTyT' y2 y1\\ncDecoder\\nEncoderFigure 1: An illustration of the proposed RNN\\nEncoder–Decoder.\\nshould note that the input and output sequence\\nlengthsTandT0may differ.\\nThe encoder is an RNN that reads each symbol\\nof an input sequence xsequentially. As it reads\",\n",
       " 'lengthsTandT0may differ.\\nThe encoder is an RNN that reads each symbol\\nof an input sequence xsequentially. As it reads\\neach symbol, the hidden state of the RNN changes\\naccording to Eq. (1). After reading the end of\\nthe sequence (marked by an end-of-sequence symbol), the hidden state of the RNN is a summary c\\nof the whole input sequence.\\nThe decoder of the proposed model is another\\nRNN which is trained to generate the output sequence by predicting the next symbol ytgiven the\\nhidden state hhti. However, unlike the RNN described in Sec. 2.1, both ytandhhtiare also conditioned onyt\\x001and on the summary cof the input\\nsequence. Hence, the hidden state of the decoder\\nat timetis computed by,\\nhhti=f\\x00\\nhht\\x001i;yt\\x001;c\\x01\\n;\\nand similarly, the conditional distribution of the\\nnext symbol is\\nP(ytjyt\\x001;yt\\x002;:::;y 1;c) =g\\x00\\nhhti;yt\\x001;c\\x01\\n:\\nfor given activation functions fandg(the latter\\nmust produce valid probabilities, e.g. with a softmax).\\nSee Fig. 1 for a graphical depiction of the proposed model architecture.',\n",
       " 'for given activation functions fandg(the latter\\nmust produce valid probabilities, e.g. with a softmax).\\nSee Fig. 1 for a graphical depiction of the proposed model architecture.\\nThe two components of the proposed RNN\\nEncoder–Decoder are jointly trained to maximize\\nthe conditional log-likelihood\\nmax\\n\\x121\\nNNX\\nn=1logp\\x12(ynjxn); (4)\\nwhere \\x12is the set of the model parameters and\\neach (xn;yn)is an (input sequence, output sequence) pair from the training set. In our case,\\nas the output of the decoder, starting from the input, is differentiable, we can use a gradient-based\\nalgorithm to estimate the model parameters.\\nOnce the RNN Encoder–Decoder is trained, the\\nmodel can be used in two ways. One way is to use\\nthe model to generate a target sequence given an\\ninput sequence. On the other hand, the model can\\nbe used to score a given pair of input and output\\nsequences, where the score is simply a probability\\np\\x12(yjx)from Eqs. (3) and (4).\\n2.3 Hidden Unit that Adaptively Remembers\\nand Forgets\\nIn addition to a novel model architecture, we also\\npropose a new type of hidden unit ( fin Eq. (1))',\n",
       " 'and Forgets\\nIn addition to a novel model architecture, we also\\npropose a new type of hidden unit ( fin Eq. (1))\\nthat has been motivated by the LSTM unit but is\\nmuch simpler to compute and implement.1Fig. 2\\nshows the graphical depiction of the proposed hidden unit.\\nLet us describe how the activation of the j-th\\nhidden unit is computed. First, the reset gaterjis\\ncomputed by\\nrj=\\x1b\\x10\\n[Wrx]j+\\x02\\nUrhht\\x001i\\x03\\nj\\x11\\n; (5)\\nwhere\\x1bis the logistic sigmoid function, and [:]j\\ndenotes the j-th element of a vector. xandht\\x001\\nare the input and the previous hidden state, respectively. WrandUrare weight matrices which are\\nlearned.\\nSimilarly, the update gatezjis computed by\\nzj=\\x1b\\x10\\n[Wzx]j+\\x02\\nUzhht\\x001i\\x03\\nj\\x11\\n: (6)\\nThe actual activation of the proposed unit hjis\\nthen computed by\\nhhti\\nj=zjhht\\x001i\\nj + (1\\x00zj)~hhti\\nj; (7)\\nwhere\\n~hhti\\nj=\\x1e\\x10\\n[Wx]j+\\x02\\nU\\x00',\n",
       " 'j; (7)\\nwhere\\n~hhti\\nj=\\x1e\\x10\\n[Wx]j+\\x02\\nU\\x00\\nr\\x0chht\\x001i\\x01\\x03\\nj\\x11\\n:(8)\\nIn this formulation, when the reset gate is close\\nto 0, the hidden state is forced to ignore the previous hidden state and reset with the current input\\n1The LSTM unit, which has shown impressive results in\\nseveral applications such as speech recognition, has a memory cell and four gating units that adaptively control the information ﬂow inside the unit, compared to only two gating\\nunits in the proposed hidden unit. For details on LSTM networks, see, e.g., (Graves, 2012).\\nz\\nrh h~xFigure 2: An illustration of the proposed hidden\\nactivation function. The update gate zselects\\nwhether the hidden state is to be updated with\\na new hidden state ~h. The reset gate rdecides\\nwhether the previous hidden state is ignored. See\\nEqs. (5)–(8) for the detailed equations of r,z,h\\nand~h.\\nonly. This effectively allows the hidden state to\\ndrop any information that is found to be irrelevant\\nlater in the future, thus, allowing a more compact\\nrepresentation.\\nOn the other hand, the update gate controls how\\nmuch information from the previous hidden state',\n",
       " 'later in the future, thus, allowing a more compact\\nrepresentation.\\nOn the other hand, the update gate controls how\\nmuch information from the previous hidden state\\nwill carry over to the current hidden state. This\\nacts similarly to the memory cell in the LSTM\\nnetwork and helps the RNN to remember longterm information. Furthermore, this may be considered an adaptive variant of a leaky-integration\\nunit (Bengio et al., 2013).\\nAs each hidden unit has separate reset and update gates, each hidden unit will learn to capture\\ndependencies over different time scales. Those\\nunits that learn to capture short-term dependencies\\nwill tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active.\\nIn our preliminary experiments, we found that\\nit is crucial to use this new unit with gating units.\\nWe were not able to get meaningful result with an\\noft-used tanh unit without any gating.\\n3 Statistical Machine Translation\\nIn a commonly used statistical machine translation\\nsystem (SMT), the goal of the system (decoder,\\nspeciﬁcally) is to ﬁnd a translation fgiven a source\\nsentence e, which maximizes\\np(fje)/p(ejf)p(f);\\nwhere the ﬁrst term at the right hand side is called',\n",
       " 'sentence e, which maximizes\\np(fje)/p(ejf)p(f);\\nwhere the ﬁrst term at the right hand side is called\\ntranslation model and the latter language model\\n(see, e.g., (Koehn, 2005)). In practice, however,\\nmost SMT systems model logp(fje)as a loglinear model with additional features and corresponding weights:\\nlogp(fje) =NX\\nn=1wnfn(f;e) + logZ(e);(9)\\nwherefnandwnare then-th feature and weight,\\nrespectively. Z(e)is a normalization constant that\\ndoes not depend on the weights. The weights are\\noften optimized to maximize the BLEU score on a\\ndevelopment set.\\nIn the phrase-based SMT framework\\nintroduced in (Koehn et al., 2003) and\\n(Marcu and Wong, 2002), the translation model\\nlogp(ejf)is factorized into the translation\\nprobabilities of matching phrases in the source\\nand target sentences.2These probabilities are\\nonce again considered additional features in the\\nlog-linear model (see Eq. (9)) and are weighted\\naccordingly to maximize the BLEU score.\\nSince the neural net language model was proposed in (Bengio et al., 2003), neural networks',\n",
       " 'accordingly to maximize the BLEU score.\\nSince the neural net language model was proposed in (Bengio et al., 2003), neural networks\\nhave been used widely in SMT systems. In\\nmany cases, neural networks have been used to\\nrescore translation hypotheses ( n-best lists) (see,\\ne.g., (Schwenk et al., 2006)). Recently, however,\\nthere has been interest in training neural networks\\nto score the translated sentence (or phrase pairs)\\nusing a representation of the source sentence as\\nan additional input. See, e.g., (Schwenk, 2012),\\n(Son et al., 2012) and (Zou et al., 2013).\\n3.1 Scoring Phrase Pairs with RNN\\nEncoder–Decoder\\nHere we propose to train the RNN Encoder–\\nDecoder (see Sec. 2.2) on a table of phrase pairs\\nand use its scores as additional features in the loglinear model in Eq. (9) when tuning the SMT decoder.\\nWhen we train the RNN Encoder–Decoder, we\\nignore the (normalized) frequencies of each phrase\\npair in the original corpora. This measure was\\ntaken in order (1) to reduce the computational expense of randomly selecting phrase pairs from a',\n",
       " 'pair in the original corpora. This measure was\\ntaken in order (1) to reduce the computational expense of randomly selecting phrase pairs from a\\nlarge phrase table according to the normalized frequencies and (2) to ensure that the RNN Encoder–\\nDecoder does not simply learn to rank the phrase\\npairs according to their numbers of occurrences.\\nOne underlying reason for this choice was that the\\nexisting translation probability in the phrase table already reﬂects the frequencies of the phrase\\n2Without loss of generality, from here on, we refer to\\np(ejf)for each phrase pair as a translation model as wellpairs in the original corpus. With a ﬁxed capacity\\nof the RNN Encoder–Decoder, we try to ensure\\nthat most of the capacity of the model is focused\\ntoward learning linguistic regularities, i.e., distinguishing between plausible and implausible translations, or learning the “manifold” (region of probability concentration) of plausible translations.\\nOnce the RNN Encoder–Decoder is trained, we\\nadd a new score for each phrase pair to the existing phrase table. This allows the new scores to enter into the existing tuning algorithm with minimal\\nadditional overhead in computation.\\nAs Schwenk pointed out in (Schwenk, 2012),\\nit is possible to completely replace the existing',\n",
       " 'additional overhead in computation.\\nAs Schwenk pointed out in (Schwenk, 2012),\\nit is possible to completely replace the existing\\nphrase table with the proposed RNN Encoder–\\nDecoder. In that case, for a given source phrase,\\nthe RNN Encoder–Decoder will need to generate\\na list of (good) target phrases. This requires, however, an expensive sampling procedure to be performed repeatedly. In this paper, thus, we only\\nconsider rescoring the phrase pairs in the phrase\\ntable.\\n3.2 Related Approaches: Neural Networks in\\nMachine Translation\\nBefore presenting the empirical results, we discuss\\na number of recent works that have proposed to\\nuse neural networks in the context of SMT.\\nSchwenk in (Schwenk, 2012) proposed a similar approach of scoring phrase pairs. Instead of the\\nRNN-based neural network, he used a feedforward\\nneural network that has ﬁxed-size inputs (7 words\\nin his case, with zero-padding for shorter phrases)\\nand ﬁxed-size outputs (7 words in the target language). When it is used speciﬁcally for scoring\\nphrases for the SMT system, the maximum phrase\\nlength is often chosen to be small. However, as the\\nlength of phrases increases or as we apply neural',\n",
       " 'phrases for the SMT system, the maximum phrase\\nlength is often chosen to be small. However, as the\\nlength of phrases increases or as we apply neural\\nnetworks to other variable-length sequence data,\\nit is important that the neural network can handle variable-length input and output. The proposed RNN Encoder–Decoder is well-suited for\\nthese applications.\\nSimilar to (Schwenk, 2012), Devlin et al.\\n(Devlin et al., 2014) proposed to use a feedforward neural network to model a translation model,\\nhowever, by predicting one word in a target phrase\\nat a time. They reported an impressive improvement, but their approach still requires the maximum length of the input phrase (or context words)\\nto be ﬁxed a priori.\\nAlthough it is not exactly a neural network they\\ntrain, the authors of (Zou et al., 2013) proposed\\nto learn a bilingual embedding of words/phrases.\\nThey use the learned embedding to compute the\\ndistance between a pair of phrases which is used\\nas an additional score of the phrase pair in an SMT\\nsystem.\\nIn (Chandar et al., 2014), a feedforward neural\\nnetwork was trained to learn a mapping from a\\nbag-of-words representation of an input phrase to\\nan output phrase. This is closely related to both the',\n",
       " 'network was trained to learn a mapping from a\\nbag-of-words representation of an input phrase to\\nan output phrase. This is closely related to both the\\nproposed RNN Encoder–Decoder and the model\\nproposed in (Schwenk, 2012), except that their input representation of a phrase is a bag-of-words.\\nA similar approach of using bag-of-words representations was proposed in (Gao et al., 2013) as\\nwell. Earlier, a similar encoder–decoder model using two recursive neural networks was proposed\\nin (Socher et al., 2011), but their model was restricted to a monolingual setting, i.e. the model\\nreconstructs an input sentence. More recently, another encoder–decoder model using an RNN was\\nproposed in (Auli et al., 2013), where the decoder is conditioned on a representation of either\\na source sentence or a source context.\\nOne important difference between the proposed RNN Encoder–Decoder and the approaches\\nin (Zou et al., 2013) and (Chandar et al., 2014) is\\nthat the order of the words in source and target phrases is taken into account. The RNN\\nEncoder–Decoder naturally distinguishes between\\nsequences that have the same words but in a different order, whereas the aforementioned approaches\\neffectively ignore order information.\\nThe closest approach related to the proposed',\n",
       " 'sequences that have the same words but in a different order, whereas the aforementioned approaches\\neffectively ignore order information.\\nThe closest approach related to the proposed\\nRNN Encoder–Decoder is the Recurrent Continuous Translation Model (Model 2) proposed in\\n(Kalchbrenner and Blunsom, 2013). In their paper, they proposed a similar model that consists\\nof an encoder and decoder. The difference with\\nour model is that they used a convolutional n-gram\\nmodel (CGM) for the encoder and the hybrid of\\nan inverse CGM and a recurrent neural network\\nfor the decoder. They, however, evaluated their\\nmodel on rescoring the n-best list proposed by the\\nconventional SMT system and computing the perplexity of the gold standard translations.\\n4 Experiments\\nWe evaluate our approach on the English/French\\ntranslation task of the WMT’14 workshop.4.1 Data and Baseline System\\nLarge amounts of resources are available to build\\nan English/French SMT system in the framework\\nof the WMT’14 translation task. The bilingual\\ncorpora include Europarl (61M words), news commentary (5.5M), UN (421M), and two crawled\\ncorpora of 90M and 780M words respectively.\\nThe last two corpora are quite noisy. To train',\n",
       " 'corpora of 90M and 780M words respectively.\\nThe last two corpora are quite noisy. To train\\nthe French language model, about 712M words of\\ncrawled newspaper material is available in addition to the target side of the bitexts. All the word\\ncounts refer to French words after tokenization.\\nIt is commonly acknowledged that training statistical models on the concatenation of all this\\ndata does not necessarily lead to optimal performance, and results in extremely large models which are difﬁcult to handle. Instead, one\\nshould focus on the most relevant subset of the\\ndata for a given task. We have done so by\\napplying the data selection method proposed in\\n(Moore and Lewis, 2010), and its extension to bitexts (Axelrod et al., 2011). By these means we\\nselected a subset of 418M words out of more\\nthan 2G words for language modeling and a\\nsubset of 348M out of 850M words for training the RNN Encoder–Decoder. We used the\\ntest set newstest2012 and 2013 for data\\nselection and weight tuning with MERT, and\\nnewstest2014 as our test set. Each set has\\nmore than 70 thousand words and a single reference translation.\\nFor training the neural networks, including the\\nproposed RNN Encoder–Decoder, we limited the\\nsource and target vocabulary to the most frequent',\n",
       " 'For training the neural networks, including the\\nproposed RNN Encoder–Decoder, we limited the\\nsource and target vocabulary to the most frequent\\n15,000 words for both English and French. This\\ncovers approximately 93% of the dataset. All the\\nout-of-vocabulary words were mapped to a special\\ntoken ( [UNK ]).\\nThe baseline phrase-based SMT system was\\nbuilt using Moses with default settings. This system achieves a BLEU score of 30.64 and 33.3 on\\nthe development and test sets, respectively (see Table 1).\\n4.1.1 RNN Encoder–Decoder\\nThe RNN Encoder–Decoder used in the experiment had 1000 hidden units with the proposed\\ngates at the encoder and at the decoder. The input matrix between each input symbol xhtiand the\\nhidden unit is approximated with two lower-rank\\nmatrices, and the output matrix is approximated\\nModelsBLEU\\ndev test\\nBaseline 30.64 33.30\\nRNN 31.20 33.87\\nCSLM + RNN 31.48 34.64\\nCSLM + RNN + WP 31.50 34.54\\nTable 1: BLEU scores computed on the development and test sets using different combinations of\\napproaches. WP denotes a word penalty , where\\nwe penalizes the number of unknown words to',\n",
       " 'Table 1: BLEU scores computed on the development and test sets using different combinations of\\napproaches. WP denotes a word penalty , where\\nwe penalizes the number of unknown words to\\nneural networks.\\nsimilarly. We used rank-100 matrices, equivalent\\nto learning an embedding of dimension 100 for\\neach word. The activation function used for ~hin\\nEq. (8) is a hyperbolic tangent function. The computation from the hidden state in the decoder to\\nthe output is implemented as a deep neural network (Pascanu et al., 2014) with a single intermediate layer having 500 maxout units each pooling\\n2 inputs (Goodfellow et al., 2013).\\nAll the weight parameters in the RNN Encoder–\\nDecoder were initialized by sampling from an\\nisotropic zero-mean (white) Gaussian distribution\\nwith its standard deviation ﬁxed to 0:01, except\\nfor the recurrent weight parameters. For the recurrent weight matrices, we ﬁrst sampled from a\\nwhite Gaussian distribution and used its left singular vectors matrix, following (Saxe et al., 2014).\\nWe used Adadelta and stochastic gradient\\ndescent to train the RNN Encoder–Decoder\\nwith hyperparameters \\x0f= 10\\x006and\\x1a=\\n0:95(Zeiler, 2012). At each update, we used 64',\n",
       " 'with hyperparameters \\x0f= 10\\x006and\\x1a=\\n0:95(Zeiler, 2012). At each update, we used 64\\nrandomly selected phrase pairs from a phrase table (which was created from 348M words). The\\nmodel was trained for approximately three days.\\nDetails of the architecture used in the experiments are explained in more depth in the supplementary material.\\n4.1.2 Neural Language Model\\nIn order to assess the effectiveness of scoring\\nphrase pairs with the proposed RNN Encoder–\\nDecoder, we also tried a more traditional approach\\nof using a neural network for learning a target\\nlanguage model (CSLM) (Schwenk, 2007). Especially, the comparison between the SMT system\\nusing CSLM and that using the proposed approach\\nof phrase scoring by RNN Encoder–Decoder will\\nclarify whether the contributions from multiple\\nneural networks in different parts of the SMT sys-tem add up or are redundant.\\nWe trained the CSLM model on 7-grams\\nfrom the target corpus. Each input word\\nwas projected into the embedding space R512,\\nand they were concatenated to form a 3072dimensional vector. The concatenated vector was\\nfed through two rectiﬁed layers (of size 1536 and\\n1024) (Glorot et al., 2011). The output layer was',\n",
       " 'fed through two rectiﬁed layers (of size 1536 and\\n1024) (Glorot et al., 2011). The output layer was\\na simple softmax layer (see Eq. (2)). All the\\nweight parameters were initialized uniformly between\\x000:01and0:01, and the model was trained\\nuntil the validation perplexity did not improve for\\n10 epochs. After training, the language model\\nachieved a perplexity of 45.80. The validation set\\nwas a random selection of 0.1% of the corpus. The\\nmodel was used to score partial translations during the decoding process, which generally leads to\\nhigher gains in BLEU score than n-best list rescoring (Vaswani et al., 2013).\\nTo address the computational complexity of\\nusing a CSLM in the decoder a buffer was\\nused to aggregate n-grams during the stacksearch performed by the decoder. Only when\\nthe buffer is full, or a stack is about to\\nbe pruned, the n-grams are scored by the\\nCSLM. This allows us to perform fast matrixmatrix multiplication on GPU using Theano\\n(Bergstra et al., 2010; Bastien et al., 2012).\\n−60 −50 −40 −30 −20 −10 0−14−12−10−8−6−4−20',\n",
       " '−60 −50 −40 −30 −20 −10 0−14−12−10−8−6−4−20\\nRNN Scores (log)TM Scores (log)\\nFigure 3: The visualization of phrase pairs according to their scores (log-probabilities) by the RNN\\nEncoder–Decoder and the translation model.\\n4.2 Quantitative Analysis\\nWe tried the following combinations:\\n1. Baseline conﬁguration\\n2. Baseline + RNN\\n3. Baseline + CSLM + RNN\\n4. Baseline + CSLM + RNN + Word penalty\\nSource Translation Model RNN Encoder–Decoder\\nat the end of the [a la ﬁn de la] [ ´r la ﬁn des ann ´ees] [ ˆetre supprim ´es`a la ﬁn de la][`a la ﬁn du] [ `a la ﬁn des] [ `a la ﬁn de la]\\nfor the ﬁrst time [rc\\rpour la premi r¨ere fois] [ ´et´e donn ´es pour\\nla premi `ere fois] [ ´et´e comm ´emor ´ee pour la',\n",
       " 'la premi `ere fois] [ ´et´e comm ´emor ´ee pour la\\npremi `ere fois][pour la premi `ere fois] [pour la premi `ere fois ,]\\n[pour la premi `ere fois que]\\nin the United States\\nand[?aux?tats-Unis et] [ ´et´e ouvertes aux ´EtatsUnis et] [ ´et´e constat ´ees aux ´Etats-Unis et][aux Etats-Unis et] [des Etats-Unis et] [des\\n´Etats-Unis et]\\n, as well as [?s , qu’] [ ?s , ainsi que] [ ?re aussi bien que] [, ainsi qu’] [, ainsi que] [, ainsi que les]\\none of the most [?t?l’ un des plus] [ ?l’ un des plus] [ ˆetre retenue\\ncomme un de ses plus][l’ un des] [le] [un des]\\n(a) Long, frequent source phrases\\nSource Translation Model RNN Encoder–Decoder',\n",
       " '(a) Long, frequent source phrases\\nSource Translation Model RNN Encoder–Decoder\\n, Minister of Communications and Transport[Secr ´etaire aux communications et aux transports :] [Secr ´etaire aux communications et aux\\ntransports][Secr ´etaire aux communications et aux transports] [Secr ´etaire aux communications et aux\\ntransports :]\\ndid not comply with\\nthe[vestimentaire , ne correspondaient pas `a des]\\n[susmentionn ´ee n’ ´etait pas conforme aux]\\n[pr´esent ´ees n’ ´etaient pas conformes `a la][n’ ont pas respect ´e les] [n’ ´etait pas conforme\\naux] [n’ ont pas respect ´e la]\\nparts of the world . [c\\rgions du monde .] [r ´egions du monde consid´er´ees .] [r ´egion du monde consid ´er´ee .][parties du monde .] [les parties du monde .]\\n[des parties du monde .]\\nthe past few days . [le petit texte .] [cours des tout derniers jours .]',\n",
       " '[des parties du monde .]\\nthe past few days . [le petit texte .] [cours des tout derniers jours .]\\n[les tout derniers jours .][ces derniers jours .] [les derniers jours .] [cours\\ndes derniers jours .]\\non Friday and Saturday[vendredi et samedi `a la] [vendredi et samedi `a]\\n[se d ´eroulera vendredi et samedi ,][le vendredi et le samedi] [le vendredi et samedi]\\n[vendredi et samedi]\\n(b) Long, rare source phrases\\nTable 2: The top scoring target phrases for a small set of source phrases according to the translation\\nmodel (direct translation probability) and by the RNN Encoder–Decoder. Source phrases were randomly\\nselected from phrases with 4 or more words. ?denotes an incomplete (partial) character. ris a Cyrillic\\nletter ghe.\\nThe results are presented in Table 1. As expected, adding features computed by neural networks consistently improves the performance over\\nthe baseline performance.\\nThe best performance was achieved when we\\nused both CSLM and the phrase scores from the\\nRNN Encoder–Decoder. This suggests that the',\n",
       " 'the baseline performance.\\nThe best performance was achieved when we\\nused both CSLM and the phrase scores from the\\nRNN Encoder–Decoder. This suggests that the\\ncontributions of the CSLM and the RNN Encoder–\\nDecoder are not too correlated and that one can\\nexpect better results by improving each method independently. Furthermore, we tried penalizing the\\nnumber of words that are unknown to the neural\\nnetworks (i.e. words which are not in the shortlist). We do so by simply adding the number of\\nunknown words as an additional feature the loglinear model in Eq. (9).3However, in this case we\\n3To understand the effect of the penalty, consider the set\\nof all words in the 15,000 large shortlist, SL. All words xi=2\\nSL are replaced by a special token [UNK]before being scored\\nby the neural networks. Hence, the conditional probability of\\nanyxi\\nt=2SL is actually given by the model as\\np(xt= [UNK]jx<t) =p(xt=2SLjx<t)\\n=X\\nxj\\nt=2SLp\\x10\\nxj\\ntjx<t\\x11\\n\\x15p\\x10\\nxi\\ntjx<t\\x11\\n;',\n",
       " '=X\\nxj\\nt=2SLp\\x10\\nxj\\ntjx<t\\x11\\n\\x15p\\x10\\nxi\\ntjx<t\\x11\\n;\\nwhere x<tis a shorthand notation for xt\\x001; : : : ; x 1.were not able to achieve better performance on the\\ntest set, but only on the development set.\\n4.3 Qualitative Analysis\\nIn order to understand where the performance improvement comes from, we analyze the phrase pair\\nscores computed by the RNN Encoder–Decoder\\nagainst the corresponding p(fje)from the translation model. Since the existing translation model\\nrelies solely on the statistics of the phrase pairs in\\nthe corpus, we expect its scores to be better estimated for the frequent phrases but badly estimated\\nfor rare phrases. Also, as we mentioned earlier\\nin Sec. 3.1, we further expect the RNN Encoder–\\nDecoder which was trained without any frequency\\ninformation to score the phrase pairs based rather\\non the linguistic regularities than on the statistics\\nof their occurrences in the corpus.\\nWe focus on those pairs whose source phrase is\\nlong (more than 3 words per source phrase) and\\nAs a result, the probability of words not in the shortlist is\\nalways overestimated. It is possible to address this issue by\\nbacking off to an existing model that contain non-shortlisted',\n",
       " 'always overestimated. It is possible to address this issue by\\nbacking off to an existing model that contain non-shortlisted\\nwords (see (Schwenk, 2007)) In this paper, however, we opt\\nfor introducing a word penalty instead, which counteracts the\\nword probability overestimation.\\nSource Samples from RNN Encoder–Decoder\\nat the end of the [`a la ﬁn de la] (\\x0211)\\nfor the ﬁrst time [pour la premi `ere fois] (\\x0224) [pour la premi `ere fois que] (\\x022)\\nin the United States and [aux ´Etats-Unis et] (\\x026) [dans les ´Etats-Unis et] (\\x024)\\n, as well as [, ainsi que] [,] [ainsi que] [, ainsi qu’] [et UNK]\\none of the most [l’ un des plus] (\\x029) [l’ un des] (\\x025) [l’ une des plus] (\\x022)\\n(a) Long, frequent source phrases\\nSource Samples from RNN Encoder–Decoder\\n, Minister of Communications and Transport[ , ministre des communications et le transport] ( \\x0213)',\n",
       " 'Source Samples from RNN Encoder–Decoder\\n, Minister of Communications and Transport[ , ministre des communications et le transport] ( \\x0213)\\ndid not comply with the [n’ tait pas conforme aux] [n’ a pas respect l’] ( \\x022) [n’ a pas respect la] ( \\x023)\\nparts of the world . [arts du monde .] (\\x0211) [des arts du monde .] ( \\x027)\\nthe past few days . [quelques jours .] (\\x025) [les derniers jours .] ( \\x025) [ces derniers jours .] ( \\x022)\\non Friday and Saturday [vendredi et samedi] ( \\x025) [le vendredi et samedi] ( \\x027) [le vendredi et le samedi] ( \\x024)\\n(b) Long, rare source phrases\\nTable 3: Samples generated from the RNN Encoder–Decoder for each source phrase used in Table 2. We\\nshow the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder–Decoder scores.\\nFigure 4: 2–D embedding of the learned word representation. The left one shows the full embedding',\n",
       " 'Figure 4: 2–D embedding of the learned word representation. The left one shows the full embedding\\nspace, while the right one shows a zoomed-in view of one region (color–coded). For more plots, see the\\nsupplementary material.\\nfrequent . For each such source phrase, we look\\nat the target phrases that have been scored high\\neither by the translation probability p(fje)or\\nby the RNN Encoder–Decoder. Similarly, we perform the same procedure with those pairs whose\\nsource phrase is long butrare in the corpus.\\nTable 2 lists the top- 3target phrases per source\\nphrase favored either by the translation model\\nor by the RNN Encoder–Decoder. The source\\nphrases were randomly chosen among long ones\\nhaving more than 4 or 5 words.\\nIn most cases, the choices of the target phrases\\nby the RNN Encoder–Decoder are closer to actual or literal translations. We can observe that the\\nRNN Encoder–Decoder prefers shorter phrases in\\ngeneral.\\nInterestingly, many phrase pairs were scored\\nsimilarly by both the translation model and the\\nRNN Encoder–Decoder, but there were as manyother phrase pairs that were scored radically different (see Fig. 3). This could arise from the\\nproposed approach of training the RNN Encoder–',\n",
       " 'proposed approach of training the RNN Encoder–\\nDecoder on a set of unique phrase pairs, discouraging the RNN Encoder–Decoder from learning\\nsimply the frequencies of the phrase pairs from the\\ncorpus, as explained earlier.\\nFurthermore, in Table 3, we show for each of\\nthe source phrases in Table 2, the generated samples from the RNN Encoder–Decoder. For each\\nsource phrase, we generated 50 samples and show\\nthe top-ﬁve phrases accordingly to their scores.\\nWe can see that the RNN Encoder–Decoder is\\nable to propose well-formed target phrases without looking at the actual phrase table. Importantly,\\nthe generated phrases do not overlap completely\\nwith the target phrases from the phrase table. This\\nencourages us to further investigate the possibility\\nof replacing the whole or a part of the phrase table\\nFigure 5: 2–D embedding of the learned phrase representation. The top left one shows the full representation space (5000 randomly selected points), while the other three ﬁgures show the zoomed-in view of\\nspeciﬁc regions (color–coded).\\nwith the proposed RNN Encoder–Decoder in the\\nfuture.\\n4.4 Word and Phrase Representations\\nSince the proposed RNN Encoder–Decoder is not',\n",
       " 'with the proposed RNN Encoder–Decoder in the\\nfuture.\\n4.4 Word and Phrase Representations\\nSince the proposed RNN Encoder–Decoder is not\\nspeciﬁcally designed only for the task of machine\\ntranslation, here we brieﬂy look at the properties\\nof the trained model.\\nIt has been known for some time that\\ncontinuous space language models using\\nneural networks are able to learn semantically meaningful embeddings (See, e.g.,\\n(Bengio et al., 2003; Mikolov et al., 2013)). Since\\nthe proposed RNN Encoder–Decoder also projects\\nto and maps back from a sequence of words into\\na continuous space vector, we expect to see a\\nsimilar property with the proposed model as well.\\nThe left plot in Fig. 4 shows the 2–D embedding\\nof the words using the word embedding matrix\\nlearned by the RNN Encoder–Decoder. The projection was done by the recently proposed BarnesHut-SNE (van der Maaten, 2013). We can clearly\\nsee that semantically similar words are clusteredwith each other (see the zoomed-in plots in Fig. 4).\\nThe proposed RNN Encoder–Decoder naturally\\ngenerates a continuous-space representation of a\\nphrase. The representation ( cin Fig. 1) in this',\n",
       " 'The proposed RNN Encoder–Decoder naturally\\ngenerates a continuous-space representation of a\\nphrase. The representation ( cin Fig. 1) in this\\ncase is a 1000-dimensional vector. Similarly to the\\nword representations, we visualize the representations of the phrases that consists of four or more\\nwords using the Barnes-Hut-SNE in Fig. 5.\\nFrom the visualization, it is clear that the RNN\\nEncoder–Decoder captures both semantic and syntactic structures of the phrases. For instance, in\\nthe bottom-left plot, most of the phrases are about\\nthe duration of time, while those phrases that are\\nsyntactically similar are clustered together. The\\nbottom-right plot shows the cluster of phrases that\\nare semantically similar (countries or regions). On\\nthe other hand, the top-right plot shows the phrases\\nthat are syntactically similar.\\n5 Conclusion\\nIn this paper, we proposed a new neural network\\narchitecture, called an RNN Encoder–Decoder\\nthat is able to learn the mapping from a sequence\\nof an arbitrary length to another sequence, possibly from a different set, of an arbitrary length. The\\nproposed RNN Encoder–Decoder is able to either\\nscore a pair of sequences (in terms of a conditional\\nprobability) or generate a target sequence given a\\nsource sequence. Along with the new architecture,',\n",
       " 'score a pair of sequences (in terms of a conditional\\nprobability) or generate a target sequence given a\\nsource sequence. Along with the new architecture,\\nwe proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control\\nhow much each hidden unit remembers or forgets\\nwhile reading/generating a sequence.\\nWe evaluated the proposed model with the task\\nof statistical machine translation, where we used\\nthe RNN Encoder–Decoder to score each phrase\\npair in the phrase table. Qualitatively, we were\\nable to show that the new model is able to capture linguistic regularities in the phrase pairs well\\nand also that the RNN Encoder–Decoder is able to\\npropose well-formed target phrases.\\nThe scores by the RNN Encoder–Decoder were\\nfound to improve the overall translation performance in terms of BLEU scores. Also, we\\nfound that the contribution by the RNN Encoder–\\nDecoder is rather orthogonal to the existing approach of using neural networks in the SMT system, so that we can improve further the performance by using, for instance, the RNN Encoder–\\nDecoder and the neural net language model together.\\nOur qualitative analysis of the trained model\\nshows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as\\nwell as phrase level. This suggests that there may',\n",
       " 'Our qualitative analysis of the trained model\\nshows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as\\nwell as phrase level. This suggests that there may\\nbe more natural language related applications that\\nmay beneﬁt from the proposed RNN Encoder–\\nDecoder.\\nThe proposed architecture has large potential\\nfor further improvement and analysis. One approach that was not investigated here is to replace the whole, or a part of the phrase table by\\nletting the RNN Encoder–Decoder propose target\\nphrases. Also, noting that the proposed model is\\nnot limited to being used with written language,\\nit will be an important future research to apply the\\nproposed architecture to other applications such as\\nspeech transcription.\\nAcknowledgments\\nKC, BM, CG, DB and YB would like to thank\\nNSERC, Calcul Qu ´ebec, Compute Canada, the\\nCanada Research Chairs and CIFAR. FB and HS\\nwere partially funded by the European Commis-sion under the project MateCat, and by DARPA\\nunder the BOLT project.\\nReferences\\n[Auli et al.2013] Michael Auli, Michel Galley, Chris\\nQuirk, and Geoffrey Zweig. 2013. Joint language\\nand translation modeling with recurrent neural networks. In Proceedings of the ACL Conference on\\nEmpirical Methods in Natural Language Processing',\n",
       " 'Quirk, and Geoffrey Zweig. 2013. Joint language\\nand translation modeling with recurrent neural networks. In Proceedings of the ACL Conference on\\nEmpirical Methods in Natural Language Processing\\n(EMNLP) , pages 1044–1054.\\n[Axelrod et al.2011] Amittai Axelrod, Xiaodong He,\\nand Jianfeng Gao. 2011. Domain adaptation via\\npseudo in-domain data selection. In Proceedings of\\nthe ACL Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 355–362.\\n[Bastien et al.2012] Fr ´ed´eric Bastien, Pascal Lamblin,\\nRazvan Pascanu, James Bergstra, Ian J. Goodfellow,\\nArnaud Bergeron, Nicolas Bouchard, and Yoshua\\nBengio. 2012. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.\\n[Bengio et al.2003] Yoshua Bengio, R ´ejean Ducharme,\\nPascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn.\\nRes., 3:1137–1155, March.',\n",
       " 'Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn.\\nRes., 3:1137–1155, March.\\n[Bengio et al.2013] Y . Bengio, N. BoulangerLewandowski, and R. Pascanu. 2013. Advances\\nin optimizing recurrent networks. In Proceedings\\nof the 38th International Conference on Acoustics,\\nSpeech, and Signal Processing (ICASSP 2013) ,\\nMay.\\n[Bergstra et al.2010] James Bergstra, Olivier Breuleux,\\nFr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu,\\nGuillaume Desjardins, Joseph Turian, David WardeFarley, and Yoshua Bengio. 2010. Theano: a CPU\\nand GPU math expression compiler. In Proceedings\\nof the Python for Scientiﬁc Computing Conference\\n(SciPy) , June. Oral Presentation.\\n[Chandar et al.2014] Sarath Chandar, Stanislas Lauly,\\nHugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. arXiv: 1402.1454 [cs.CL] , February.',\n",
       " '[Dahl et al.2012] George E. Dahl, Dong Yu, Li Deng,\\nand Alex Acero. 2012. Context-dependent pretrained deep neural networks for large vocabulary\\nspeech recognition. IEEE Transactions on Audio,\\nSpeech, and Language Processing , 20(1):33–42.\\n[Devlin et al.2014] Jacob Devlin, Rabih Zbib,\\nZhongqiang Huang, Thomas Lamar, Richard\\nSchwartz, , and John Makhoul. 2014. Fast and\\nrobust neural network joint models for statistical\\nmachine translation. In Proceedings of the ACL\\n2014 Conference , ACL ’14, pages 1370–1380.\\n[Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau\\nYih, and Li Deng. 2013. Learning semantic representations for the phrase translation model. Technical report, Microsoft Research.\\n[Glorot et al.2011] X. Glorot, A. Bordes, and Y . Bengio. 2011. Deep sparse rectiﬁer neural networks. In\\nAISTATS’2011 .\\n[Goodfellow et al.2013] Ian J. Goodfellow, David\\nWarde-Farley, Mehdi Mirza, Aaron Courville, and\\nYoshua Bengio. 2013. Maxout networks. In\\nICML’2013 .',\n",
       " 'Warde-Farley, Mehdi Mirza, Aaron Courville, and\\nYoshua Bengio. 2013. Maxout networks. In\\nICML’2013 .\\n[Graves2012] Alex Graves. 2012. Supervised Sequence Labelling with Recurrent Neural Networks .\\nStudies in Computational Intelligence. Springer.\\n[Hochreiter and Schmidhuber1997] S. Hochreiter and\\nJ. Schmidhuber. 1997. Long short-term memory.\\nNeural Computation , 9(8):1735–1780.\\n[Kalchbrenner and Blunsom2013] Nal Kalchbrenner\\nand Phil Blunsom. 2013. Two recurrent continuous\\ntranslation models. In Proceedings of the ACL Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP) , pages 1700–1709.\\n[Koehn et al.2003] Philipp Koehn, Franz Josef Och,\\nand Daniel Marcu. 2003. Statistical phrase-based\\ntranslation. In Proceedings of the 2003 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics on Human Language\\nTechnology - Volume 1 , NAACL ’03, pages 48–54.\\n[Koehn2005] P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine',\n",
       " '[Koehn2005] P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine\\nTranslation Summit X , pages 79–86, Phuket, Thailand.\\n[Krizhevsky et al.2012] Alex Krizhevsky, Ilya\\nSutskever, and Geoffrey Hinton. 2012. ImageNet classiﬁcation with deep convolutional neural\\nnetworks. In Advances in Neural Information\\nProcessing Systems 25 (NIPS’2012) .\\n[Marcu and Wong2002] Daniel Marcu and William\\nWong. 2002. A phrase-based, joint probability\\nmodel for statistical machine translation. In Proceedings of the ACL-02 Conference on Empirical\\nMethods in Natural Language Processing - Volume\\n10, EMNLP ’02, pages 133–139.\\n[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,\\nKai Chen, Greg Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and\\ntheir compositionality. In Advances in Neural Information Processing Systems 26 , pages 3111–3119.\\n[Moore and Lewis2010] Robert C. Moore and William\\nLewis. 2010. Intelligent selection of language\\nmodel training data. In Proceedings of the ACL\\n2010 Conference Short Papers , ACLShort ’10,',\n",
       " 'Lewis. 2010. Intelligent selection of language\\nmodel training data. In Proceedings of the ACL\\n2010 Conference Short Papers , ACLShort ’10,\\npages 220–224, Stroudsburg, PA, USA.[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho,\\nand Y . Bengio. 2014. How to construct deep recurrent neural networks. In Proceedings of the Second\\nInternational Conference on Learning Representations (ICLR 2014) , April.\\n[Saxe et al.2014] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. 2014. Exact solutions\\nto the nonlinear dynamics of learning in deep linear neural networks. In Proceedings of the Second\\nInternational Conference on Learning Representations (ICLR 2014) , April.\\n[Schwenk et al.2006] Holger Schwenk, Marta R. CostaJuss`a, and Jos ´e A. R. Fonollosa. 2006. Continuous\\nspace language models for the iwslt 2006 task. In\\nIWSLT , pages 166–173.\\n[Schwenk2007] Holger Schwenk. 2007. Continuous\\nspace language models. Comput. Speech Lang. ,\\n21(3):492–518, July.\\n[Schwenk2012] Holger Schwenk. 2012. Continuous',\n",
       " 'space language models. Comput. Speech Lang. ,\\n21(3):492–518, July.\\n[Schwenk2012] Holger Schwenk. 2012. Continuous\\nspace translation models for phrase-based statistical machine translation. In Martin Kay and Christian Boitet, editors, Proceedings of the 24th International Conference on Computational Linguistics\\n(COLIN) , pages 1071–1080.\\n[Socher et al.2011] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y . Ng, and Christopher D.\\nManning. 2011. Dynamic pooling and unfolding\\nrecursive autoencoders for paraphrase detection. In\\nAdvances in Neural Information Processing Systems\\n24.\\n[Son et al.2012] Le Hai Son, Alexandre Allauzen, and\\nFranc ¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of\\nthe 2012 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies , NAACL HLT ’12,\\npages 39–48, Stroudsburg, PA, USA.\\n[van der Maaten2013] Laurens van der Maaten. 2013.\\nBarnes-hut-sne. In Proceedings of the First International Conference on Learning Representations\\n(ICLR 2013) , May.',\n",
       " 'Barnes-hut-sne. In Proceedings of the First International Conference on Learning Representations\\n(ICLR 2013) , May.\\n[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,\\nVictoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. Proceedings of the Conference\\non Empirical Methods in Natural Language Processing , pages 1387–1392.\\n[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:\\nan adaptive learning rate method. Technical report,\\narXiv 1212.5701.\\n[Zou et al.2013] Will Y . Zou, Richard Socher,\\nDaniel M. Cer, and Christopher D. Manning.\\n2013. Bilingual word embeddings for phrase-based\\nmachine translation. In Proceedings of the ACL\\nConference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 1393–1398.\\nA RNN Encoder–Decoder\\nIn this document, we describe in detail the architecture of the RNN Encoder–Decoder used in the experiments.\\nLet us denote an source phrase by X= (x1;x2;:::;xN)and a target phrase by Y=',\n",
       " 'Let us denote an source phrase by X= (x1;x2;:::;xN)and a target phrase by Y=\\n(y1;y2;:::;yM). Each phrase is a sequence of K-dimensional one-hot vectors, such that only one\\nelement of the vector is 1and all the others are 0. The index of the active ( 1) element indicates the word\\nrepresented by the vector.\\nA.1 Encoder\\nEach word of the source phrase is embedded in a 500-dimensional vector space: e(xi)2R500.e(x)is\\nused in Sec. 4.4 to visualize the words.\\nThe hidden state of an encoder consists of 1000 hidden units, and each one of them at time tis\\ncomputed by\\nhhti\\nj=zjhht\\x001i\\nj + (1\\x00zj)~hhti\\nj;\\nwhere\\n~hhti\\nj= tanh\\x10\\n[We(xt)]j+\\x02\\nU\\x00\\nr\\x0chht\\x001i\\x01\\x03\\nj\\x11\\n;\\nzj=\\x1b\\x10\\n[Wze(xt)]j+\\x02\\nUzhht\\x001i\\x03\\nj\\x11\\n;\\nrj=\\x1b\\x10\\n[Wre(xt)]j+\\x02\\nUrhht\\x001i\\x03\\nj\\x11\\n:',\n",
       " 'j\\x11\\n;\\nrj=\\x1b\\x10\\n[Wre(xt)]j+\\x02\\nUrhht\\x001i\\x03\\nj\\x11\\n:\\n\\x1band\\x0care a logistic sigmoid function and an element-wise multiplication, respectively. To make the\\nequations uncluttered, we omit biases. The initial hidden state hh0i\\njis ﬁxed to 0.\\nOnce the hidden state at the Nstep (the end of the source phrase) is computed, the representation of\\nthe source phrase cis\\nc= tanh\\x10\\nVhhNi\\x11\\n:\\nA.1.1 Decoder\\nThe decoder starts by initializing the hidden state with\\nh0h0i= tanh\\x00\\nV0c\\x01\\n;\\nwhere we will use\\x010to distinguish parameters of the decoder from those of the encoder.\\nThe hidden state at time tof the decoder is computed by\\nh0hti\\nj=z0jh0ht\\x001i\\nj + (1\\x00z0j)~h0hti\\nj;\\nwhere\\n~h0hti\\nj= tanh\\x10\\x02\\nW0e(yt\\x001)\\x03\\nj+r0j\\x02\\nU0h0\\nht\\x001i+Cc\\x03\\x11\\n;\\nz0j=\\x1b\\x10\\x02',\n",
       " 'j+r0j\\x02\\nU0h0\\nht\\x001i+Cc\\x03\\x11\\n;\\nz0j=\\x1b\\x10\\x02\\nW0ze(yt\\x001)\\x03\\nj+\\x02\\nU0zh0\\nht\\x001i\\x03\\nj+ [Czc]j\\x11\\n;\\nr0j=\\x1b\\x10\\x02\\nW0re(yt\\x001)\\x03\\nj+\\x02\\nU0rh0\\nht\\x001i\\x03\\nj+ [Crc]j\\x11\\n;\\nande(y0)is an all-zero vector. Similarly to the case of the encoder, e(y)is an embedding of a target\\nword.\\nUnlike the encoder which simply encodes the source phrase, the decoder is learned to generate a target\\nphrase. At each time t, the decoder computes the probability of generating j-th word by\\np(yt;j= 1jyt\\x001;:::;y1;X) =exp\\x00\\ngjshti\\x01\\nPK\\nj0=1exp\\x00\\ngj0shti\\x01;\\nwhere thei-element of shtiis\\nshti\\ni= maxn\\ns0hti\\n2i\\x001;s0hti\\n2io\\nand\\ns0hti=Ohh0hti+Oyyt\\x001+Occ:',\n",
       " 's0hti\\n2i\\x001;s0hti\\n2io\\nand\\ns0hti=Ohh0hti+Oyyt\\x001+Occ:\\nIn short, the shti\\niis a so-called maxout unit.\\nFor the computational efﬁciency, instead of a single-matrix output weight G, we use a product of two\\nmatrices such that\\nG=GlGr;\\nwhere Gl2RK\\x02500andGr2R500\\x021000.\\nB Word and Phrase Representations\\nHere, we show enlarged plots of the word and phrase representations in Figs. 4–5.\\nFigure 6: 2–D embedding of the learned word representation. The top left one shows the full embedding space, while the other three ﬁgures show the zoomed-in\\nview of speciﬁc regions (color–coded).\\nFigure 7: 2–D embedding of the learned phrase representation. The top left one shows the full representation space (1000 randomly selected points), while the\\nother three ﬁgures show the zoomed-in view of speciﬁc regions (color–coded).',\n",
       " '1\\nSpatial Pyramid Pooling in Deep Convolutional\\nNetworks for Visual Recognition\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun\\nAbstract —Existing deep convolutional neural networks (CNNs) require a ﬁxed-size ( e.g., 224\\x02224) input image. This requirement is “artiﬁcial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this\\nwork, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The\\nnew network structure, called SPP-net, can generate a ﬁxed-length representation regardless of image size/scale. Pyramid\\npooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image\\nclassiﬁcation methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN\\narchitectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-theart classiﬁcation results using a single full-image representation and no ﬁne-tuning.',\n",
       " 'The power of SPP-net is also signiﬁcant in object detection. Using SPP-net, we compute the feature maps from the entire\\nimage only once, and then pool features in arbitrary regions (sub-images) to generate ﬁxed-length representations for training\\nthe detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is\\n24-102\\x02faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007.\\nIn ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in\\nimage classiﬁcation among all 38 teams. This manuscript also introduces the improvement made for this competition.\\nIndex Terms —Convolutional Neural Networks, Spatial Pyramid Pooling, Image Classiﬁcation, Object Detection\\nF\\n1 I NTRODUCTION\\nWe are witnessing a rapid, revolutionary change in\\nour vision community, mainly caused by deep convolutional neural networks (CNNs) [1] and the availability of large scale training data [2]. Deep-networksbased approaches have recently been substantially\\nimproving upon the state of the art in image classiﬁcation [3], [4], [5], [6], object detection [7], [8], [5],',\n",
       " 'improving upon the state of the art in image classiﬁcation [3], [4], [5], [6], object detection [7], [8], [5],\\nmany other recognition tasks [9], [10], [11], [12], and\\neven non-recognition tasks.\\nHowever, there is a technical issue in the training\\nand testing of the CNNs: the prevalent CNNs require\\naﬁxed input image size ( e.g., 224\\x02224), which limits\\nboth the aspect ratio and the scale of the input image.\\nWhen applied to images of arbitrary sizes, current\\nmethods mostly ﬁt the input image to the ﬁxed size,\\neither via cropping [3], [4] or via warping [13], [7],\\nas shown in Figure 1 (top). But the cropped region\\nmay not contain the entire object, while the warped\\ncontent may result in unwanted geometric distortion.\\nRecognition accuracy can be compromised due to the\\ncontent loss or distortion. Besides, a pre-deﬁned scale\\n\\x0fK. He and J. Sun are with Microsoft Research, Beijing, China. E-mail:\\nfkahe,jiansung@microsoft.com\\n\\x0fX. Zhang is with Xi’an Jiaotong University, Xi’an, China. Email:',\n",
       " 'fkahe,jiansung@microsoft.com\\n\\x0fX. Zhang is with Xi’an Jiaotong University, Xi’an, China. Email:\\nxyz.clx@stu.xjtu.edu.cn\\n\\x0fS. Ren is with University of Science and Technology of China, Hefei,\\nChina. Email: sqren@mail.ustc.edu.cn\\nThis work was done when X. Zhang and S. Ren were interns at Microsoft\\nResearch.\\ncrop warp\\nspatial pyramid poolingcrop / warp\\nconv layers image fc layers outputimage conv layers fc layers outputFigure 1: Top: cropping or warping to ﬁt a ﬁxed\\nsize. Middle: a conventional CNN. Bottom: our spatial\\npyramid pooling network structure.\\nmay not be suitable when object scales vary. Fixing\\ninput sizes overlooks the issues involving scales.\\nSo why do CNNs require a ﬁxed input size? A CNN\\nmainly consists of two parts: convolutional layers,\\nand fully-connected layers that follow. The convolutional layers operate in a sliding-window manner\\nand output feature maps which represent the spatial\\narrangement of the activations (Figure 2). In fact, convolutional layers do not require a ﬁxed image size and',\n",
       " 'and output feature maps which represent the spatial\\narrangement of the activations (Figure 2). In fact, convolutional layers do not require a ﬁxed image size and\\ncan generate feature maps of any sizes. On the other\\nhand, the fully-connected layers need to have ﬁxedsize/length input by their deﬁnition. Hence, the ﬁxedsize constraint comes only from the fully-connected\\nlayers, which exist at a deeper stage of the network.\\nIn this paper, we introduce a spatial pyramid pooling (SPP) [14], [15] layer to remove the ﬁxed-size\\nconstraint of the network. Speciﬁcally, we add anarXiv:1406.4729v4  [cs.CV]  23 Apr 2015\\n2\\nSPP layer on top of the last convolutional layer. The\\nSPP layer pools the features and generates ﬁxedlength outputs, which are then fed into the fullyconnected layers (or other classiﬁers). In other words,\\nwe perform some information “aggregation” at a\\ndeeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid\\nthe need for cropping or warping at the beginning.',\n",
       " 'deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid\\nthe need for cropping or warping at the beginning.\\nFigure 1 (bottom) shows the change of the network\\narchitecture by introducing the SPP layer. We call the\\nnew network structure SPP-net .\\nSpatial pyramid pooling [14], [15] (popularly\\nknown as spatial pyramid matching or SPM [15]), as\\nan extension of the Bag-of-Words (BoW) model [16],\\nis one of the most successful methods in computer\\nvision. It partitions the image into divisions from\\nﬁner to coarser levels, and aggregates local features\\nin them. SPP has long been a key component in the\\nleading and competition-winning systems for classiﬁcation ( e.g., [17], [18], [19]) and detection ( e.g., [20])\\nbefore the recent prevalence of CNNs. Nevertheless,\\nSPP has not been considered in the context of CNNs.\\nWe note that SPP has several remarkable properties\\nfor deep CNNs: 1) SPP is able to generate a ﬁxedlength output regardless of the input size, while the\\nsliding window pooling used in the previous deep\\nnetworks [3] cannot; 2) SPP uses multi-level spatial',\n",
       " 'sliding window pooling used in the previous deep\\nnetworks [3] cannot; 2) SPP uses multi-level spatial\\nbins, while the sliding window pooling uses only\\na single window size. Multi-level pooling has been\\nshown to be robust to object deformations [15]; 3) SPP\\ncan pool features extracted at variable scales thanks\\nto the ﬂexibility of input scales. Through experiments\\nwe show that all these factors elevate the recognition\\naccuracy of deep networks.\\nSPP-net not only makes it possible to generate representations from arbitrarily sized images/windows\\nfor testing, but also allows us to feed images with\\nvarying sizes or scales during training. Training with\\nvariable-size images increases scale-invariance and\\nreduces over-ﬁtting. We develop a simple multi-size\\ntraining method. For a single network to accept\\nvariable input sizes, we approximate it by multiple\\nnetworks that share all parameters, while each of\\nthese networks is trained using a ﬁxed input size. In\\neach epoch we train the network with a given input\\nsize, and switch to another input size for the next\\nepoch. Experiments show that this multi-size training\\nconverges just as the traditional single-size training,\\nand leads to better testing accuracy.',\n",
       " 'epoch. Experiments show that this multi-size training\\nconverges just as the traditional single-size training,\\nand leads to better testing accuracy.\\nThe advantages of SPP are orthogonal to the speciﬁc\\nCNN designs. In a series of controlled experiments on\\nthe ImageNet 2012 dataset, we demonstrate that SPP\\nimproves four different CNN architectures in existing\\npublications [3], [4], [5] (or their modiﬁcations), over\\nthe no-SPP counterparts. These architectures have\\nvarious ﬁlter numbers/sizes, strides, depths, or other\\ndesigns. It is thus reasonable for us to conjecture\\nthat SPP should improve more sophisticated (deeperand larger) convolutional architectures. SPP-net also\\nshows state-of-the-art classiﬁcation results on Caltech101 [21] and Pascal VOC 2007 [22] using only a\\nsingle full-image representation and no ﬁne-tuning.\\nSPP-net also shows great strength in object detection. In the leading object detection method R-CNN\\n[7], the features from candidate windows are extracted\\nvia deep convolutional networks. This method shows\\nremarkable detection accuracy on both the VOC and\\nImageNet datasets. But the feature computation in RCNN is time-consuming, because it repeatedly applies',\n",
       " 'remarkable detection accuracy on both the VOC and\\nImageNet datasets. But the feature computation in RCNN is time-consuming, because it repeatedly applies\\nthe deep convolutional networks to the raw pixels\\nof thousands of warped regions per image. In this\\npaper, we show that we can run the convolutional\\nlayers only once on the entire image (regardless of\\nthe number of windows), and then extract features\\nby SPP-net on the feature maps. This method yields\\na speedup of over one hundred times over R-CNN.\\nNote that training/running a detector on the feature\\nmaps (rather than image regions) is actually a more\\npopular idea [23], [24], [20], [5]. But SPP-net inherits\\nthe power of the deep CNN feature maps and also the\\nﬂexibility of SPP on arbitrary window sizes, which\\nleads to outstanding accuracy and efﬁciency. In our\\nexperiment, the SPP-net-based system (built upon the\\nR-CNN pipeline) computes features 24-102 \\x02faster\\nthan R-CNN, while has better or comparable accuracy.\\nWith the recent fast proposal method of EdgeBoxes\\n[25], our system takes 0.5 seconds processing an image\\n(including all steps ). This makes our method practical\\nfor real-world applications.',\n",
       " '[25], our system takes 0.5 seconds processing an image\\n(including all steps ). This makes our method practical\\nfor real-world applications.\\nA preliminary version of this manuscript has been\\npublished in ECCV 2014. Based on this work, we\\nattended the competition of ILSVRC 2014 [26], and\\nranked #2 in object detection and #3 in image classiﬁcation (both are provided-data-only tracks) among\\nall 38 teams. There are a few modiﬁcations made\\nfor ILSVRC 2014. We show that the SPP-nets can\\nboost various networks that are deeper and larger\\n(Sec. 3.1.2-3.1.4) over the no-SPP counterparts. Further, driven by our detection framework, we ﬁnd\\nthat multi-view testing on feature maps with ﬂexibly\\nlocated/sized windows (Sec. 3.1.5) can increase the\\nclassiﬁcation accuracy. This manuscript also provides\\nthe details of these modiﬁcations.\\nWe have released the code to facilitate future research ( http://research.microsoft.com/en-us/um/people/kahe/ ).\\n2 D EEP NETWORKS WITH SPATIAL PYRAMIDPOOLING\\n2.1 Convolutional Layers and Feature Maps',\n",
       " '2 D EEP NETWORKS WITH SPATIAL PYRAMIDPOOLING\\n2.1 Convolutional Layers and Feature Maps\\nConsider the popular seven-layer architectures [3], [4].\\nThe ﬁrst ﬁve layers are convolutional, some of which\\nare followed by pooling layers. These pooling layers\\ncan also be considered as “convolutional”, in the sense\\nthat they are using sliding windows. The last two\\n3\\n  \\n  \\nfilter #175\\nfilter #55\\n(a) image (b) feature maps (c) strongest activations\\n  \\n  \\nfilter #66\\nfilter #118\\n(a) image (b) feature maps (c) strongest activations\\nFigure 2: Visualization of the feature maps. (a) Two images in Pascal VOC 2007. (b) The feature maps of some\\nconv 5ﬁlters. The arrows indicate the strongest responses and their corresponding positions in the images.\\n(c) The ImageNet images that have the strongest responses of the corresponding ﬁlters. The green rectangles\\nmark the receptive ﬁelds of the strongest responses.\\nlayers are fully connected, with an N-way softmax as\\nthe output, where N is the number of categories.\\nThe deep network described above needs a ﬁxed',\n",
       " 'layers are fully connected, with an N-way softmax as\\nthe output, where N is the number of categories.\\nThe deep network described above needs a ﬁxed\\nimage size. However, we notice that the requirement\\nof ﬁxed sizes is only due to the fully-connected layers\\nthat demand ﬁxed-length vectors as inputs. On the\\nother hand, the convolutional layers accept inputs of\\narbitrary sizes. The convolutional layers use sliding\\nﬁlters, and their outputs have roughly the same aspect\\nratio as the inputs. These outputs are known as feature\\nmaps [1] - they involve not only the strength of the\\nresponses, but also their spatial positions.\\nIn Figure 2, we visualize some feature maps. They\\nare generated by some ﬁlters of the conv 5layer. Figure 2(c) shows the strongest activated images of these\\nﬁlters in the ImageNet dataset. We see a ﬁlter can be\\nactivated by some semantic content. For example, the\\n55-th ﬁlter (Figure 2, bottom left) is most activated by\\na circle shape; the 66-th ﬁlter (Figure 2, top right) is',\n",
       " 'a circle shape; the 66-th ﬁlter (Figure 2, top right) is\\nmost activated by a ^-shape; and the 118-th ﬁlter (Figure 2, bottom right) is most activated by a _-shape.\\nThese shapes in the input images (Figure 2(a)) activate\\nthe feature maps at the corresponding positions (the\\narrows in Figure 2).\\nIt is worth noticing that we generate the feature\\nmaps in Figure 2 without ﬁxing the input size. These\\nfeature maps generated by deep convolutional layers are analogous to the feature maps in traditional\\nmethods [27], [28]. In those methods, SIFT vectors\\n[29] or image patches [28] are densely extracted and\\nthen encoded, e.g., by vector quantization [16], [15],\\n[30], sparse coding [17], [18], or Fisher kernels [19].\\nThese encoded features consist of the feature maps,\\nand are then pooled by Bag-of-Words (BoW) [16] or\\nspatial pyramids [14], [15]. Analogously, the deep\\nconvolutional features can be pooled in a similar way.\\n2.2 The Spatial Pyramid Pooling Layer\\nThe convolutional layers accept arbitrary input sizes,',\n",
       " 'convolutional features can be pooled in a similar way.\\n2.2 The Spatial Pyramid Pooling Layer\\nThe convolutional layers accept arbitrary input sizes,\\nbut they produce outputs of variable sizes. The classiﬁers (SVM/softmax) or fully-connected layers require\\nconvolutional layersfeature maps of conv 5\\n(arbitrary size)\\nfixed-length representation\\ninput image16×256-d 4×256-d 256-d…...\\n…...\\nspatial pyramid pooling layerfully-connected layers (fc 6, fc 7)Figure 3: A network structure with a spatial pyramid\\npooling layer . Here 256 is the ﬁlter number of the\\nconv 5layer, and conv 5is the last convolutional layer.\\nﬁxed-length vectors. Such vectors can be generated\\nby the Bag-of-Words (BoW) approach [16] that pools\\nthe features together. Spatial pyramid pooling [14],\\n[15] improves BoW in that it can maintain spatial\\ninformation by pooling in local spatial bins. These\\nspatial bins have sizes proportional to the image size,\\nso the number of bins is ﬁxed regardless of the image\\nsize. This is in contrast to the sliding window pooling\\nof the previous deep networks [3], where the number',\n",
       " 'so the number of bins is ﬁxed regardless of the image\\nsize. This is in contrast to the sliding window pooling\\nof the previous deep networks [3], where the number\\nof sliding windows depends on the input size.\\nTo adopt the deep network for images of arbitrary sizes, we replace the last pooling layer ( e.g.,\\npool 5, after the last convolutional layer) with a spatial\\npyramid pooling layer . Figure 3 illustrates our method.\\nIn each spatial bin, we pool the responses of each\\nﬁlter (throughout this paper we use max pooling).\\nThe outputs of the spatial pyramid pooling are kMdimensional vectors with the number of bins denoted\\nasM(kis the number of ﬁlters in the last convolutional layer). The ﬁxed-dimensional vectors are the\\ninput to the fully-connected layer.\\nWith spatial pyramid pooling, the input image can\\n4\\nbe of any sizes. This not only allows arbitrary aspect\\nratios, but also allows arbitrary scales. We can resize\\nthe input image to any scale ( e.g.,min(w;h)=180, 224,\\n...) and apply the same deep network. When the\\ninput image is at different scales, the network (with\\nthe same ﬁlter sizes) will extract features at different',\n",
       " '...) and apply the same deep network. When the\\ninput image is at different scales, the network (with\\nthe same ﬁlter sizes) will extract features at different\\nscales. The scales play important roles in traditional\\nmethods, e.g., the SIFT vectors are often extracted at\\nmultiple scales [29], [27] (determined by the sizes of\\nthe patches and Gaussian ﬁlters). We will show that\\nthe scales are also important for the accuracy of deep\\nnetworks.\\nInterestingly, the coarsest pyramid level has a single\\nbin that covers the entire image. This is in fact a\\n“global pooling” operation, which is also investigated\\nin several concurrent works. In [31], [32] a global\\naverage pooling is used to reduce the model size\\nand also reduce overﬁtting; in [33], a global average\\npooling is used on the testing stage after all fc layers\\nto improve accuracy; in [34], a global max pooling is\\nused for weakly supervised object recognition. The\\nglobal pooling operation corresponds to the traditional Bag-of-Words method.\\n2.3 Training the Network\\nTheoretically, the above network structure can be\\ntrained with standard back-propagation [1], regardless of the input image size. But in practice the GPU',\n",
       " 'Theoretically, the above network structure can be\\ntrained with standard back-propagation [1], regardless of the input image size. But in practice the GPU\\nimplementations (such as cuda-convnet [3] and Caffe\\n[35]) are preferably run on ﬁxed input images. Next\\nwe describe our training solution that takes advantage\\nof these GPU implementations while still preserving\\nthe spatial pyramid pooling behaviors.\\nSingle-size training\\nAs in previous works, we ﬁrst consider a network taking a ﬁxed-size input (224 \\x02224) cropped from images.\\nThe cropping is for the purpose of data augmentation.\\nFor an image with a given size, we can pre-compute\\nthe bin sizes needed for spatial pyramid pooling.\\nConsider the feature maps after conv 5that have a size\\nofa\\x02a(e.g., 13\\x0213). With a pyramid level of n\\x02n\\nbins, we implement this pooling level as a sliding\\nwindow pooling, where the window size win=da=ne\\nand stride str=ba=ncwithd\\x01eandb\\x01cdenoting\\nceiling and ﬂoor operations. With an l-level pyramid,\\nwe implement lsuch layers. The next fully-connected\\nlayer (fc 6) will concatenate the loutputs. Figure 4',\n",
       " 'we implement lsuch layers. The next fully-connected\\nlayer (fc 6) will concatenate the loutputs. Figure 4\\nshows an example conﬁguration of 3-level pyramid\\npooling (3\\x023, 2\\x022, 1\\x021) in the cuda-convnet style [3].\\nThe main purpose of our single-size training is to\\nenable the multi-level pooling behavior. Experiments\\nshow that this is one reason for the gain of accuracy.\\nMulti-size training\\nOur network with SPP is expected to be applied on\\nimages of any sizes. To address the issue of varying\\n[fc6]\\ntype=fcoutputs=4096inputs=pool3x3,pool2x2,pool1x1[pool1x1]\\ntype=poolpool=maxinputs=conv5sizeX=13stride=13[pool3x3]\\ntype=poolpool=maxinputs=conv5sizeX=5stride=4[pool2x2]\\ntype=poolpool=maxinputs=conv5sizeX=7stride=6Figure 4: An example 3-level pyramid pooling in the\\ncuda-convnet style [3]. Here sizeX is the size of the\\npooling window. This conﬁguration is for a network',\n",
       " 'cuda-convnet style [3]. Here sizeX is the size of the\\npooling window. This conﬁguration is for a network\\nwhose feature map size of conv 5is 13\\x0213, so the\\npool 3\\x023, pool 2\\x022, and pool 1\\x021layers will have 3 \\x023,\\n2\\x022, and 1\\x021 bins respectively.\\nimage sizes in training, we consider a set of predeﬁned sizes. We consider two sizes: 180 \\x02180 in addition to 224\\x02224. Rather than crop a smaller 180 \\x02180\\nregion, we resize the aforementioned 224 \\x02224 region\\nto 180\\x02180. So the regions at both scales differ only\\nin resolution but not in content/layout. For the network to accept 180 \\x02180 inputs, we implement another\\nﬁxed-size-input (180 \\x02180) network. The feature map\\nsize after conv 5isa\\x02a= 10\\x0210 in this case. Then we\\nstill usewin=da=neandstr=ba=ncto implement\\neach pyramid pooling level. The output of the spatial\\npyramid pooling layer of this 180-network has the\\nsame ﬁxed length as the 224-network. As such, this\\n180-network has exactly the same parameters as the',\n",
       " 'same ﬁxed length as the 224-network. As such, this\\n180-network has exactly the same parameters as the\\n224-network in each layer. In other words, during\\ntraining we implement the varying-input-size SPP-net\\nby two ﬁxed-size networks that share parameters.\\nTo reduce the overhead to switch from one network\\n(e.g., 224) to the other ( e.g., 180), we train each full\\nepoch on one network, and then switch to the other\\none (keeping all weights) for the next full epoch. This\\nis iterated. In experiments, we ﬁnd the convergence\\nrate of this multi-size training to be similar to the\\nabove single-size training.\\nThe main purpose of our multi-size training is to\\nsimulate the varying input sizes while still leveraging\\nthe existing well-optimized ﬁxed-size implementations. Besides the above two-scale implementation, we\\nhave also tested a variant using s\\x02sas input where\\nsis randomly and uniformly sampled from [180;224]\\nat each epoch. We report the results of both variants\\nin the experiment section.\\nNote that the above single/multi-size solutions are\\nfor training only. At the testing stage, it is straightforward to apply SPP-net on images of any sizes.\\n5',\n",
       " 'Note that the above single/multi-size solutions are\\nfor training only. At the testing stage, it is straightforward to apply SPP-net on images of any sizes.\\n5\\nmodel conv 1 conv 2 conv 3 conv 4 conv 5 conv 6 conv 7\\nZF-5 96\\x0272, str 2 256\\x0252, str 2 384\\x0232384\\x0232256\\x0232\\nLRN, pool 32, str 2 LRN, pool 32, str 2 - map size 55\\x0255 27\\x0227 13\\x0213 13\\x0213 13\\x0213\\nConvnet*-5 96\\x02112, str 4 256\\x0252384\\x0232384\\x0232256\\x0232\\nLRN, LRN, pool 32, str 2 pool 32, 2 - map size 55\\x0255 27\\x0227 13\\x0213 13\\x0213 13\\x0213\\nOverfeat-5/7 96\\x0272, str 2 256\\x0252512\\x0232512\\x0232512\\x0232512\\x0232512\\x0232\\npool 32, str 3, LRN pool 22, str 2\\nmap size 36\\x0236 18\\x0218 18\\x0218 18\\x0218 18\\x0218 18\\x0218 18\\x0218\\nTable 1: Network architectures: ﬁlter number \\x02ﬁlter size ( e.g.,96\\x0272), ﬁlter stride ( e.g., str 2), pooling window',\n",
       " 'size ( e.g., pool 32), and the output feature map size ( e.g., map size 55\\x0255). LRN represents Local Response\\nNormalization. The padding is adjusted to produce the expected output feature map size.\\n3 SPP- NET FOR IMAGE CLASSIFICATION\\n3.1 Experiments on ImageNet 2012 Classiﬁcation\\nWe train the networks on the 1000-category training\\nset of ImageNet 2012. Our training algorithm follows\\nthe practices of previous work [3], [4], [36]. The images are resized so that the smaller dimension is 256,\\nand a 224\\x02224 crop is picked from the center or the\\nfour corners from the entire image1. The data are augmented by horizontal ﬂipping and color altering [3].\\nDropout [3] is used on the two fully-connected layers.\\nThe learning rate starts from 0.01, and is divided by 10\\n(twice) when the error plateaus. Our implementation\\nis based on the publicly available code of cuda-convnet\\n[3] and Caffe [35]. All networks in this paper can be\\ntrained on a single GeForce GTX Titan GPU (6 GB\\nmemory) within two to four weeks.\\n3.1.1 Baseline Network Architectures\\nThe advantages of SPP are independent of the convolutional network architectures used. We investigate',\n",
       " 'memory) within two to four weeks.\\n3.1.1 Baseline Network Architectures\\nThe advantages of SPP are independent of the convolutional network architectures used. We investigate\\nfour different network architectures in existing publications [3], [4], [5] (or their modiﬁcations), and we\\nshow SPP improves the accuracy of all these architectures. These baseline architectures are in Table 1 and\\nbrieﬂy introduced below:\\n\\x0fZF-5 : this architecture is based on Zeiler and Fergus’s (ZF) “fast” (smaller) model [4]. The number\\nindicates ﬁve convolutional layers.\\n\\x0fConvnet*-5 : this is a modiﬁcation on Krizhevsky\\net al.’s network [3]. We put the two pooling layers\\nafter conv 2and conv 3(instead of after conv 1and\\nconv 2). As a result, the feature maps after each\\nlayer have the same size as ZF-5.\\n\\x0fOverfeat-5/7 : this architecture is based on the\\nOverfeat paper [5], with some modiﬁcations as in\\n[6]. In contrast to ZF-5/Convnet*-5, this architecture produces a larger feature map ( 18\\x0218instead',\n",
       " '[6]. In contrast to ZF-5/Convnet*-5, this architecture produces a larger feature map ( 18\\x0218instead\\nof13\\x0213) before the last pooling layer. A larger\\nﬁlter number (512) is used in conv 3and the following convolutional layers. We also investigate\\n1. In [3], the four corners are picked from the corners of the\\ncentral 256\\x02256 crop.a deeper architecture with 7 convolutional layers,\\nwhere conv 3to conv 7have the same structures.\\nIn the baseline models, the pooling layer after the last\\nconvolutional layer generates 6\\x026feature maps, with\\ntwo 4096-d fc layers and a 1000-way softmax layer\\nfollowing. Our replications of these baseline networks\\nare in Table 2 (a). We train 70 epochs for ZF-5 and\\n90 epochs for the others. Our replication of ZF-5 is\\nbetter than the one reported in [4]. This gain is because\\nthe corner crops are from the entire image, as is also\\nreported in [36].\\n3.1.2 Multi-level Pooling Improves Accuracy\\nIn Table 2 (b) we show the results using singlesize training. The training and testing sizes are both\\n224\\x02224. In these networks, the convolutional layers',\n",
       " 'In Table 2 (b) we show the results using singlesize training. The training and testing sizes are both\\n224\\x02224. In these networks, the convolutional layers\\nhave the same structures as the corresponding baseline models, whereas the pooling layer after the ﬁnal\\nconvolutional layer is replaced with the SPP layer. For\\nthe results in Table 2, we use a 4-level pyramid. The\\npyramid isf6\\x026, 3\\x023, 2\\x022, 1\\x021g(totally 50 bins).\\nFor fair comparison, we still use the standard 10view prediction with each view a 224 \\x02224 crop. Our\\nresults in Table 2 (b) show considerable improvement\\nover the no-SPP baselines in Table 2 (a). Interestingly,\\nthe largest gain of top-1 error (1.65%) is given by the\\nmost accurate architecture. Since we are still using the\\nsame 10 cropped views as in (a), these gains are solely\\nbecause of multi-level pooling.\\nIt is worth noticing that the gain of multi-level\\npooling is notsimply due to more parameters; rather,\\nit is because the multi-level pooling is robust to the\\nvariance in object deformations and spatial layout\\n[15]. To show this, we train another ZF-5 network with',\n",
       " 'it is because the multi-level pooling is robust to the\\nvariance in object deformations and spatial layout\\n[15]. To show this, we train another ZF-5 network with\\na different 4-level pyramid: f4\\x024, 3\\x023, 2\\x022, 1\\x021g\\n(totally 30 bins). This network has fewer parameters\\nthan its no-SPP counterpart, because its fc 6layer has\\n30\\x02256-d inputs instead of 36 \\x02256-d. The top-1/top5 errors of this network are 35.06/14.04. This result\\nis similar to the 50-bin pyramid above (34.98/14.14),\\nbut considerably better than the no-SPP counterpart\\n(35.99/14.76).\\n6\\ntop-1 error (%)\\nZF-5 Convnet*-5 Overfeat-5 Overfeat-7\\n(a) no SPP 35.99 34.93 34.13 32.01\\n(b) SPP single-size trained 34.98 (1.01) 34.38 (0.55) 32.87 (1.26) 30.36 (1.65)\\n(c) SPP multi-size trained 34.60 (1.39) 33.94 (0.99) 32.26 (1.87) 29.68 (2.33)',\n",
       " 'top-5 error (%)\\nZF-5 Convnet*-5 Overfeat-5 Overfeat-7\\n(a) no SPP 14.76 13.92 13.52 11.97\\n(b) SPP single-size trained 14.14 (0.62) 13.54 (0.38) 12.80 (0.72) 11.12 (0.85)\\n(c) SPP multi-size trained 13.64 (1.12) 13.33 (0.59) 12.33 (1.19) 10.95 (1.02)\\nTable 2: Error rates in the validation set of ImageNet 2012. All the results are obtained using standard 10-view\\ntesting. In the brackets are the gains over the “no SPP” baselines.\\nSPP on test view top-1 val\\nZF-5, single-size trained 1 crop 38.01\\nZF-5, single-size trained 1 full 37.55\\nZF-5, multi-size trained 1 crop 37.57\\nZF-5, multi-size trained 1 full 37.07\\nOverfeat-7, single-size trained 1 crop 33.18\\nOverfeat-7, single-size trained 1 full 32.72\\nOverfeat-7, multi-size trained 1 crop 32.57',\n",
       " 'Overfeat-7, single-size trained 1 full 32.72\\nOverfeat-7, multi-size trained 1 crop 32.57\\nOverfeat-7, multi-size trained 1 full 31.25\\nTable 3: Error rates in the validation set of ImageNet\\n2012 using a single view. The images are resized so\\nmin(w;h) = 256 . The crop view is the central 224 \\x02224\\nof the image.\\n3.1.3 Multi-size Training Improves Accuracy\\nTable 2 (c) shows our results using multi-size training.\\nThe training sizes are 224 and 180, while the testing\\nsize is still 224. We still use the standard 10-view\\nprediction. The top-1/top-5 errors of all architectures\\nfurther drop. The top-1 error of SPP-net (Overfeat-7)\\ndrops to 29.68%, which is 2.33% better than its noSPP counterpart and 0.68% better than its single-size\\ntrained counterpart.\\nBesides using the two discrete sizes of 180 and\\n224, we have also evaluated using a random size\\nuniformly sampled from [180;224]. The top-1/5 error\\nof SPP-net (Overfeat-7) is 30.06%/10.96%. The top1 error is slightly worse than the two-size version,',\n",
       " 'of SPP-net (Overfeat-7) is 30.06%/10.96%. The top1 error is slightly worse than the two-size version,\\npossibly because the size of 224 (which is used for\\ntesting) is visited less. But the results are still better\\nthe single-size version.\\nThere are previous CNN solutions [5], [36] that deal\\nwith various scales/sizes, but they are mostly based\\non testing. In Overfeat [5] and Howard’s method [36],\\nthe single network is applied at multiple scales in the\\ntesting stage, and the scores are averaged. Howard\\nfurther trains two different networks on low/highresolution image regions and averages the scores. To\\nour knowledge, our method is the ﬁrst one that trains\\na single network with input images of multiple sizes.3.1.4 Full-image Representations Improve Accuracy\\nNext we investigate the accuracy of the full-image\\nviews. We resize the image so that min(w;h)=256\\nwhile maintaining its aspect ratio. The SPP-net is\\napplied on this full image to compute the scores of\\nthe full view. For fair comparison, we also evaluate\\nthe accuracy of the single view in the center 224 \\x02224\\ncrop (which is used in the above evaluations). The\\ncomparisons of single-view testing accuracy are in',\n",
       " 'the accuracy of the single view in the center 224 \\x02224\\ncrop (which is used in the above evaluations). The\\ncomparisons of single-view testing accuracy are in\\nTable 3. Here we evaluate ZF-5/Overfeat-7. The top-1\\nerror rates are all reduced by the full-view representation. This shows the importance of maintaining the\\ncomplete content. Even though our network is trained\\nusing square images only, it generalizes well to other\\naspect ratios.\\nComparing Table 2 and Table 3, we ﬁnd that the\\ncombination of multiple views is substantially better\\nthan the single full-image view. However, the fullimage representations are still of good merits. First,\\nwe empirically ﬁnd that (discussed in the next subsection) even for the combination of dozens of views,\\nthe additional two full-image views (with ﬂipping)\\ncan still boost the accuracy by about 0.2%. Second,\\nthe full-image view is methodologically consistent\\nwith the traditional methods [15], [17], [19] where the\\nencoded SIFT vectors of the entire image are pooled\\ntogether. Third, in other applications such as image\\nretrieval [37], an image representation, rather than a\\nclassiﬁcation score, is required for similarity ranking.\\nA full-image representation can be preferred.',\n",
       " 'retrieval [37], an image representation, rather than a\\nclassiﬁcation score, is required for similarity ranking.\\nA full-image representation can be preferred.\\n3.1.5 Multi-view Testing on Feature Maps\\nInspired by our detection algorithm (described in\\nthe next section), we further propose a multi-view\\ntesting method on the feature maps. Thanks to the\\nﬂexibility of SPP , we can easily extract the features\\nfrom windows (views) of arbitrary sizes from the\\nconvolutional feature maps.\\nOn the testing stage, we resize an image so\\nmin(w;h) =swheresrepresents a predeﬁned scale\\n(like 256). Then we compute the convolutional feature maps from the entire image. For the usage of\\n7\\nmethod test scales test views top-1 val top-5 val top-5 test\\nKrizhevsky et al. [3] 1 10 40.7 18.2\\nOverfeat (fast) [5] 1 - 39.01 16.97\\nOverfeat (fast) [5] 6 - 38.12 16.27\\nOverfeat (big) [5] 4 - 35.74 14.18\\nHoward (base) [36] 3 162 37.0 15.8\\nHoward (high-res) [36] 3 162 36.8 16.2',\n",
       " 'Howard (base) [36] 3 162 37.0 15.8\\nHoward (high-res) [36] 3 162 36.8 16.2\\nZeiler & Fergus (ZF) (fast) [4] 1 10 38.4 16.5\\nZeiler & Fergus (ZF) (big) [4] 1 10 37.5 16.0\\nChatﬁeld et al. [6] 1 10 - 13.1\\nours (SPP O-7) 1 10 29.68 10.95\\nours (SPP O-7) 6 96+2full 27.86 9.14 9.08\\nTable 4: Error rates in ImageNet 2012. All the results are based on a single network . The number of views in\\nOverfeat depends on the scales and strides, for which there are several hundreds at the ﬁnest scale.\\nﬂipped views, we also compute the feature maps of\\nthe ﬂipped image. Given any view (window) in the\\nimage, we map this window to the feature maps (the\\nway of mapping is in Appendix), and then use SPP\\nto pool the features from this window (see Figure 5).\\nThe pooled features are then fed into the fc layers\\nto compute the softmax score of this window. These\\nscores are averaged for the ﬁnal prediction. For the',\n",
       " 'The pooled features are then fed into the fc layers\\nto compute the softmax score of this window. These\\nscores are averaged for the ﬁnal prediction. For the\\nstandard 10-view, we use s= 256 and the views\\nare 224\\x02224 windows on the corners or center. Experiments show that the top-5 error of the 10-view\\nprediction on feature maps is within 0.1% around the\\noriginal 10-view prediction on image crops.\\nWe further apply this method to extract multiple\\nviews from multiple scales. We resize the image to six\\nscaless2 f224;256;300;360;448;560gand compute\\nthe feature maps on the entire image for each scale.\\nWe use 224\\x02224 as the view size for any scale,\\nso these views have different relative sizes on the\\noriginal image for different scales. We use 18 views\\nfor each scale: one at the center, four at the corners,\\nand four on the middle of each side, with/without\\nﬂipping (when s= 224 there are 6 different views).\\nThe combination of these 96 views reduces the top-5\\nerror from 10.95% to 9.36%. Combining the two fullimage views (with ﬂipping) further reduces the top-5\\nerror to 9.14%.',\n",
       " 'error from 10.95% to 9.36%. Combining the two fullimage views (with ﬂipping) further reduces the top-5\\nerror to 9.14%.\\nIn the Overfeat paper [5], the views are also extracted from the convolutional feature maps instead\\nof image crops. However, their views cannot have arbitrary sizes; rather, the windows are those where the\\npooled features match the desired dimensionality. We\\nempirically ﬁnd that these restricted windows are less\\nbeneﬁcial than our ﬂexibly located/sized windows.\\n3.1.6 Summary and Results for ILSVRC 2014\\nIn Table 4 we compare with previous state-of-theart methods. Krizhevsky et al .’s [3] is the winning\\nmethod in ILSVRC 2012; Overfeat [5], Howard’s [36],\\nand Zeiler and Fergus’s [4] are the leading methodsrank team top-5 test\\n1 GoogLeNet [32] 6.66\\n2 VGG [33] 7.32\\n3 ours 8.06\\n4 Howard 8.11\\n5 DeeperVision 9.50\\n6 NUS-BST 9.79\\n7 TTIC ECP 10.22',\n",
       " '3 ours 8.06\\n4 Howard 8.11\\n5 DeeperVision 9.50\\n6 NUS-BST 9.79\\n7 TTIC ECP 10.22\\nTable 5: The competition results of ILSVRC 2014 classiﬁcation [26]. The best entry of each team is listed.\\nin ILSVRC 2013. We only consider single-network\\nperformance for manageable comparisons.\\nOur best single network achieves 9.14% top-5 error\\non the validation set. This is exactly the single-model\\nentry we submitted to ILSVRC 2014 [26]. The top-5\\nerror is 9.08% on the testing set (ILSVRC 2014 has\\nthe same training/validation/testing data as ILSVRC\\n2012). After combining eleven models, our team’s result ( 8.06%) is ranked #3 among all 38 teams attending\\nILSVRC 2014 (Table 5). Since the advantages of SPPnet should be in general independent of architectures,\\nwe expect that it will further improve the deeper and\\nlarger convolutional architectures [33], [32].\\n3.2 Experiments on VOC 2007 Classiﬁcation\\nOur method can generate a full-view image representation. With the above networks pre-trained on\\nImageNet, we extract these representations from the',\n",
       " 'Our method can generate a full-view image representation. With the above networks pre-trained on\\nImageNet, we extract these representations from the\\nimages in the target datasets and re-train SVM classiﬁers [38]. In the SVM training, we intentionally do\\nnot use any data augmentation (ﬂip/multi-view). We\\nl2-normalize the features for SVM training.\\nThe classiﬁcation task in Pascal VOC 2007 [22]\\ninvolves 9,963 images in 20 categories. 5,011 images\\nare for training, and the rest are for testing. The\\nperformance is evaluated by mean Average Precision\\n(mAP). Table 6 summarizes the results.\\n8\\n(a) (b) (c) (d) (e)\\nmodel no SPP (ZF-5) SPP (ZF-5) SPP (ZF-5) SPP (ZF-5) SPP (Overfeat-7)\\ncrop crop full full full\\nsize 224\\x02224 224\\x02224 224\\x02- 392\\x02- 364\\x02conv 4 59.96 57.28 - - conv 5 66.34 65.43 - - pool 5=7(6\\x026) 69.14 68.76 70.82 71.67 76.09',\n",
       " 'fc6=8 74.86 75.55 77.32 78.78 81.58\\nfc7=9 75.90 76.45 78.39 80.10 82.44\\nTable 6: Classiﬁcation mAP in Pascal VOC 2007. For SPP-net, the pool 5=7layer uses the 6\\x026 pyramid level.\\n(a) (b) (c) (d)\\nmodel no SPP (ZF-5) SPP (ZF-5) SPP (ZF-5) SPP (Overfeat-7)\\ncrop crop full full\\nsize 224\\x02224 224\\x02224 224\\x02- 224\\x02conv 4 80.12 81.03 - conv 5 84.40 83.76 - pool 5=7(6\\x026) 87.98 87.60 89.46 91.46\\nSPP pool 5=7 - 89.47 91.44 93.42\\nfc6=8 87.86 88.54 89.50 91.83\\nfc7=9 85.30 86.10 87.08 90.00\\nTable 7: Classiﬁcation accuracy in Caltech101. For SPP-net, the pool 5=7layer uses the 6\\x026 pyramid level.\\nWe start from a baseline in Table 6 (a). The model is',\n",
       " 'We start from a baseline in Table 6 (a). The model is\\nZF-5 without SPP . To apply this model, we resize the\\nimage so that its smaller dimension is 224, and crop\\nthe center 224\\x02224 region. The SVM is trained via\\nthe features of a layer. On this dataset, the deeper the\\nlayer is, the better the result is. In Table 6 (b), we replace the no-SPP net with our SPP-net. As a ﬁrst-step\\ncomparison, we still apply the SPP-net on the center\\n224\\x02224 crop. The results of the fc layers improve.\\nThis gain is mainly due to multi-level pooling.\\nTable 6 (c) shows our results on full images, where\\nthe images are resized so that the shorter side is 224.\\nWe ﬁnd that the results are considerably improved\\n(78.39% vs. 76.45%). This is due to the full-image\\nrepresentation that maintains the complete content.\\nBecause the usage of our network does not depend\\non scale, we resize the images so that the smaller\\ndimension is sand use the same network to extract\\nfeatures. We ﬁnd that s= 392 gives the best results\\n(Table 6 (d)) based on the validation set. This is mainly\\nbecause the objects occupy smaller regions in VOC',\n",
       " '(Table 6 (d)) based on the validation set. This is mainly\\nbecause the objects occupy smaller regions in VOC\\n2007 but larger regions in ImageNet, so the relative\\nobject scales are different between the two sets. These\\nresults indicate scale matters in the classiﬁcation tasks,\\nand SPP-net can partially address this “scale mismatch” issue.\\nIn Table 6 (e) the network architecture is replaced\\nwith our best model (Overfeat-7, multi-size trained),\\nand the mAP increases to 82.44% . Table 8 summarizes\\nour results and the comparisons with the state-of-theart methods. Among these methods, VQ [15], LCC\\n[18], and FK [19] are all based on spatial pyramids\\nmatching, and [13], [4], [34], [6] are based on deepnetworks. In these results, Oquab et al.’s (77.7%) and\\nChatﬁeld et al .’s (82.42%) are obtained by network\\nﬁne-tuning and multi-view testing. Our result is comparable with the state of the art, using only a single\\nfull-image representation and without ﬁne-tuning.\\n3.3 Experiments on Caltech101',\n",
       " 'full-image representation and without ﬁne-tuning.\\n3.3 Experiments on Caltech101\\nThe Caltech101 dataset [21] contains 9,144 images in\\n102 categories (one background). We randomly sample 30 images per category for training and up to 50\\nimages per category for testing. We repeat 10 random\\nsplits and average the accuracy. Table 7 summarizes\\nour results.\\nThere are some common observations in the Pascal\\nVOC 2007 and Caltech101 results: SPP-net is better\\nthan the no-SPP net (Table 7 (b) vs. (a)), and the fullview representation is better than the crop ((c) vs. (b)).\\nBut the results in Caltech101 have some differences\\nwith Pascal VOC. The fully-connected layers are less\\naccurate, and the SPP layers are better. This is possibly\\nbecause the object categories in Caltech101 are less\\nrelated to those in ImageNet, and the deeper layers\\nare more category-specialized. Further, we ﬁnd that\\nthe scale 224 has the best performance among the\\nscales we tested on this dataset. This is mainly because\\nthe objects in Caltech101 also occupy large regions of\\nthe images, as is the case of ImageNet.\\nBesides cropping, we also evaluate warping the\\nimage to ﬁt the 224 \\x02224 size. This solution maintains',\n",
       " 'the images, as is the case of ImageNet.\\nBesides cropping, we also evaluate warping the\\nimage to ﬁt the 224 \\x02224 size. This solution maintains\\nthe complete content, but introduces distortion. On\\nthe SPP (ZF-5) model, the accuracy is 89.91% using\\nthe SPP layer as features - lower than 91.44% which\\nuses the same model on the undistorted full image.\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]y56.07 74.41 \\x061.0\\nLLC [18]y57.66 76.95 \\x060.4\\nFK [19]y61.69 77.78 \\x060.6\\nDeCAF [13] - 86.91 \\x060.7\\nZeiler & Fergus [4] 75.90z86.5\\x060.5\\nOquab et al. [34] 77.7 Chatﬁeld et al. [6] 82.42 88.54\\x060.3\\nours 82.44 93.42 \\x060.5\\nTable 8: Classiﬁcation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy).ynumbers reported\\nby [27].zour implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the',\n",
       " '(mAP) and Caltech101 (accuracy).ynumbers reported\\nby [27].zour implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42% ) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brieﬂy review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN ﬁrst extracts about 2,000 candidate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a ﬁxed size (227 \\x02227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classiﬁer is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous methods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image',\n",
       " 'the major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we apply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a ﬁxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from windows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to predeﬁne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv 5\\nconvolutional layersfixed-length representation\\ninput imagewindow…...fully-connected layers (fc 6, fc 7)Figure 5: Pooling features from arbitrary windows',\n",
       " 'convolutional layersfixed-length representation\\ninput imagewindow…...fully-connected layers (fc 6, fc 7)Figure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the “fast” mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w;h) =s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 \\x021, 2\\x022, 3\\x023, 6\\x026, totally\\n50 bins) to pool the features. This generates a 12,800d (256\\x0250) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classiﬁer for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to generate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%',\n",
       " '[20], [7]. We use the ground-truth windows to generate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the standard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classiﬁer\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w;h) =\\ns2S=f480;576;688;864;1200g, and compute the\\nfeature maps of conv 5for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically ﬁnd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s2S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 \\x02224. Then we only use the\\nfeature maps extracted from this scale to compute',\n",
       " 'such that the scaled candidate window has a number\\nof pixels closest to 224 \\x02224. Then we only use the\\nfeature maps extracted from this scale to compute\\n10\\nthe feature of this window. If the pre-deﬁned scales\\nare dense enough and the window is approximately\\nsquare, our method is roughly equivalent to resizing\\nthe window to 224 \\x02224 and then extracting features\\nfrom it. Nevertheless, our method only requires computing the feature maps once (at each scale) from the\\nentire image, regardless of the number of candidate\\nwindows.\\nWe also ﬁne-tune our pre-trained network, following [7]. Since our features are pooled from the conv 5\\nfeature maps from windows of any sizes, for simplicity we only ﬁne-tune the fully-connected layers.\\nIn this case, the data layer accepts the ﬁxed-length\\npooled features after conv 5, and the fc 6;7layers and\\na new 21-way (one extra negative category) fc 8layer\\nfollow. The fc 8weights are initialized with a Gaussian\\ndistribution of \\x1b=0.01. We ﬁx all the learning rates to\\n1e-4 and then adjust to 1e-5 for all three layers. During',\n",
       " 'distribution of \\x1b=0.01. We ﬁx all the learning rates to\\n1e-4 and then adjust to 1e-5 for all three layers. During\\nﬁne-tuning, the positive samples are those overlapping with a ground-truth window by [0:5;1], and\\nthe negative samples by [0:1;0:5). In each mini-batch,\\n25% of the samples are positive. We train 250k minibatches using the learning rate 1e-4, and then 50k\\nmini-batches using 1e-5. Because we only ﬁne-tune\\nthe fc layers, the training is very fast and takes about\\n2 hours on the GPU (excluding pre-caching feature\\nmaps which takes about 1 hour). Also following [7],\\nwe use bounding box regression to post-process the\\nprediction windows. The features used for regression\\nare the pooled features from conv 5(as a counterpart\\nof the pool 5features used in [7]). The windows used\\nfor the regression training are those overlapping with\\na ground-truth window by at least 50%.\\n4.2 Detection Results\\nWe evaluate our method on the detection task of the\\nPascal VOC 2007 dataset. Table 9 shows our results\\non various layers, by using 1-scale ( s=688) or 5-scale.',\n",
       " 'Pascal VOC 2007 dataset. Table 9 shows our results\\non various layers, by using 1-scale ( s=688) or 5-scale.\\nHere the R-CNN results are as reported in [7] using\\nthe AlexNet [3] with 5 conv layers. Using the pool 5\\nlayers (in our case the pooled features), our result\\n(44.9%) is comparable with R-CNN’s result (44.2%).\\nBut using the non-ﬁne-tuned fc 6layers, our results are\\ninferior. An explanation is that our fc layers are pretrained using image regions, while in the detection\\ncase they are used on the feature map regions. The\\nfeature map regions can have strong activations near\\nthe window boundaries, while the image regions may\\nnot. This difference of usages can be addressed by\\nﬁne-tuning. Using the ﬁne-tuned fc layers (ftfc 6;7), our\\nresults are comparable with or slightly better than\\nthe ﬁne-tuned results of R-CNN. After bounding box\\nregression, our 5-scale result ( 59.2%) is 0.7% better\\nthan R-CNN (58.5%), and our 1-scale result (58.0%)\\nis 0.5% worse.',\n",
       " 'than R-CNN (58.5%), and our 1-scale result (58.0%)\\nis 0.5% worse.\\nIn Table 10 we further compare with R-CNN using the same pre-trained model of SPPnet (ZF-5). InSPP (1-sc) SPP (5-sc) R-CNN\\n(ZF-5) (ZF-5) (Alex-5)\\npool 5 43.0 44.9 44.2\\nfc6 42.5 44.8 46.2\\nftfc6 52.3 53.7 53.1\\nftfc7 54.5 55.2 54.2\\nftfc7bb 58.0 59.2 58.5\\nconv time (GPU) 0.053s 0.293s 8.96s\\nfc time (GPU) 0.089s 0.089s 0.07s\\ntotal time (GPU) 0.142s 0.382s 9.03s\\nspeedup ( vs. RCNN) 64\\x02 24\\x02 Table 9: Detection results (mAP) on Pascal VOC 2007.\\n“ft” and “bb” denote ﬁne-tuning and bounding box\\nregression.\\nSPP (1-sc) SPP (5-sc) R-CNN',\n",
       " 'regression.\\nSPP (1-sc) SPP (5-sc) R-CNN\\n(ZF-5) (ZF-5) (ZF-5)\\nftfc7 54.5 55.2 55.1\\nftfc7bb 58.0 59.2 59.2\\nconv time (GPU) 0.053s 0.293s 14.37s\\nfc time (GPU) 0.089s 0.089s 0.089s\\ntotal time (GPU) 0.142s 0.382s 14.46s\\nspeedup ( vs. RCNN) 102\\x02 38\\x02 Table 10: Detection results (mAP) on Pascal VOC 2007,\\nusing the same pre-trained model of SPP (ZF-5).\\nthis case, our method and R-CNN have comparable\\naveraged scores. The R-CNN result is boosted by\\nthis pre-trained model. This is because of the better\\narchitecture of ZF-5 than AlexNet, and also because\\nof the multi-level pooling of SPPnet (if using the noSPP ZF-5, the R-CNN result drops). Table 11 shows\\nthe results for each category.\\nTable 11 also includes additional methods. Selective\\nSearch (SS) [20] applies spatial pyramid matching on',\n",
       " 'the results for each category.\\nTable 11 also includes additional methods. Selective\\nSearch (SS) [20] applies spatial pyramid matching on\\nSIFT feature maps. DPM [23] and Regionlet [39] are\\nbased on HOG features [24]. The Regionlet method\\nimproves to 46.1% [8] by combining various features including conv 5. DetectorNet [40] trains a deep\\nnetwork that outputs pixel-wise object masks. This\\nmethod only needs to apply the deep network once\\nto the entire image, as is the case for our method. But\\nthis method has lower mAP (30.5%).\\n4.3 Complexity and Running Time\\nDespite having comparable accuracy, our method is\\nmuch faster than R-CNN. The complexity of the convolutional feature computation in R-CNN is O(n\\x01\\n2272)with the window number n(\\x182000). This complexity of our method is O(r\\x01s2)at a scales, where\\nris the aspect ratio. Assume ris about 4/3. In the\\nsingle-scale version when s= 688 , this complexity is\\n11\\nmethod mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv',\n",
       " '11\\nmethod mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\\nDPM [23] 33.7 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5\\nSS [20] 33.8 43.5 46.5 10.4 12.0 9.3 49.4 53.7 39.4 12.5 36.9 42.2 26.4 47.0 52.4 23.5 12.1 29.9 36.3 42.2 48.8\\nRegionlet [39] 41.7 54.2 52.0 20.3 24.0 20.1 55.5 68.7 42.6 19.2 44.2 49.1 26.6 57.0 54.5 43.4 16.4 36.6 37.7 59.4 52.3',\n",
       " 'DetNet [40] 30.5 29.2 35.2 19.4 16.7 3.7 53.2 50.2 27.2 10.2 34.8 30.2 28.2 46.6 41.7 26.2 10.3 32.8 26.8 39.8 47.0\\nRCNN ftfc 7(A5) 54.2 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7\\nRCNN ftfc 7(ZF5) 55.1 64.8 68.4 47.0 39.5 30.9 59.8 70.5 65.3 33.5 62.5 50.3 59.5 61.6 67.9 54.1 33.4 57.3 52.9 60.2 62.9\\nSPP ftfc 7(ZF5) 55.2 65.5 65.9 51.7 38.4 32.7 62.6 68.6 69.7 33.1 66.6 53.1 58.2 63.6 68.8 50.4 27.4 53.7 48.2 61.7 64.7',\n",
       " 'RCNN bb (A5) 58.5 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8\\nRCNN bb (ZF5) 59.2 68.4 74.0 54.0 40.9 35.2 64.1 74.4 69.8 35.5 66.9 53.8 64.2 69.9 69.6 58.9 36.8 63.4 56.0 62.8 64.9\\nSPP bb (ZF5) 59.2 68.6 69.7 57.1 41.2 40.5 66.3 71.3 72.5 34.4 67.3 61.7 63.1 71.0 69.8 57.6 29.7 59.0 50.2 65.2 68.0\\nTable 11: Comparisons of detection results on Pascal VOC 2007.\\nmethod mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv',\n",
       " 'Table 11: Comparisons of detection results on Pascal VOC 2007.\\nmethod mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\\nSPP-net (1) 59.2 68.6 69.7 57.1 41.2 40.5 66.3 71.3 72.5 34.4 67.3 61.7 63.1 71.0 69.8 57.6 29.7 59.0 50.2 65.2 68.0\\nSPP-net (2) 59.1 65.7 71.4 57.4 42.4 39.9 67.0 71.4 70.6 32.4 66.7 61.7 64.8 71.7 70.4 56.5 30.8 59.9 53.2 63.9 64.6\\ncombination 60.9 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8\\nTable 12: Detection results on VOC 2007 using model combination. The results of both models use “ftfc 7bb”.\\nabout 1/160 of R-CNN’s; in the 5-scale version, this',\n",
       " 'about 1/160 of R-CNN’s; in the 5-scale version, this\\ncomplexity is about 1/24 of R-CNN’s.\\nIn Table 10, we provide a fair comparison on the\\nrunning time of the feature computation using the\\nsame SPP (ZF-5) model . The implementation of RCNN is from the code published by the authors implemented in Caffe [35]. We also implement our feature\\ncomputation in Caffe . In Table 10 we evaluate the\\naverage time of 100 random VOC images using GPU.\\nR-CNN takes 14.37s per image for convolutions, while\\nour 1-scale version takes only 0.053s per image. So\\nours is 270\\x02faster than R-CNN. Our 5-scale version\\ntakes 0.293s per image for convolutions, so is 49 \\x02\\nfaster than R-CNN. Our convolutional feature computation is so fast that the computational time of fc layers\\ntakes a considerable portion. Table 10 shows that the\\nGPU time of computing the 4,096-d fc 7features is\\n0.089s per image. Considering both convolutional and\\nfully-connected features, our 1-scale version is 102\\x02\\nfaster than R-CNN and is 1.2% inferior; our 5-scale',\n",
       " 'fully-connected features, our 1-scale version is 102\\x02\\nfaster than R-CNN and is 1.2% inferior; our 5-scale\\nversion is 38\\x02faster and has comparable results.\\nWe also compares the running time in Table 9 where\\nR-CNN uses AlexNet [3] as is in the original paper\\n[7]. Our method is 24 \\x02to 64\\x02faster. Note that the\\nAlexNet [3] has the same number of ﬁlters as our ZF5 on each conv layer. The AlexNet is faster because\\nit uses splitting on some layers, which was designed\\nfor two GPUs in [3].\\nWe further achieve an efﬁcient full system with the\\nhelp of the recent window proposal method [25]. The\\nSelective Search (SS) proposal [20] takes about 1-2 seconds per image on a CPU. The method of EdgeBoxes\\n[25] only takes\\x180.2s. Note that it is sufﬁcient to use\\na fast proposal method during testing only. Using the\\nsame model trained as above (using SS), we test proposals generated by EdgeBoxes only. The mAP is 52.8\\nwithout bounding box regression. This is reasonableconsidering that EdgeBoxes are not used for training.\\nThen we use both SS and EdgeBox as proposals in',\n",
       " 'without bounding box regression. This is reasonableconsidering that EdgeBoxes are not used for training.\\nThen we use both SS and EdgeBox as proposals in\\nthe training stage, and adopt only EdgeBoxes in the\\ntesting stage. The mAP is 56.3 without bounding box\\nregression, which is better than 55.2 (Table 10) due to\\nadditional training samples. In this case, the overall\\ntesting time is\\x180.5s per image including all steps\\n(proposal and recognition). This makes our method\\npractical for real-world applications.\\n4.4 Model Combination for Detection\\nModel combination is an important strategy for boosting CNN-based classiﬁcation accuracy [3]. We propose a simple combination method for detection.\\nWe pre-train another network in ImageNet, using\\nthe same structure but different random initializations. Then we repeat the above detection algorithm.\\nTable 12 (SPP-net (2)) shows the results of this network. Its mAP is comparable with the ﬁrst network\\n(59.1% vs. 59.2%), and outperforms the ﬁrst network\\nin 11 categories.\\nGiven the two models, we ﬁrst use either model\\nto score all candidate windows on the test image.\\nThen we perform non-maximum suppression on the\\nunion of the two sets of candidate windows (with',\n",
       " 'to score all candidate windows on the test image.\\nThen we perform non-maximum suppression on the\\nunion of the two sets of candidate windows (with\\ntheir scores). A more conﬁdent window given by\\none method can suppress those less conﬁdent given\\nby the other method. After combination, the mAP\\nis boosted to 60.9% (Table 12). In 17 out of all 20\\ncategories the combination performs better than either\\nindividual model. This indicates that the two models\\nare complementary.\\nWe further ﬁnd that the complementarity is mainly\\nbecause of the convolutional layers. We have tried to\\ncombine two randomly initialized ﬁne-tuned results\\nof the same convolutional model, and found no gain.\\n12\\n4.5 ILSVRC 2014 Detection\\nThe ILSVRC 2014 detection [26] task involves 200\\ncategories. There are \\x18450k/20k/40k images in the\\ntraining/validation/testing sets. We focus on the task\\nof the provided-data-only track (the 1000-category\\nCLS training data is not allowed to use).\\nThere are three major differences between the detection (DET) and classiﬁcation (CLS) training datasets,\\nwhich greatly impacts the pre-training quality. First,',\n",
       " 'There are three major differences between the detection (DET) and classiﬁcation (CLS) training datasets,\\nwhich greatly impacts the pre-training quality. First,\\nthe DET training data is merely 1/3 of the CLS\\ntraining data. This seems to be a fundamental challenge of the provided-data-only DET task. Second, the\\ncategory number of DET is 1/5 of CLS. To overcome\\nthis problem, we harness the provided subcategory\\nlabels2for pre-training. There are totally 499 nonoverlapping subcategories ( i.e., the leaf nodes in the\\nprovided category hierarchy). So we pre-train a 499category network on the DET training set. Third, the\\ndistributions of object scales are different between\\nDET/CLS training sets. The dominant object scale in\\nCLS is about 0.8 of the image length, but in DET is\\nabout 0.5. To address the scale difference, we resize\\neach training image to min(w;h) = 400 (instead of\\n256), and randomly crop 224\\x02224views for training.\\nA crop is only used when it overlaps with a ground\\ntruth object by at least 50%.\\nWe verify the effect of pre-training on Pascal VOC\\n2007. For a CLS-pre-training baseline, we consider',\n",
       " 'truth object by at least 50%.\\nWe verify the effect of pre-training on Pascal VOC\\n2007. For a CLS-pre-training baseline, we consider\\nthe pool 5features (mAP 43.0% in Table 9). Replaced\\nwith a 200-category network pre-trained on DET, the\\nmAP signiﬁcantly drops to 32.7%. A 499-category\\npre-trained network improves the result to 35.9%.\\nInterestingly, even if the amount of training data\\ndo not increase, training a network of more categories boosts the feature quality. Finally, training with\\nmin(w;h) = 400 instead of 256 further improves the\\nmAP to 37.8%. Even so, we see that there is still a\\nconsiderable gap to the CLS-pre-training result. This\\nindicates the importance of big data to deep learning.\\nFor ILSVRC 2014, we train a 499-category Overfeat7 SPP-net. The remaining steps are similar to the\\nVOC 2007 case. Following [7], we use the validation\\nset to generate the positive/negative samples, with\\nwindows proposed by the selective search fast mode.\\nThe training set only contributes positive samples\\nusing the ground truth windows. We ﬁne-tune the fc\\nlayers and then train the SVMs using the samples in',\n",
       " 'The training set only contributes positive samples\\nusing the ground truth windows. We ﬁne-tune the fc\\nlayers and then train the SVMs using the samples in\\nboth validation and training sets. The bounding box\\nregression is trained on the validation set.\\nOur single model leads to 31.84% mAP in the\\nILSVRC 2014 testing set [26]. We combine six similar\\nmodels using the strategy introduced in this paper.\\nThe mAP is 35.11% in the testing set [26]. This result\\nranks #2 in the provided-data-only track of ILSVRC\\n2014 (Table 13) [26]. The winning result is 37.21% from\\n2. Using the provided subcategory labels is allowed, as is explicitly stated in the competition introduction.rank team mAP\\n1 NUS 37.21\\n2 ours 35.11\\n3 UvA 32.02\\n- (our single-model) (31.84)\\n4 Southeast-CASIA 30.47\\n5 1-HKUST 28.86\\n6 CASIA CRIPAC 2 28.61\\nTable 13: The competition results of ILSVRC 2014\\ndetection (provided-data-only track) [26]. The best\\nentry of each team is listed.\\nNUS, which uses contextual information.\\nOur system still shows great advantages on speed',\n",
       " 'entry of each team is listed.\\nNUS, which uses contextual information.\\nOur system still shows great advantages on speed\\nfor this dataset. It takes our single model 0.6 seconds\\n(0.5 for conv, 0.1 for fc, excluding proposals) per testing image on a GPU extracting convolutional features\\nfrom all 5 scales. Using the same model, it takes 32\\nseconds per image in the way of RCNN. For the 40k\\ntesting images, our method requires 8 GPU \\x01hours to\\ncompute convolutional features, while RCNN would\\nrequire 15 GPU\\x01days.\\n5 C ONCLUSION\\nSPP is a ﬂexible solution for handling different scales,\\nsizes, and aspect ratios. These issues are important in\\nvisual recognition, but received little consideration in\\nthe context of deep networks. We have suggested a solution to train a deep network with a spatial pyramid\\npooling layer. The resulting SPP-net shows outstanding accuracy in classiﬁcation/detection tasks and\\ngreatly accelerates DNN-based detection. Our studies\\nalso show that many time-proven techniques/insights\\nin computer vision can still play important roles in\\ndeep-networks-based recognition.\\nAPPENDIX A\\nIn the appendix, we describe some implementation\\ndetails:\\nMean Subtraction.\\nThe 224\\x02224 cropped training/testing images are',\n",
       " 'APPENDIX A\\nIn the appendix, we describe some implementation\\ndetails:\\nMean Subtraction.\\nThe 224\\x02224 cropped training/testing images are\\noften pre-processed by subtracting the per-pixel mean\\n[3]. When input images are in any sizes, the ﬁxedsize mean image is not directly applicable. In the\\nImageNet dataset, we warp the 224 \\x02224 mean image\\nto the desired size and then subtract it. In Pascal VOC\\n2007 and Caltech101, we use the constant mean (128)\\nin all the experiments.\\nImplementation of Pooling Bins.\\nWe use the following implementation to handle all\\nbins when applying the network. Denote the width\\nand height of the conv 5feature maps (can be the\\nfull image or a window) as wandh. For a pyramid level with n\\x02nbins, the (i;j)-th bin is in the\\nrange of [bi\\x001\\nnwc;di\\nnwe]\\x02[bj\\x001\\nnhc;dj\\nnhe]. Intuitively,\\n13\\nbottle: 0. 24person:1.20\\nsheep:1.52\\nchair:0.21\\ndiningtable:0.78person:1.16pers on:1.05',\n",
       " 'sheep:1.52\\nchair:0.21\\ndiningtable:0.78person:1.16pers on:1.05\\npottedplant:0.21chair:4.79pottedplant:0.73\\nc hair: 0. 33diningtable: 0.34\\nchair:0.89\\nbus:0.56\\ncar:3.24\\nca r:3.4 5pers on:1.52\\ntrain:0.31\\ntrain:1.62\\npottedplant:0.33\\npottedplant:0.78\\nsofa:0.55tvmonitor:1.77\\naeroplane: 0.45\\naeroplane:1.40aeroplane:1.01\\naeroplane:0.94aeroplane:0.93\\naeroplane:0.91aeroplane:0.79\\naeroplane:0.57 aeroplane:0.54\\nperson:0.93\\nperson:0.68\\nhorse:1.73pers on:1.91\\nboat:0.60pers on:3.20\\ncar:2.52\\nbus:2.01\\ncar:0.93per son:4.16person:0.79\\npers on:0.32\\nhors e:1.68\\nhors e:0.61\\nhorse:1.29',\n",
       " 'pers on:0.32\\nhors e:1.68\\nhors e:0.61\\nhorse:1.29\\nper son:1.23\\nc hair:0.87pers on:1.79\\nper son:0.91\\nsofa:0.22person:0.85\\nsofa:0.58\\nc ow:2.36\\ncow:1.88co w:1 .86\\ncow:1.82cow:1.39 cow:1.31\\nca t:0.5 2person:1.02\\nperson:0.40\\nbicy cle:2.85bicy cle:2.71\\nbicycle:2.04bicy cle:0.67person:3.35\\nperson:2.39\\npers on:2.11\\npers on:0.27\\nperson:0.22bus:1.42\\nperson:3.29person:1.18\\nbottle:1.15pottedplant:0.81\\ns heep:1.81s heep:1.17\\nsheep:0.81\\nbird:0.24pott edplant:0.35 pot tedplant: 0. 20\\ncar:1.31person:1.60\\nperson:0.62\\ndog:0.37person:0.38\\ndog:0.99person:1.48',\n",
       " 'car:1.31person:1.60\\nperson:0.62\\ndog:0.37person:0.38\\ndog:0.99person:1.48\\npers on:0.22\\ncow:0.80person:3.29\\nperson:2.69pers on:2.42\\nperson:1.05\\nperson:0.92pers on:0.76\\nbird:1.39\\nbird:0.84\\nbottle:1.20diningtable:0.96pers on:1.53pers on:1.52\\npers on:0.73\\ncar:0.12\\ncar:0.11\\ncar:0.04car:0.03\\ncar:3.98car:1.95car:1.39\\ncar:0.50\\nbird:1.47\\nsofa:0.41 person:2.15pers on:0 .86tvmonitor:2.24\\nmotorbike:1.11motorbike:0.74person:1.36\\npers on:1.10\\nFigure 6: Example detection results of “SPP-net ftfc 7bb” on the Pascal VOC 2007 testing set (59.2% mAP).\\nAll windows with scores >0 are shown. The predicted category/score are marked. The window color is',\n",
       " 'All windows with scores >0 are shown. The predicted category/score are marked. The window color is\\nassociated with the predicted category. These images are manually selected because we ﬁnd them impressive.\\nVisit our project website to see all 4,952 detection results in the testing set.\\nif rounding is needed, we take the ﬂoor operation on\\nthe left/top boundary and ceiling on the right/bottom\\nboundary.\\nMapping a Window to Feature Maps.\\nIn the detection algorithm (and multi-view testing\\non feature maps), a window is given in the image\\ndomain, and we use it to crop the convolutional feature maps ( e.g., conv 5) which have been sub-sampled\\nseveral times. So we need to align the window on the\\nfeature maps.\\nIn our implementation, we project the corner point\\nof a window onto a pixel in the feature maps, such\\nthat this corner point in the image domain is closest\\nto the center of the receptive ﬁeld of that feature mappixel. The mapping is complicated by the padding\\nof all convolutional and pooling layers. To simplify\\nthe implementation, during deployment we pad bp=2c\\npixels for a layer with a ﬁlter size of p. As such, for\\na response centered at (x0;y0), its effective receptive',\n",
       " 'pixels for a layer with a ﬁlter size of p. As such, for\\na response centered at (x0;y0), its effective receptive\\nﬁeld in the image domain is centered at (x;y) =\\n(Sx0;Sy0)whereSis the product of all previous\\nstrides. In our models, S= 16 for ZF-5 on conv 5,\\nandS= 12 for Overfeat-5/7 on conv 5=7. Given a\\nwindow in the image domain, we project the left (top)\\nboundary by: x0=bx=Sc+ 1 and the right (bottom)\\nboundaryx0=dx=Se\\x001. If the padding is not bp=2c,\\nwe need to add a proper offset to x.\\n14\\nREFERENCES\\n[1] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\\nW. Hubbard, and L. D. Jackel, “Backpropagation applied to\\nhandwritten zip code recognition,” Neural computation , 1989.',\n",
       " 'W. Hubbard, and L. D. Jackel, “Backpropagation applied to\\nhandwritten zip code recognition,” Neural computation , 1989.\\n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei, “Imagenet: A large-scale hierarchical image database,” in\\nCVPR , 2009.\\n[3] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in NIPS ,\\n2012.\\n[4] M. D. Zeiler and R. Fergus, “Visualizing and understanding\\nconvolutional neural networks,” arXiv:1311.2901 , 2013.\\n[5] P . Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y. LeCun, “Overfeat: Integrated recognition, localization\\nand detection using convolutional networks,” arXiv:1312.6229 ,\\n2013.\\n[6] A. V . K. Chatﬁeld, K. Simonyan and A. Zisserman, “Return of',\n",
       " '2013.\\n[6] A. V . K. Chatﬁeld, K. Simonyan and A. Zisserman, “Return of\\nthe devil in the details: Delving deep into convolutional nets,”\\ninArXiv:1405.3531 , 2014.\\n[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\\nhierarchies for accurate object detection and semantic segmentation,” in CVPR , 2014.\\n[8] W. Y. Zou, X. Wang, M. Sun, and Y. Lin, “Generic object detection with dense neural patterns and regionlets,” in\\nArXiv:1404.4316 , 2014.\\n[9] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “Cnn\\nfeatures off-the-shelf: An astounding baseline for recogniton,”\\ninCVPR 2014, DeepVision Workshop , 2014.\\n[10] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface:\\nClosing the gap to human-level performance in face veriﬁcation,” in CVPR , 2014.',\n",
       " 'Closing the gap to human-level performance in face veriﬁcation,” in CVPR , 2014.\\n[11] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdevr,\\n“Panda: Pose aligned networks for deep attribute modeling,”\\ninCVPR , 2014.\\n[12] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale\\norderless pooling of deep convolutional activation features,”\\ninArXiv:1403.1840 , 2014.\\n[13] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng,\\nand T. Darrell, “Decaf: A deep convolutional activation feature\\nfor generic visual recognition,” arXiv:1310.1531 , 2013.\\n[14] K. Grauman and T. Darrell, “The pyramid match kernel:\\nDiscriminative classiﬁcation with sets of image features,” in\\nICCV , 2005.\\n[15] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial pyramid matching for recognizing natural scene',\n",
       " '[15] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial pyramid matching for recognizing natural scene\\ncategories,” in CVPR , 2006.\\n[16] J. Sivic and A. Zisserman, “Video google: a text retrieval\\napproach to object matching in videos,” in ICCV , 2003.\\n[17] J. Yang, K. Yu, Y. Gong, and T. Huang, “Linear spatial pyramid\\nmatching using sparse coding for image classiﬁcation,” in\\nCVPR , 2009.\\n[18] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong, “Localityconstrained linear coding for image classiﬁcation,” in CVPR ,\\n2010.\\n[19] F. Perronnin, J. S ´anchez, and T. Mensink, “Improving the ﬁsher\\nkernel for large-scale image classiﬁcation,” in ECCV , 2010.\\n[20] K. E. van de Sande, J. R. Uijlings, T. Gevers, and A. W. Smeulders, “Segmentation as selective search for object recognition,”',\n",
       " 'inICCV , 2011.\\n[21] L. Fei-Fei, R. Fergus, and P . Perona, “Learning generative\\nvisual models from few training examples: An incremental\\nbayesian approach tested on 101 object categories,” CVIU ,\\n2007.\\n[22] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman, “The PASCAL Visual Object Classes Challenge\\n2007 (VOC2007) Results,” 2007.\\n[23] P . F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object detection with discriminatively trained partbased models,” P AMI , 2010.\\n[24] N. Dalal and B. Triggs, “Histograms of oriented gradients for\\nhuman detection,” in CVPR , 2005.\\n[25] C. L. Zitnick and P . Doll ´ar, “Edge boxes: Locating object\\nproposals from edges,” in ECCV , 2014.\\n[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,',\n",
       " '[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein\\net al. , “Imagenet large scale visual recognition challenge,”\\narXiv:1409.0575 , 2014.\\n[27] K. Chatﬁeld, V . Lempitsky, A. Vedaldi, and A. Zisserman, “The\\ndevil is in the details: an evaluation of recent feature encoding\\nmethods,” in BMVC , 2011.\\n[28] A. Coates and A. Ng, “The importance of encoding versus\\ntraining with sparse coding and vector quantization,” in ICML ,\\n2011.\\n[29] D. G. Lowe, “Distinctive image features from scale-invariant\\nkeypoints,” IJCV , 2004.\\n[30] J. C. van Gemert, J.-M. Geusebroek, C. J. Veenman, and A. W.\\nSmeulders, “Kernel codebooks for scene categorization,” in',\n",
       " 'Smeulders, “Kernel codebooks for scene categorization,” in\\nECCV , 2008.[31] M. Lin, Q. Chen, and S. Yan, “Network in network,”\\narXiv:1312.4400 , 2013.\\n[32] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper\\nwith convolutions,” arXiv:1409.4842 , 2014.\\n[33] K. Simonyan and A. Zisserman, “Very deep convolutional\\nnetworks for large-scale image recognition,” arXiv:1409.1556 ,\\n2014.\\n[34] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , “Learning and\\ntransferring mid-level image representations using convolutional neural networks,” in CVPR , 2014.\\n[35] Y. Jia, “Caffe: An open source convolutional architecture\\nfor fast feature embedding,” http://caffe.berkeleyvision.org/,\\n2013.',\n",
       " 'for fast feature embedding,” http://caffe.berkeleyvision.org/,\\n2013.\\n[36] A. G. Howard, “Some improvements on deep convolutional\\nneural network based image classiﬁcation,” ArXiv:1312.5402 ,\\n2013.\\n[37] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P . Perez, and\\nC. Schmid, “Aggregating local image descriptors into compact\\ncodes,” TP AMI , vol. 34, no. 9, pp. 1704–1716, 2012.\\n[38] C.-C. Chang and C.-J. Lin, “Libsvm: a library for support\\nvector machines,” ACM Transactions on Intelligent Systems and\\nTechnology (TIST) , 2011.\\n[39] X. Wang, M. Yang, S. Zhu, and Y. Lin, “Regionlets for generic\\nobject detection,” in ICCV , 2013.\\n[40] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks\\nfor object detection,” in NIPS , 2013.\\nCHANGELOG\\narXiv v1 . Initial technical report for ECCV 2014 paper.',\n",
       " 'for object detection,” in NIPS , 2013.\\nCHANGELOG\\narXiv v1 . Initial technical report for ECCV 2014 paper.\\narXiv v2 . Submitted version for TPAMI. Includes extra\\nexperiments of SPP on various architectures. Includes\\ndetails for ILSVRC 2014.\\narXiv v3 . Accepted version for TPAMI. Includes comparisons with R-CNN using the same architecture.\\nIncludes detection experiments using EdgeBoxes.\\narXiv v4 . Revised “Mapping a Window to Feature\\nMaps” in Appendix for easier implementation.',\n",
       " 'Accepted as a workshop contribution at ICLR 2015\\nLEARNING DEEPSTRUCTURED MODELS\\nLiang-Chieh Chen\\x03\\nDepartment of Computer Science, UCLA, lcchen@cs.ucla.edu\\nAlexander G. Schwing\\x03\\nDepartment of Computer Science, University of Toronto, aschwing@cs.toronto.edu\\nAlan L. Yuille\\nDepartment of Statistics, UCLA, yuille@stat.ucla.edu\\nRaquel Urtasun\\nDepartment of Computer Science, University of Toronto, urtasun@cs.toronto.edu\\nABSTRACT\\nMany problems in real-world applications involve predicting several random variables which are statistically related. Markov random ﬁelds (MRFs) are a great\\nmathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations\\nwhile taking into account the dependencies between the output random variables.\\nTowards this goal, we propose a training algorithm that is able to learn structured\\nmodels jointly with deep features that form the MRF potentials. Our approach is\\nefﬁcient as it blends learning and inference and makes use of GPU acceleration.\\nWe demonstrate the effectiveness of our algorithm in the tasks of predicting words\\nfrom noisy images, as well as multi-class classiﬁcation of Flickr photographs. We',\n",
       " 'We demonstrate the effectiveness of our algorithm in the tasks of predicting words\\nfrom noisy images, as well as multi-class classiﬁcation of Flickr photographs. We\\nshow that joint learning of the deep features and the MRF parameters results in\\nsigniﬁcant performance gains.\\n1 I NTRODUCTION\\nDeep learning algorithms attempt to model high-level abstractions of the data using architectures composed of multiple non-linear transformations. A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007;\\nSalakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a\\nwide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen\\net al., 2014). Recently, state-of-the-art results have been achieved in many computer vision tasks,\\noutperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al.,\\n2014).\\nDeep neural networks can, however, be even more powerful when combined with graphical models',\n",
       " '2014).\\nDeep neural networks can, however, be even more powerful when combined with graphical models\\nin order to capture the statistical dependencies between the variables of interest. For example, Deng\\net al. (2014) exploit mutual exclusion, overlapping and subsumption properties of class labels in order to better predict in large scale classiﬁcation tasks. In pose estimation, more accurate predictions\\ncan be obtained when encoding the spatial relationships between joint locations (Tompson et al.,\\n2014).\\nIt is, however, an open problem how to develop scalable deep learning algorithms that can learn\\nhigher-order knowledge taking into account the output variable’s dependencies. Existing approaches\\noften rely on a two-step process (Nowozin et al., 2011; Xu et al., 2014) where a non-linear classiﬁer\\nthat employs deep features is trained ﬁrst, and its output is used to generate potentials for the structured predictor. This piece-wise training is, however, suboptimal as the deep features are learned\\n\\x03The ﬁrst two authors contributed equally to this work.\\n1arXiv:1407.2538v3  [cs.LG]  27 Apr 2015\\nAccepted as a workshop contribution at ICLR 2015',\n",
       " '1arXiv:1407.2538v3  [cs.LG]  27 Apr 2015\\nAccepted as a workshop contribution at ICLR 2015\\nwhile ignoring the dependencies between the variables of interest, e.g., independently learned segmentation and detection features (Hariharan et al., 2014) might be focusing on predicting the same\\nexamples correctly. But when learned jointly they can improve their predictive power by exploiting\\ncomplementary information to ﬁx additional mistakes.\\nIn this paper we extend deep learning algorithms to learn complex representations taking into account the dependencies between the output random variables. Towards this goal, we propose a\\nlearning algorithm that is able to learn structured models with arbitrary graphs jointly with deep\\nfeatures that form Markov random ﬁeld (MRF) potentials. Our approach is efﬁcient as it blends\\nlearning and inference resulting in a single loop algorithm which makes use of GPU acceleration.\\nWe demonstrate the effectiveness of our method in the tasks of predicting words from noisy images,\\nand multi-class classiﬁcation of Flickr photographs. We show that joint learning of deep features\\nand MRF parameters results in big performance gains.\\n2 L EARNING DEEPSTRUCTURED MODELS',\n",
       " 'and MRF parameters results in big performance gains.\\n2 L EARNING DEEPSTRUCTURED MODELS\\nIn this section we investigate how to learn ‘deep features’ that take into account the dependencies between the output variables. Let y2Y be the set of random variables y= (y1;:::;yN)\\nthat we are interested in predicting. In this work we assume the space of valid conﬁgurations to\\nbe a product space, i.e.,Y=QN\\ni=1Yi, and the domain of each individual variable yito be discrete, i.e.,Yi=f1;:::;jYijg. Given input data x2X and parameters w2RAof the function\\nF(x;y;w) :X\\x02Y\\x02 RA!R, inference amounts to ﬁnding the highest scoring conﬁguration\\ny\\x03= arg max yF(x;y;w):Note that ifFis a deep network and there are no connections between\\nthe output variables to be predicted, inference corresponds to a forward pass to evaluate the function,\\nfollowed by independently ﬁnding the largest response for each variable. This can be interpreted as\\ninference in a graphical model with only unary potentials. However, for arbitrary graphical models',\n",
       " 'inference in a graphical model with only unary potentials. However, for arbitrary graphical models\\nit is NP-hard to ﬁnd the maximizing conﬁguration y\\x03since the inference program generally requires\\na search over a space of sizeQN\\ni=1jYij. Note also that log-linear models are a special case of this\\nprogram, with F(x;y;w) =w>\\x1e(x;y)and\\x1e(x;y)denoting a feature vector computed using the\\ninput-output pair (x;y).\\nIn this work, we consider the general setting where F(x;y;w)is an arbitrary scalar-valued function ofwand(x;y). In our experiments Fis a function composition of non-linear base mappings\\nlike convolutions, rectiﬁcations, pooling etc. We let the probability of an arbitrary conﬁguration ^y\\nbe given by the annealed soft-max p(x;y)(^yjw;\\x0f) =1\\nZ\\x0f(x;w)exp(F(x;^y;w))1=\\x0f:HerebyZ\\x0f(x;w)\\nrefers to the partition function, normalizing the distribution p(x;y)to lie within the probability simplex\\x01viaZ(x;w) =P',\n",
       " 'refers to the partition function, normalizing the distribution p(x;y)to lie within the probability simplex\\x01viaZ(x;w) =P\\n^y2Yexp(F(x;^y;w))1=\\x0f. The annealing/temperature parameter \\x0f\\x150is\\nused to adjust the uniformity of the distribution. We consider general graphical models where the\\ncomputation of Z\\x0f(x;w)is #P-hard.\\n2.1 L EARNING VIA GRADIENT DESCENT\\nDuring learning, given a training set Dof input-output pairs (x;y)2D, we are interested in ﬁnding\\nthe parameters wof the model. We do so by maximizing the data likelihood, i.e., minimizing the\\nnegative log-likelihood \\x00lnQ\\n(x;y)2Dp(x;y)(yjw;\\x0f)which yields\\nmin\\nwX\\n(x;y)2D(\\x0flnZ\\x0f(x;w)\\x00F(x;y;w)): (1)\\nNote that this is equivalent to maximizing the cross-entropy between a target distributionp(x;y);tg(^y) =\\x0e(^y=y)placing all its mass on the groundtruth label,',\n",
       " 'and the model distribution p(x;y)(^yjw;\\x0f). Hence Eq. (1) is equivalently obtained by\\nmaxwP\\n(x;y);^y2Yp(x;y);tg(^y) lnp(x;y)(^yjw;\\x0f). It is easily possible to incorporate more general target distributions into Eq. (1). Note also that \\x0f= 0recovers the structured hinge loss objective.\\nMinimizing Eq. (1) w.r.t. wrequires computation of the gradient@\\n@wP\\n(x;y)2D\\x00lnp(x;y)(yjw;\\x0f),\\nwhich is given by a transformed difference between the distributions of the model p(x;y)(^yjw;\\x0f)and\\n2\\nAccepted as a workshop contribution at ICLR 2015\\nRepeat until stopping criteria\\n1. Forward pass to compute F(x;^y;w)\\n2. Compute p(x;y)(^yjw;\\x0f)\\n3. Backward pass via chain rule to obtain gradient\\n4. Update parameters w\\nFigure 1: Gradient descent algorithm for learning deep structured models.\\nthe targetp(x;y);tg(^y):\\nX\\n(x;y)2DX\\n^y2Y@',\n",
       " 'the targetp(x;y);tg(^y):\\nX\\n(x;y)2DX\\n^y2Y@\\n@wF(x;^y;w)\\x00\\np(x;y)(^yjw;\\x0f)\\x00p(x;y);tg(^y)\\x01\\n: (2)\\nA gradient descent algorithm for minimizing Eq. (1) will iterate between the following steps: (i) For\\na givenwevaluate the function F, (ii) compute the model distribution p(x;y)(^yjw;\\x0f), (iii) propagate\\nthe difference between the model and target distribution using a backward pass (resembling the chain\\nrule for composite functions) and (iv) update the parameters w. This is summarized in Fig. 1.\\n2.2 A PPROXIMATE LEARNING\\nNote that for general graphical models the exact computation of p(x;y)(^yjw;\\x0f)is not possible. As a\\nconsequence it is intractable to compute the exact gradient of the cost-function given in Eq. (2) and\\none has to resort to approximate solutions.\\nInspired by approximations used for log-linear models, we make use of the following identity (Wainwright & Jordan, 2008; Koller & Friedman, 2009):',\n",
       " 'Inspired by approximations used for log-linear models, we make use of the following identity (Wainwright & Jordan, 2008; Koller & Friedman, 2009):\\n\\x0flnZ\\x0f(x;w) = max\\np(x;y)(^y)2\\x01E[F(x;^y;w)] +\\x0fH(p(x;y)); (3)\\nwhere Edenotes an expectation over p(x;y)(^y)andHrefers to the entropy.\\nFor most applications, F(x;y;w)decomposes into a sum of functions, each depending on a local\\nsubset of variables yr,i.e.,F(x;y;w) =P\\nr2Rfr(x;yr;w):Herebyris a restriction of the variable\\ntupley= (y1;:::;yN)to the subset r\\x12f1;:::;Ng,i.e.,yr= (yi)i2r. All subsets rrequired to\\ncompute the model function Fare summarized in the set R.\\nPlugging this decomposition into Eq. (3), we equivalently get the log-partition function\\n\\x0flnZ\\x0f(x;w)viamaxp(x;y)(^y)2\\x01P',\n",
       " '\\x0flnZ\\x0f(x;w)viamaxp(x;y)(^y)2\\x01P\\nr;^yrp(x;y);r(^yr)fr(x;^yr;w) +\\x0fH(p(x;y));where we use\\nmarginalsp(x;y);r(^yr) =P\\nynyrp(x;y)(y).\\nDespite the assumed locality of the scoring function, the learning task remains computationally\\nchallenging since the entropy H(p(x;y))can only be computed exactly for a very small set of models,\\ne.g., models for which the joint distribution p(x;y)(y)is equivalently described by low tree-width\\nmodels. In addition, the marginalization constraints are exponential in size.\\nTo deal with both issues a common solution in log-linear models is to approximate the true marginals\\np(x;y);rwith local beliefs b(x;y);rthat are not required to fulﬁll marginalization constraints globally,\\nbut only locally (Wainwright & Jordan, 2008). That means marginals b(x;y);rare not required to arise\\nfrom a common joint distribution p(x;y). In addition, we approximate the entropy via the fractional',\n",
       " 'from a common joint distribution p(x;y). In addition, we approximate the entropy via the fractional\\nentropy (Wiegerinck & Heskes, 2003), i.e.,H(p(x;y))\\x19P\\nrcrH(b(x;y);r). Counting numbers cr\\nare employed to weight the marginal entropies. Putting all this together, we obtain the following\\napproximation for \\x0flnZ\\x0f(x;w):\\nmax\\nb(x;y)2C(x;y)X\\nr;^yrb(x;y);r(^yr)fr(x;^yr;w) +X\\nr\\x0fcrH(b(x;y);r): (4)\\nHereby beliefs are constrained to the local polytope\\nC(x;y)=(8r b (x;y);r2\\x01\\n8r;^yr;p2P(r)P\\n^ypn^yrb(x;y);p(^yp) =b(x;y);r(^yr);\\n3\\nAccepted as a workshop contribution at ICLR 2015\\nLet~cr;p=cp=(cr+P\\np02P(r)cp0). Repeat until stopping criteria',\n",
       " '3\\nAccepted as a workshop contribution at ICLR 2015\\nLet~cr;p=cp=(cr+P\\np02P(r)cp0). Repeat until stopping criteria\\n1. Forward pass to compute fr(x;^yr;w)8(x;y);r;yr\\n2. Get beliefs b(x;y);r/exp^fr(x;^yr;w;\\x15)\\n\\x0fcrby iterating over r:8(x;y);p2P(r);^yr\\n\\x16(x;y);p!r(^yr) =\\x0fcplnX\\n^ypn^yrexpfp(x;^yp;w)\\x00P\\np02P(p)\\x15(x;y);p!p0(^yp) +P\\nr02C(p)nr\\x15(x;y);r0!p(^yr0)\\n\\x0fcp\\n\\x15(x;y);r!p(^yr)/~cr;p0\\n@fr(x;^yr;w) +X\\nc2C(r)\\x15(x;y);c!r(^yc) +X\\np2P(r)\\x16(x;y);p!r(^yr)1\\nA\\x00\\x16(x;y);p!r(^yr)',\n",
       " 'p2P(r)\\x16(x;y);p!r(^yr)1\\nA\\x00\\x16(x;y);p!r(^yr)\\n3. Backward pass via chain-rule for gradient g=P\\n(x;y);r;^yrb(x;y);r(^yr)rwfr(x;^yr;w)\\x00rwF(w)\\n4. Update parameters wusing stepsize \\x11viaw w\\x00\\x11g\\nFigure 2: Efﬁcient learning algorithm that blends learning and inference.\\nwithP(r)the set of parents of region r,i.e.,P(r)\\x12fp2R :r\\x1apg, which subsumes those\\nregions for which we want the marginalization constraint to hold. Conversely, we deﬁne the set of\\nchildren asC(r) =fc2R:r2P(c)g.\\nWe can thus rewrite the learning problem by plugging the approximations derived in Eq. (4) into\\nEq. (1). This gives rise to the new approximated learning program\\nmin\\nwX\\n(x;y)2D0\\n@max\\nb(x;y)2C(x;y)8\\n<\\n:X',\n",
       " 'min\\nwX\\n(x;y)2D0\\n@max\\nb(x;y)2C(x;y)8\\n<\\n:X\\nr;^yrb(x;y);r(^yr)fr(x;^yr;w) +X\\nr\\x0fcrH(b(x;y);r)9\\n=\\n;\\x00F(x;y;w)1\\nA:(5)\\nTo iteratively update the parameters for the non-smooth approximated cost function given in Eq. (5)\\nwe require a sub-gradient w.r.t. w, which in turn requires to solve the maximization w.r.t. the beliefs\\nbexactly. This is a non-trivial task in itself as inference in general graphical models is NP-hard.\\nIterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005;\\nWeiss et al., 2007; Meltzer et al., 2009) are typically employed. Importantly, note that combining\\nthe procedure outlined in Fig. 1 with iterative message passing to approximate p(x;y)(^yjw;\\x0f)results\\nin a double-loop algorithm which would be slow for many graphical models of interest.',\n",
       " 'in a double-loop algorithm which would be slow for many graphical models of interest.\\n2.3 E FFICIENT APPROXIMATE LEARNING BY BLENDING LEARNING AND INFERENCE\\nIn this section we propose a more efﬁcient algorithm that is based on the principle of blending\\nlearning ( i.e., parameter updates) and inference. Thus we are interested in only performing a single\\nmessage passing iteration before updating the parameters w. Note that simply reducing the number\\nof iterations is generally not an option as the obtained beliefs b(x;y);rare by no means accurate.\\nHowever, assuming all counting numbers crto be positive, we can derive an algorithm that is able\\nto interleave minimization w.r.t. wand maximization of the beliefs b. Such a procedure is more\\nefﬁcient as we are able to update the parameters wmuch more frequently.\\nTo interleave both programs we convert maximization of the beliefs into a minimization by employing the dual program as detailed in the following claim. This is possible since the maximization\\nproblem is concave in b(x;y)if8r,\\x0fcr\\x150.\\nClaim 1 Assume\\x0fcr\\x1508rand letF(w) =P\\n(x;y)2DF(x;y;w)denote the sum of empirical',\n",
       " 'Claim 1 Assume\\x0fcr\\x1508rand letF(w) =P\\n(x;y)2DF(x;y;w)denote the sum of empirical\\nfunction observations. Let \\x15(x;y);r!p(^yr)be the Lagrange multipliers for each marginalization\\nconstraintP\\n^ypn^yrb(x;y);p(^yp) =b(x;y);r(^yr)within the polytope C(x;y). Then the approximated\\ngeneral structured prediction task shown in Eq. (5)is equivalent to\\nmin\\nw;\\x15X\\n(x;y);r\\x0fcrlnX\\n^yrexp^fr(x;^yr;w;\\x15)\\n\\x0fcr\\x00F(w); (6)\\n4\\nAccepted as a workshop contribution at ICLR 2015\\nbanal julep resty\\ndrein yojan mothy\\nFigure 3: Samples from the Word50 dataset. High degree of rotation, scaling and translation.\\nwhere we employed the re-parameterization score ^fr(x;^yr;w;\\x15) =fr(x;^yr;w) +P\\nc2C(r)\\x15(x;y);c!r(^yc)\\x00P',\n",
       " 'c2C(r)\\x15(x;y);c!r(^yc)\\x00P\\np2P(r)\\x15(x;y);r!p(^yr).\\nProof: To obtain the dual of the maximization w.r.t. b(x;y)we utilize its Lagrangian L(x;y)=P\\nr;^yrb(x;y);r(^yr)^fr(x;^yr;w;\\x15)+P\\nr\\x0fcrH(b(x;y);r):Maximization of the Lagrangian w.r.t. the primal variables bis possible by employing the relationship stated in Eq. (3) locally 8r. We then obtain\\nthe dual function being the ﬁrst term in Eq. (6). For strict convexity, i.e.,\\x0fcr>0, we reconstruct the beliefs to be proportional to the exponentiated, loss-augmented re-parameterization score\\nb(x;y);r/exp^fr(x;^yr;w;\\x15)\\n\\x0fcr:For\\x0fcr= 0the beliefs correspond to a uniform distribution over the set\\nof maximizers of the loss-augmented re-parameterization score ^fr(x;^yr;w;\\x15). \\x04',\n",
       " 'of maximizers of the loss-augmented re-parameterization score ^fr(x;^yr;w;\\x15). \\x04\\nIt is important to note that by applying duality we managed to convert the min-max task in Eq. (5)\\ninto a single minimization as shown in Eq. (6). Performing block coordinate descent updates to\\nminimize Eq. (6), we are therefore able to interleave both, updating the weights ( i.e., learning) and\\nthe messages ( i.e., inference). This results in a more efﬁcient algorithm, as inference does not have\\nto be run until convergence. Even a single update of the messages sufﬁces. We note that this is\\npossible only if \\x0fcr\\x1508r. Strictly speaking, we require concavity only within the set of feasible\\nbeliefsC(x;y). However, for simplicity we neglect this extension in the following.\\nFig. 2 summarizes our efﬁcient deep structured prediction algorithm which iterates between the\\nfollowing steps. Given parameters wwe perform a standard forward pass to compute fr(x;^yr;w)\\nfor all regions. We then iterate through all regions rand use block-coordinate descent to ﬁnd the',\n",
       " 'for all regions. We then iterate through all regions rand use block-coordinate descent to ﬁnd the\\nglobally optimal value of Eq. (6) w.r.t. \\x15(x;y);r!p(^yr)8(x;y);^yr;p2P(r). This can be done in\\nclosed form and therefore is computed very efﬁciently. We refer the reader to Schwing (2013) for\\na derivation in the log-linear setting. We then compute the gradient using a standard backward pass\\nbefore we update the parameters by performing a step of size \\x11along the negative gradient.\\n2.4 I MPLEMENTATION DETAILS\\nWe implemented the general algorithm presented in Fig. 2 in C++ as a library for Linux, Windows\\nand OS X platforms. It supports usage of the GPU for the forward and backward pass using both,\\nstandard linear algebra packages and manually tuned GPU-kernels. In addition to standard gradient descent, we allow speciﬁcation of both mini-batches, moments and different regularizers like\\n2-norm and1-norm. Between iterations the step-size can be reduced based on either the negative log-likelihood or validation set performance. Contrasting available deep learning packages, our',\n",
       " '2-norm and1-norm. Between iterations the step-size can be reduced based on either the negative log-likelihood or validation set performance. Contrasting available deep learning packages, our\\nfunctionFis speciﬁed using a general computation tree. Hence we support an arbitrarily nested\\nfunction structure composed of data, parameters and function prototypes (convolution, afﬁne function aka fully connected, dropout, local response normalization, pooling, rectiﬁed linear, sigmoid\\nand softmax units). The aforementioned library is accompanied by a program performing learning,\\ninference and gradient checks. To accommodate for large datasets it reads data from HDF5 storage\\nwhile a second thread simultaneously performs the computation. Google protocol buffers are employed to effectively specify the function Fwithout the need to modify any source code. We will\\nrelease this library upon publication. We believe that it will be useful for many researchers.\\n5\\nAccepted as a workshop contribution at ICLR 2015\\nGraph MLP Method H1= 128H1= 256H1= 512H1= 768H1= 1024\\n1st order Markov One LayerUnary only 8.60 / 61.32 10.80 / 64.41 12.50 / 65.69 12.95 / 66.66 13.40 / 67.02',\n",
       " 'JointTrain 16.80 / 65.28 25.20 / 70.75 31.80 / 74.90 33.05 / 76.42 34.30 / 77.02\\nPwTrain 12.70 / 64.35 18.00 / 68.27 22.80 / 71.29 23.25 / 72.62 26.30 / 73.96\\nPreTrainJoint 20.65 /67.42 25.70 /71.65 31.70 / 75.56 34.50 /77.14 35.85 /78.05\\n2nd order Markov One LayerJointTrain 25.50 / 67.13 34.60 / 73.19 45.55 / 79.60 51.55 /82.37 54.05 /83.57\\nPwTrain 10.05 / 58.90 14.10 / 63.44 18.10 / 67.31 20.40 / 70.14 22.20 / 71.25\\nPreTrainJoint 28.15 /69.07 36.85 /75.21 45.75 /80.09 50.10 / 82.30 52.25 / 83.39\\nH1= 512H2= 32H2= 64H2= 128H2= 256H2= 512',\n",
       " 'H1= 512H2= 32H2= 64H2= 128H2= 256H2= 512\\n1st order Markov Two LayerUnary only 15.25 / 69.04 18.15 / 70.66 19.00 / 71.43 19.20 / 72.06 20.40 / 72.51\\nJointTrain 35.95 / 76.92 43.80 / 81.64 44.75 / 82.22 46.00 / 82.96 47.70 / 83.64\\nPwTrain 34.85 / 79.11 38.95 / 80.93 42.75 / 82.38 45.10 / 83.67 45.75 / 83.88\\nPreTrainJoint 42.25 /81.10 44.85 /82.96 46.85 /83.50 47.95 /84.21 47.05 / 84.08\\n2nd order Markov Two LayerJointTrain 54.65 / 83.98 61.80 / 87.30 66.15 / 89.09 64.85 / 88.93 68.00 / 89.96\\nPwTrain 39.95 / 81.14 48.25 / 84.45 52.65 / 86.24 57.10 / 87.61 62.90 / 89.49',\n",
       " 'PwTrain 39.95 / 81.14 48.25 / 84.45 52.65 / 86.24 57.10 / 87.61 62.90 / 89.49\\nPreTrainJoint 62.60 /88.03 65.80 /89.32 68.75 /90.47 68.60 /90.42 69.35 /90.75\\nTable 1: Word / Character accuracy. Performance improves as (1) joint-training is employed, (2) the\\nmodel is more structured, and (3) deeper unary classiﬁers are utilized. The number of hidden units\\nfor the ﬁrst and second layer are denoted as H1andH2respectively.\\nabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\\nabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\\n00.511.522.533.544.55\\nx 1043004005006007008009001000Neg. Log−Likelihood\\nIteration  \\nJointTrain\\nPwTrain\\nPreTrainJoint\\nUnary weights distance-1 edges distance-2 edges Neg. Log-Likelihood',\n",
       " 'Iteration  \\nJointTrain\\nPwTrain\\nPreTrainJoint\\nUnary weights distance-1 edges distance-2 edges Neg. Log-Likelihood\\nFigure 4: (left) Subset of the learned unary weights. Pairwise weights (middle two panels), the\\ndarker, the larger the weight. (right) Negative log-likelihood for different learning approaches.\\n3 E XPERIMENTAL EVALUATION\\nWe demonstrate the performance of our model on two tasks: word recognition and image classiﬁcation. We investigate four strategies to learn the model parameters. ‘ Unary only ’ denotes training\\nonly unary classiﬁers while ignoring the structure of the graphical model, i.e., pairwise weights are\\nequal to 0. ‘JointTrain ’ initializes all weights at random and trains them jointly. ‘ PwTrain ’ uses\\npiecewise training by ﬁrst training the unary potentials and then keeping them ﬁxed when learning the pairwise potentials. ‘ PreTrainJoint ’ pre-trains the unaries but jointly optimizes pairwise\\nweights as well as unary weights in a second step.\\n3.1 W ORD RECOGNITION : W ORD50\\nOur ﬁrst task consists of word recognition from noisy images. Towards this goal, we created a',\n",
       " '3.1 W ORD RECOGNITION : W ORD50\\nOur ﬁrst task consists of word recognition from noisy images. Towards this goal, we created a\\nchallenging dataset by randomly selecting 50 words, each consisting of ﬁve characters. We then\\ngenerated writing variations of each word as follows: we took the lower case characters from the\\nChars74K dataset (de Campos et al., 2009), and inserted them in random background image patches\\n(similar to Larochelle et al. (2007)) by alpha matting, i.e., characters have transparency. To increase\\nthe difﬁculty, we perturbed each character image of size 28\\x0228by scaling, rotation and translation.\\nAs shown in Fig. 3 the task is very challenging, some characters are fairly difﬁcult to recognize even\\nfor humans. We denote the resulting dataset ‘ Word50 .’ The training, validation and test sets have\\n10;000,2;000and2;000variations of words respectively.\\nWe experimented with graphical models composed of unary and pairwise regions deﬁned over ﬁve',\n",
       " 'We experimented with graphical models composed of unary and pairwise regions deﬁned over ﬁve\\nrandom variables, one per character. We encode unary potentials fr(x;yi;wu)using multi-layer perceptrons (MLPs) with rectiﬁed linear units (ReLU). Unless otherwise stated, we deﬁne all pairwise\\n6\\nAccepted as a workshop contribution at ICLR 2015\\nH1=128 H1=256 H1=512 H1=768 H1=1024161820222426283032343638\\n  \\nLinear\\nPairH16\\nPairH32\\nPairH64\\nH2=32 H2=64 H2=128 H2=256 H2=512343638404244464850\\n  \\nLinear\\nPairH16\\nPairH32\\nPairH64\\nOne-Layer MLP Chain Two-Layer MLP Chain\\nFigure 5: Learning non-linear pairwise functions: Word recognition as a function of the number\\nof hidden units for the unary potential. Colors represent different number of hidden units for the\\npairwise potentials. The y-axis shows the word accuracy of using Linear function, or 16 (PairH16),\\n32 (PairH32), and 64 (PairH64) hidden units for the pairwise function.',\n",
       " '32 (PairH32), and 64 (PairH64) hidden units for the pairwise function.\\nfemale/indoor/portrait sky/plant life/tree water/animals/sea animals/dog/indoor indoor/ﬂower/plant life\\nfemale/indoor/portrait sky/plant life/tree water/animals/sky animals/dog ;\\nFigure 6: Flickr test set images and some assigned tags as well as our predictions (bottom row).\\ninteractions via\\nfr(x;yi;yj;wp) =X\\nmnWmn\\x01\\x0e(yi=m;yj=n); (7)\\nwherer=fi;jg,wp=fWg,Wmnis the element of matrix W, and\\x0erefers to the indicator\\nfunction.\\nFor all experiments, we share all unary weights across the nodes of the graphical model as well as all\\npairwise weights for all edges. Note that due to the use of ReLU units, the negative log-likelihood\\nis non-smooth, non-linear and non-convex w.r.t. w. Because of the non-smoothness of F, we utilize\\nmomentum based sub-gradient descent methods to estimate the weights. In particular, we use a',\n",
       " 'momentum based sub-gradient descent methods to estimate the weights. In particular, we use a\\nmini-batch size of 100, a step size of 0:01and a momentum of 0:95. If the unary potential is pretrained, the initial step size is reduced to 0:001. All the unary classiﬁers are trained with 100;000\\niterations over mini-batches. For all experiments, the validation set is only used to decrease the\\nstep size, i.e., if the accuracy on the validation set decreases, we reduce the step size by 0:5. We\\nuse\\x0f= 1, setcr= 1 for all regions r, and perform 10 message passing iterations to compute the\\nmarginal beliefs b(x;y);rat step 2 in Fig. 2 when dealing with loopy models.\\nWe experiment with two graphical models, Markov models of ﬁrst ( i.e., there are links only between\\nyiandyi+1) and second order ( i.e., there are links between yiandyi+1,yi+2) as well as two types of\\nunary potentials with varying degree of structure. We report two metrics, the average character and\\nword accuracy, which correspond to Hamming loss and zero-one loss respectively. Table 1 depicts\\nthe results for the different models, learning strategies and number of hidden units. We observe the',\n",
       " 'word accuracy, which correspond to Hamming loss and zero-one loss respectively. Table 1 depicts\\nthe results for the different models, learning strategies and number of hidden units. We observe the\\nfollowing trends.\\nJoint training helps: Joint training with pre-trained unary classiﬁers (PreTrainJoint) outperforms\\nall the other approaches in almost all cases. The piecewise training method (PwTrain), unable to\\nadapt the non-linearities while learning pairwise weights, often leads to performance worse than\\njoint training.\\nStructure helps: Adding structure to the model is key to capture complex dependencies. As shown\\nin Table 1, more structured models ( i.e., second order Markov model) consistently improves performance.\\n7\\nAccepted as a workshop contribution at ICLR 2015\\n0.00 0.68 0.04 0.06 0.02 0.24 0.03 −0.00 −0.01 0.01 0.04 −0.00 −0.05 −0.01 0.07 −0.01 −0.00 −0.12 0.04 0.01 0.01 0.02 0.04 0.02',\n",
       " '0.68 0.00 0.06 0.06 −0.00 0.36 0.03 −0.08 −0.05 −0.03 0.02 −0.06 −0.12 −0.05 0.74 −0.04 −0.03 −0.21 0.01 −0.03 −0.03 −0.03 0.05 −0.03\\n0.04 0.06 0.00 0.05 −0.06 0.07 −0.12 −0.07 −0.35 −0.03 −0.46 −0.02 −0.34 0.11 0.02 −0.15 −0.14 −0.01 −0.07 −0.21 0.03 −0.08 0.06 −0.03\\n0.06 0.06 0.05 0.00 0.10 0.11 0.07 0.09 0.03 0.10 0.01 0.10 0.02 0.09 0.06 0.08 0.07 0.07 0.08 0.06 0.09 0.09 0.08 0.10',\n",
       " '0.02 −0.00 −0.06 0.10 0.00 0.04 0.08 0.05 0.16 0.17 −0.02 0.09 −0.02 0.06 0.03 0.14 0.36 0.06 0.05 0.01 0.08 0.14 0.06 0.10\\n0.24 0.36 0.07 0.11 0.04 0.00 0.01 0.03 −0.02 0.05 −0.02 0.04 −0.01 0.03 0.12 0.02 0.01 −0.07 0.05 0.05 0.03 0.04 0.07 0.05\\n0.03 0.03 −0.12 0.07 0.08 0.01 0.00 0.02 0.14 0.07 0.14 0.04 0.05 0.03 0.06 0.08 0.07 −0.03 0.36 0.10 0.04 0.05 0.04 0.07',\n",
       " '−0.00 −0.08 −0.07 0.09 0.05 0.03 0.02 0.00 0.02 0.07 −0.03 0.07 0.34 0.04 −0.04 0.04 0.04 0.02 0.05 0.06 0.06 0.06 0.02 0.07\\n−0.01 −0.05 −0.35 0.03 0.16 −0.02 0.14 0.02 0.00 0.12 0.22 0.04 0.24 −0.02 −0.00 0.44 0.12 −0.04 0.10 0.30 0.01 0.23 0.05 0.11\\n0.01 −0.03 −0.03 0.10 0.17 0.05 0.07 0.07 0.12 0.00 −0.00 0.09 0.09 0.07 0.01 0.12 0.26 0.06 0.06 0.10 0.07 0.12 0.07 0.18',\n",
       " '0.04 0.02 −0.46 0.01 −0.02 −0.02 0.14 −0.03 0.22 −0.00 0.00 0.01 0.04 −0.05 0.06 0.08 −0.04 −0.06 0.14 0.09 −0.00 0.06 0.03 0.02\\n−0.00 −0.06 −0.02 0.10 0.09 0.04 0.04 0.07 0.04 0.09 0.01 0.00 0.04 0.07 −0.01 0.06 0.09 0.26 0.06 0.05 0.07 0.09 0.05 0.09\\n−0.05 −0.12 −0.34 0.02 −0.02 −0.01 0.05 0.34 0.24 0.09 0.04 0.04 0.00 −0.03 −0.07 0.09 0.01 0.01 0.08 0.68 0.02 0.05 −0.07 0.10',\n",
       " '−0.01 −0.05 0.11 0.09 0.06 0.03 0.03 0.04 −0.02 0.07 −0.05 0.07 −0.03 0.00 −0.01 0.03 0.03 0.03 0.05 0.01 0.06 0.06 0.04 0.07\\n0.07 0.74 0.02 0.06 0.03 0.12 0.06 −0.04 −0.00 0.01 0.06 −0.01 −0.07 −0.01 0.00 0.00 −0.01 −0.10 0.04 −0.02 0.01 0.00 0.06 0.01\\n−0.01 −0.04 −0.15 0.08 0.14 0.02 0.08 0.04 0.44 0.12 0.08 0.06 0.09 0.03 0.00 0.00 0.09 −0.00 0.07 0.11 0.05 0.22 −0.01 0.10',\n",
       " '−0.00 −0.03 −0.14 0.07 0.36 0.01 0.07 0.04 0.12 0.26 −0.04 0.09 0.01 0.03 −0.01 0.09 0.00 0.05 0.02 0.03 0.05 0.10 0.03 0.27\\n−0.12 −0.21 −0.01 0.07 0.06 −0.07 −0.03 0.02 −0.04 0.06 −0.06 0.26 0.01 0.03 −0.10 −0.00 0.05 0.00 0.02 0.00 0.22 0.03 −0.01 0.05\\n0.04 0.01 −0.07 0.08 0.05 0.05 0.36 0.05 0.10 0.06 0.14 0.06 0.08 0.05 0.04 0.07 0.02 0.02 0.00 0.11 0.06 0.08 0.07 0.06',\n",
       " '0.01 −0.03 −0.21 0.06 0.01 0.05 0.10 0.06 0.30 0.10 0.09 0.05 0.68 0.01 −0.02 0.11 0.03 0.00 0.11 0.00 0.04 0.09 −0.00 0.12\\n0.01 −0.03 0.03 0.09 0.08 0.03 0.04 0.06 0.01 0.07 −0.00 0.07 0.02 0.06 0.01 0.05 0.05 0.22 0.06 0.04 0.00 0.06 0.05 0.07\\n0.02 −0.03 −0.08 0.09 0.14 0.04 0.05 0.06 0.23 0.12 0.06 0.09 0.05 0.06 0.00 0.22 0.10 0.03 0.08 0.09 0.06 0.00 0.06 0.10',\n",
       " '0.04 0.05 0.06 0.08 0.06 0.07 0.04 0.02 0.05 0.07 0.03 0.05 −0.07 0.04 0.06 −0.01 0.03 −0.01 0.07 −0.00 0.05 0.06 0.00 0.07\\n0.02 −0.03 −0.03 0.10 0.10 0.05 0.07 0.07 0.11 0.18 0.02 0.09 0.10 0.07 0.01 0.10 0.27 0.05 0.06 0.12 0.07 0.10 0.07 0.00female\\npeople\\nindoor\\nbaby\\nsea\\nportrait\\ntransport\\nflower\\nsky\\nlake\\nstructures\\nbird\\nplant life\\nfood\\nmale\\nclouds\\nwater\\nanimals\\ncar\\ntree\\ndog\\nsunset\\nnight\\nriver\\nfemalepeopleindoorbabyseaportraittransportflowerskylakestructuresbirdplant lifefoodmalecloudswateranimalscartreedogsunsetnightriver\\n0 5000 100000246810x 105\\nTime [s]Neg. Log−Likelihood\\n  \\nw/o blend\\nw blend\\n0 5000 100000200040006000800010000\\nTime [s]Training error',\n",
       " 'Time [s]Neg. Log−Likelihood\\n  \\nw/o blend\\nw blend\\n0 5000 100000200040006000800010000\\nTime [s]Training error\\n  \\nw/o blend\\nw blend\\n(a) (b)\\nFigure 7: (a) Correlation matrix ( i.e., pairwise potentials) learned on the Flickr dataset. (b) Blending\\nlearning and inference speeds-up training signiﬁcantly.\\nDeep helps: We tested our models using one layer and two-layer perceptrons with both shortrange and long-range connections in the MRF. For the two-layer MLP, the number of hidden units\\nin the ﬁrst layer is ﬁxed to H1= 512 , and we varied the number of hidden units H2in the second\\nlayer. As shown in Table 1 we observe that the deeper and the more structured the model, the better\\nthe performance we achieve. As expected, performance also grows with the number of hidden units.\\nEfﬁciency: Using GPUs, it takes on average 0.064s per iteration for the 1st order Markov model\\nand 0.104s for the 2nd order Markov model. The time employed for training the one layer vs. the',\n",
       " 'and 0.104s for the 2nd order Markov model. The time employed for training the one layer vs. the\\nmulti-layer models is approximately the same. Note that our approach is very efﬁcient, as this is the\\ntime per iteration to train 831,166 weights.\\nLearned parameters: As shown in the left column of Fig. 4, the learned unary weights resemble\\ncharacter strokes. The middle two panels show the learned pairwise weights for distance-1 edges\\n(i.e., edges with only neighboring connections) and distance-2 edges ( i.e., edges connecting every\\nother variable). For example, it shows that ‘q’ is likely to be followed by ‘u,’ and ‘e’ is likely to\\nbe distance-2 away from ‘q’ in this dataset. On the right-most panel, we also show the negative\\nlog-likelihood as a function of the number of joint training iterations. PreTrainJoint can achieve the\\nlowest cost value, while PwTrain has the highest value.\\nNon-linear pairwise functions: To further demonstrate the generality of our approach, we replaced\\nthe linear pairwise function in Eq. (7) by a one-layer MLP, while keeping the other settings identical.',\n",
       " 'the linear pairwise function in Eq. (7) by a one-layer MLP, while keeping the other settings identical.\\nFor this experiment we utilize a 1st order Markov model. As shown in Fig. 5, our model attains best\\nperformance when using a non-linear pairwise function. We found 16 to 64 hidden units for the\\nnon-linear pairwise function to be sufﬁcient for modeling the bi-gram combinations in this dataset.\\nIn this case the largest model has 974,846 weights and training takes on average 0.068s per iteration.\\n3.2 I MAGE CLASSIFICATION : FLICKR\\nWe next evaluate the importance of blending learning and inference. Towards this goal, we make\\nuse of the Flickr dataset, which consists of 10;000training and 10;000test images from Flickr. The\\ntask is to predict which of 38 possible tags should be assigned to each image. Fig. 6 shows some\\nexamples. The graphical model has 38 binary random variables, each denoting the presence/absence\\nof a particular tag. We deﬁne the non-linear unaries fr(x;yi;wu)using the 8-layer deep-net architecture from Krizhevsky et al. (2013) followed by a 76-dimensional top layer. Hence the function',\n",
       " 'is composed out of two subsequent stacks of convolution, rectiﬁed linear (ReLU), pooling and local\\nresponse normalization units. Those are followed by three convolution–ReLU function pairs. Afterwards pooling is applied before two fully-connected–ReLU–dropout combinations are employed to\\nyield the input into a fully connected layer which ﬁnally computes the unary potentials. We employ\\npairwise potentials similar to Eq. (7) which now fully model the correlations between any pair of\\n8\\nAccepted as a workshop contribution at ICLR 2015\\noutput variables. This amounts to a total of 57;182;408parameters arising from the convolutional\\nunits, fully connected units and corresponding biases as well as the pairwise weights.\\nWe use a momentum based sub-gradient method for training with a mini-batch size of 300, a step\\nsize of 0:0001 , a momentum of 0:95and set\\x0f= 1 andcr= 18r. We initialized the deep-net\\nparameters using a model pre-trained on ImageNet (Deng et al., 2009). Our error metric is the\\nclassiﬁcation error ( i.e., Hamming loss).\\nJoint training helps: The mean error for unary only potentials (‘Unary only’), piecewise training',\n",
       " 'Joint training helps: The mean error for unary only potentials (‘Unary only’), piecewise training\\n(‘PwTrain’) and joint pretraining (‘PreTrainJoint’) is 9.36%, 7.70% and 7.25% respectively. Similar\\nto the Word50 dataset we observe that joint training is beneﬁcial. We provide examples for perfect\\n(two left-most images), roughly accurate and failing predictions (right image) in Fig. 6.\\nLearned pairwise weights: In Fig. 7(a) we illustrate the learned correlations for a subset of the 38\\nclasses. We observe that the class ‘people’ correlates highly with ‘female,’ ‘male,’ and ‘portrait.’\\nThe ‘indoor’ tag does not co-occur with ‘sky,’ ‘structures,’ ‘plant life’ and ‘tree.’ ‘Sea’ appears\\ntypically with ‘water,’ ‘clouds,’ ‘lake’ and ‘sky.’\\nEfﬁciency of Blending: To illustrate that blending is indeed beneﬁcial we compare the negative',\n",
       " 'Efﬁciency of Blending: To illustrate that blending is indeed beneﬁcial we compare the negative\\nlog-likelihood and the training error as a function of run-time in Fig. 7(b). The standard approach\\nis limited to 20 iterations of message passing to avoid time-consuming, repeated computation of a\\nstopping criterion involving both the approximated log-partition function and its dual. As show in\\nFig. 7(b) blending learning and inference speeds up parameter estimation signiﬁcantly. For larger\\ngraphical models, we expect the differences to be even more signiﬁcant.\\n4 D ISCUSSION & C ONCLUSION\\nJoint training of neural networks and graphical models: Neural Networks have been incorporated as unary potentials in graphical models. One of the earliest works by Bridle (1990) jointly optimizes a system consisting of multilayer perceptrons and hidden Markov models for speech recognition. For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks\\nto jointly optimize sub-tasks, such as word segmentation and character recognition. Several works\\n(Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar &',\n",
       " '(Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar &\\nFosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in\\nMRFs to incorporate non-linearities. However, they assume that exact inference can be performed\\neither via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is\\nhard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin\\nloss during training which is incorporated into our framework by choosing \\x0f= 0. More recently, Li\\n& Zemel (2014) use a hinge loss to learn the unary term deﬁned as a neural net, but keep the pairwise\\npotentials ﬁxed ( i.e., no joint training). Domke (2013) considers non-linear structured prediction and\\ndecomposes the learning problem into a subset of logistic regressors, which require the parameter\\nupdates to be run till convergence before updating the messages. Tompson et al. (2014) also jointly\\ntrain convolutional neural networks and a graphical model for pose estimation. However, the MRF',\n",
       " 'train convolutional neural networks and a graphical model for pose estimation. However, the MRF\\ninference procedure is approximated by their Spatial-Model which ignores the partition function.\\nBlending learning and inference: In this paper we deﬁned learning to be a min-max task. The\\nblending strategy, which was previously employed for learning log-linear models by (Meshi et al.,\\n2010; Hazan & Urtasun, 2010), amounts to converting the maximization task into a minimization\\nproblem using its dual. Subsequently we make use of block-coordinate descent strategies to obtain\\na more efﬁcient algorithm. Importantly any order of block-updates is possible. It remains an open\\nproblem to ﬁnd the optimal tradeoff.\\nWe have proposed an efﬁcient algorithm to learn deep models enriched to capture the dependencies\\nbetween the output variables. Our experiments on word prediction from noisy images and multiclass image classiﬁcation showed that the deeper and the more structured the model, the better the\\nperformance we achieve. Furthermore, joint learning of all weights outperforms all other strategies. In the future we plan to learn deeper models in applications such as holistic semantic scene\\nunderstanding. We will also extend our approach to deal with hidden variables.\\n9\\nAccepted as a workshop contribution at ICLR 2015\\nACKNOWLEDGMENTS',\n",
       " 'understanding. We will also extend our approach to deal with hidden variables.\\n9\\nAccepted as a workshop contribution at ICLR 2015\\nACKNOWLEDGMENTS\\nWe thank NVIDIA Corporation for the donation of GPUs used in this research. This work was\\npartially funded by ONR-N00014-14-1-0232.\\nREFERENCES\\nBengio, Y ., Lamblin, P., Popovici, D., and Larochelle, H. Greedy Layer-Wise Training of Deep Networks. In\\nProc. NIPS , 2007.\\nBottou, L., Bengio, Y ., and LeCun, Y . Global training of document processing systems using graph transformer\\nnetworks. In Proc. CVPR , 1997.\\nBridle, J. S. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In Proc. NIPS , 1990.\\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing\\n(almost) from scratch. JMLR , 2011.\\nde Campos, T. E., Babu, B. R., and Varma, M. Character recognition in natural images. In Proc. VISAPP , 2009.',\n",
       " 'de Campos, T. E., Babu, B. R., and Varma, M. Character recognition in natural images. In Proc. VISAPP , 2009.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image\\ndatabase. In Proc. CVPR , 2009.\\nDeng, J., Ding, N., Jia, Y ., Frome, A., Murphy, K., Bengio, S., Li, Y ., Neven, H., and Adam, H. Large-Scale\\nObject Classiﬁcation using Label Relation Graphs. In Proc. ECCV , 2014.\\nDo, T.-M.-T. and Artieres, T. Neural conditional random ﬁelds. In Proc. AISTATS , 2010.\\nDomke, J. Structured Learning via Logistic Regression. In Proc. NIPS , 2013.\\nEigen, D., Rolfe, J., Fergus, R., and LeCun, Y . Understanding Deep Architectures using a Recursive Convolutional Network. In Proc. ICLR , 2014.\\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and',\n",
       " 'Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and\\nsemantic segmentation. In Proc. CVPR , 2014.\\nHariharan, B., Arbel ´aez, P., Girshick, R., and Malik, J. Simultaneous detection and segmentation. In Proc.\\nECCV , 2014.\\nHazan, T. and Urtasun, R. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction. In Proc. NIPS , 2010.\\nHinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. Science ,\\n2006.\\nHinton, G. E., Sejnowski, T. J., and Ackley, D. H. Boltzmann Machines: Constraint Satisfaction Networks that\\nLearn. Technical report, University of Toronto, 1984.\\nJia, Y . Caffe: An Open Source Convolutional Architecture for Fast Feature Embedding. http://caffe.\\nberkeleyvision.org/ , 2013.\\nKoller, D. and Friedman, N. Probabilistic Graphical Models: Principles and Techniques . MIT Press, 2009.',\n",
       " 'berkeleyvision.org/ , 2013.\\nKoller, D. and Friedman, N. Probabilistic Graphical Models: Principles and Techniques . MIT Press, 2009.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet Classiﬁcation with Deep Convolutional Neural\\nNetworks. In Proc. NIPS , 2013.\\nLarochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y . An empirical evaluation of deep architectures on problems with many factors of variation. In Proc. ICML , 2007.\\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-based learning applied to document recognition.\\nProceedings of the IEEE , 1998.\\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Y . Convolutional deep belief networks for scalable unsupervised\\nlearning of hierarchical representations. In Proc. ICML , 2009.\\nLi, Y . and Zemel, R. High Order Regularization for Semi-Supervised Learning of Structured Output Problems.\\nInProc. ICML , 2014.\\n10\\nAccepted as a workshop contribution at ICLR 2015',\n",
       " 'InProc. ICML , 2014.\\n10\\nAccepted as a workshop contribution at ICLR 2015\\nMa, J., Peng, J., Wang, S., and Xu, J. A conditional neural ﬁelds model for protein threading. Bioinformatics ,\\n2012.\\nMeltzer, T., Globerson, A., and Weiss, Y . Convergent Message Passing Algorithms: a unifying view. In Proc.\\nUAI, 2009.\\nMeshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning Efﬁciently with Approximate inference via\\nDual Losses. In Proc. ICML , 2010.\\nMorris, J. and Fosler-Lussier, E. Conditional random ﬁelds for integrating local discriminative classiﬁers. IEEE\\nTrans. Audio, Speech, and Language Processing , 2008.\\nNowozin, S., Rother, C., Bagon, S., Sharp, T., Yao, B., and Kohli, P. Decision tree ﬁelds. In Proc. ICCV , 2011.\\nPearl, J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference . Morgan Kaufmann,\\n1988.',\n",
       " 'Pearl, J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference . Morgan Kaufmann,\\n1988.\\nPeng, J., Bo, L., and Xu, J. Conditional Neural Fields. In Proc. NIPS , 2009.\\nPrabhavalkar, R. and Fosler-Lussier, E. Backpropagation training for multilayer conditional random ﬁeld based\\nphone recognition. In Proc. ICASSP , 2010.\\nSalakhutdinov, R. R. and Hinton, G. E. An Efﬁcient Learning Procedure for Deep Boltzmann Machines. Neural\\nComputation , 2012.\\nSchwing, A. G. Inference and Learning Algorithms with Applications to 3D Indoor Scene Understanding . PhD\\nthesis, ETH Zurich, 2013.\\nSocher, R., Huval, B., Bhat, B., Manning, C. D., and Ng, A. Y . Convolutional-Recursive Deep Learning for 3D\\nObject Classiﬁcation. In Proc. NIPS , 2012.\\nTompson, J., Jain, A., LeCun, Y ., and Bregler, C. Joint Training of a Convolutional Network and a Graphical\\nModel for Human Pose Estimation. In Proc. NIPS , 2014.',\n",
       " 'Model for Human Pose Estimation. In Proc. NIPS , 2014.\\nWainwright, M. J. and Jordan, M. I. Graphical Models, Exponential Families and Variational Inference . Foundations and Trends in Machine Learning, 2008.\\nWainwright, M. J., Jaakkola, T., and Willsky, A. S. A new class of upper bounds on the log partition function.\\nTrans. Information Theory , 2005.\\nWeiss, Y ., Yanover, C., and Meltzer, T. MAP Estimation, Linear Programming and Belief Propagation with\\nConvex Free Energies. In Proc. UAI , 2007.\\nWiegerinck, W. and Heskes, T. Fractional belief propagation. In Proc. NIPS , 2003.\\nXu, J., Schwing, A. G., and Urtasun, R. Tell me what you see and I will show you where it is. In Proc. CVPR ,\\n2014.\\nYedidia, J. S., Freeman, W. T., and Weiss, Y . Constructing free-energy approximations and generalized belief\\npropagation algorithms. Trans. Information Theory , 2005.\\nZeiler, M. D. and Fergus, R. Visualizing and Understanding Convolutional Networks. In Proc. ECCV , 2014.\\n11',\n",
       " 'Published as a conference paper at ICLR 2015\\nNEURAL MACHINE TRANSLATION\\nBYJOINTLY LEARNING TO ALIGN AND TRANSLATE\\nDzmitry Bahdanau\\nJacobs University Bremen, Germany\\nKyungHyun Cho Yoshua Bengio\\x03\\nUniversit ´e de Montr ´eal\\nABSTRACT\\nNeural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine\\ntranslation aims at building a single neural network that can be jointly tuned to\\nmaximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode\\na source sentence into a ﬁxed-length vector from which a decoder generates a\\ntranslation. In this paper, we conjecture that the use of a ﬁxed-length vector is a\\nbottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search\\nfor parts of a source sentence that are relevant to predicting a target word, without\\nhaving to form these parts as a hard segment explicitly. With this new approach,\\nwe achieve a translation performance comparable to the existing state-of-the-art\\nphrase-based system on the task of English-to-French translation. Furthermore,',\n",
       " 'we achieve a translation performance comparable to the existing state-of-the-art\\nphrase-based system on the task of English-to-French translation. Furthermore,\\nqualitative analysis reveals that the (soft-)alignments found by the model agree\\nwell with our intuition.\\n1 I NTRODUCTION\\nNeural machine translation is a newly emerging approach to machine translation, recently proposed\\nby Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the\\ntraditional phrase-based translation system (see, e.g., Koehn et al. , 2003) which consists of many\\nsmall sub-components that are tuned separately, neural machine translation attempts to build and\\ntrain a single, large neural network that reads a sentence and outputs a correct translation.\\nMost of the proposed neural machine translation models belong to a family of encoder–\\ndecoders (Sutskever et al. , 2014; Cho et al. , 2014a), with an encoder and a decoder for each language, or involve a language-speciﬁc encoder applied to each sentence whose outputs are then compared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sentence into a ﬁxed-length vector. A decoder then outputs a translation from the encoded vector. The',\n",
       " 'whole encoder–decoder system, which consists of the encoder and the decoder for a language pair,\\nis jointly trained to maximize the probability of a correct translation given a source sentence.\\nA potential issue with this encoder–decoder approach is that a neural network needs to be able to\\ncompress all the necessary information of a source sentence into a ﬁxed-length vector. This may\\nmake it difﬁcult for the neural network to cope with long sentences, especially those that are longer\\nthan the sentences in the training corpus. Cho et al. (2014b) showed that indeed the performance of\\na basic encoder–decoder deteriorates rapidly as the length of an input sentence increases.\\nIn order to address this issue, we introduce an extension to the encoder–decoder model which learns\\nto align and translate jointly. Each time the proposed model generates a word in a translation, it\\n(soft-)searches for a set of positions in a source sentence where the most relevant information is\\nconcentrated. The model then predicts a target word based on the context vectors associated with\\nthese source positions and all the previous generated target words.\\n\\x03CIFAR Senior Fellow\\n1arXiv:1409.0473v7  [cs.CL]  19 May 2016\\nPublished as a conference paper at ICLR 2015',\n",
       " '\\x03CIFAR Senior Fellow\\n1arXiv:1409.0473v7  [cs.CL]  19 May 2016\\nPublished as a conference paper at ICLR 2015\\nThe most important distinguishing feature of this approach from the basic encoder–decoder is that\\nit does not attempt to encode a whole input sentence into a single ﬁxed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively\\nwhile decoding the translation. This frees a neural translation model from having to squash all the\\ninformation of a source sentence, regardless of its length, into a ﬁxed-length vector. We show this\\nallows a model to cope better with long sentences.\\nIn this paper, we show that the proposed approach of jointly learning to align and translate achieves\\nsigniﬁcantly improved translation performance over the basic encoder–decoder approach. The improvement is more apparent with longer sentences, but can be observed with sentences of any\\nlength. On the task of English-to-French translation, the proposed approach achieves, with a single\\nmodel, a translation performance comparable, or close, to the conventional phrase-based system.\\nFurthermore, qualitative analysis reveals that the proposed model ﬁnds a linguistically plausible\\n(soft-)alignment between a source sentence and the corresponding target sentence.',\n",
       " 'Furthermore, qualitative analysis reveals that the proposed model ﬁnds a linguistically plausible\\n(soft-)alignment between a source sentence and the corresponding target sentence.\\n2 B ACKGROUND : NEURAL MACHINE TRANSLATION\\nFrom a probabilistic perspective, translation is equivalent to ﬁnding a target sentence ythat maximizes the conditional probability of ygiven a source sentence x, i.e., arg maxyp(yjx). In\\nneural machine translation, we ﬁt a parameterized model to maximize the conditional probability\\nof sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a\\ntranslation model, given a source sentence a corresponding translation can be generated by searching\\nfor the sentence that maximizes the conditional probability.\\nRecently, a number of papers have proposed the use of neural networks to directly learn this conditional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al. , 2014a; Sutskever et al. ,\\n2014; Cho et al. , 2014b; Forcada and ˜Neco, 1997). This neural machine translation approach typically consists of two components, the ﬁrst of which encodes a source sentence xand the second\\ndecodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by',\n",
       " 'decodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by\\n(Cho et al. , 2014a) and (Sutskever et al. , 2014) to encode a variable-length source sentence into a\\nﬁxed-length vector and to decode the vector into a variable-length target sentence.\\nDespite being a quite new approach, neural machine translation has already shown promising results.\\nSutskever et al. (2014) reported that the neural machine translation based on RNNs with long shortterm memory (LSTM) units achieves close to the state-of-the-art performance of the conventional\\nphrase-based machine translation system on an English-to-French translation task.1Adding neural\\ncomponents to existing translation systems, for instance, to score the phrase pairs in the phrase\\ntable (Cho et al. , 2014a) or to re-rank candidate translations (Sutskever et al. , 2014), has allowed to\\nsurpass the previous state-of-the-art performance level.\\n2.1 RNN E NCODER –DECODER\\nHere, we describe brieﬂy the underlying framework, called RNN Encoder–Decoder , proposed by\\nCho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns',\n",
       " 'Cho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns\\nto align and translate simultaneously.\\nIn the Encoder–Decoder framework, an encoder reads the input sentence, a sequence of vectors\\nx= (x1;\\x01\\x01\\x01;xTx), into a vector c.2The most common approach is to use an RNN such that\\nht=f(xt;ht\\x001) (1)\\nand\\nc=q(fh1;\\x01\\x01\\x01;hTxg);\\nwhereht2Rnis a hidden state at time t, andcis a vector generated from the sequence of the\\nhidden states. fandqare some nonlinear functions. Sutskever et al. (2014) used an LSTM as fand\\nq(fh1;\\x01\\x01\\x01;hTg) =hT, for instance.\\n1We mean by the state-of-the-art performance, the performance of the conventional phrase-based system\\nwithout using any neural network-based component.\\n2Although most of the previous works (see, e.g., Cho et al. , 2014a; Sutskever et al. , 2014; Kalchbrenner and',\n",
       " '2Although most of the previous works (see, e.g., Cho et al. , 2014a; Sutskever et al. , 2014; Kalchbrenner and\\nBlunsom, 2013) used to encode a variable-length input sentence into a ﬁxed-length vector, it is not necessary,\\nand even it may be beneﬁcial to have a variable-length vector, as we will show later.\\n2\\nPublished as a conference paper at ICLR 2015\\nThe decoder is often trained to predict the next word yt0given the context vector cand all the\\npreviously predicted words fy1;\\x01\\x01\\x01;yt0\\x001g. In other words, the decoder deﬁnes a probability over\\nthe translation yby decomposing the joint probability into the ordered conditionals:\\np(y) =TY\\nt=1p(ytjfy1;\\x01\\x01\\x01;yt\\x001g;c); (2)\\nwhere y=\\x00\\ny1;\\x01\\x01\\x01;yTy\\x01\\n. With an RNN, each conditional probability is modeled as\\np(ytjfy1;\\x01\\x01\\x01;yt\\x001g;c) =g(yt\\x001;st;c); (3)\\nwheregis a nonlinear, potentially multi-layered, function that outputs the probability of yt, andstis',\n",
       " 'wheregis a nonlinear, potentially multi-layered, function that outputs the probability of yt, andstis\\nthe hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN\\nand a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).\\n3 L EARNING TO ALIGN AND TRANSLATE\\nIn this section, we propose a novel architecture for neural machine translation. The new architecture\\nconsists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching\\nthrough a source sentence during decoding a translation (Sec. 3.1).\\n3.1 D ECODER : GENERAL DESCRIPTION\\nx1x2x3xT+\\nαt,1\\nαt,2 αt,3αt,Tyt-1yt\\nh1h2h3 hTh1h2h3 hTst-1st\\nFigure 1: The graphical illustration of the proposed model\\ntrying to generate the t-th target wordytgiven a source\\nsentence (x1;x2;:::;x T).In a new model architecture, we deﬁne each conditional probability\\nin Eq. (2) as:',\n",
       " 'sentence (x1;x2;:::;x T).In a new model architecture, we deﬁne each conditional probability\\nin Eq. (2) as:\\np(yijy1;:::;y i\\x001;x) =g(yi\\x001;si;ci); (4)\\nwheresiis an RNN hidden state for time i, computed by\\nsi=f(si\\x001;yi\\x001;ci):\\nIt should be noted that unlike the existing encoder–decoder approach (see Eq. (2)), here the probability is conditioned on a distinct\\ncontext vector cifor each target word yi.\\nThe context vector cidepends on a sequence of annotations\\n(h1;\\x01\\x01\\x01;hTx)to which an encoder maps the input sentence. Each\\nannotationhicontains information about the whole input sequence\\nwith a strong focus on the parts surrounding the i-th word of the\\ninput sequence. We explain in detail how the annotations are computed in the next section.\\nThe context vector ciis, then, computed as a weighted sum of these\\nannotationshi:\\nci=TxX\\nj=1\\x0bijhj: (5)\\nThe weight\\x0bijof each annotation hjis computed by\\n\\x0bij=exp (eij)PTx',\n",
       " 'ci=TxX\\nj=1\\x0bijhj: (5)\\nThe weight\\x0bijof each annotation hjis computed by\\n\\x0bij=exp (eij)PTx\\nk=1exp (eik); (6)\\nwhere\\neij=a(si\\x001;hj)\\nis an alignment model which scores how well the inputs around position jand the output at position\\nimatch. The score is based on the RNN hidden state si\\x001(just before emitting yi, Eq. (4)) and the\\nj-th annotation hjof the input sentence.\\nWe parametrize the alignment model aas a feedforward neural network which is jointly trained with\\nall the other components of the proposed system. Note that unlike in traditional machine translation,\\n3\\nPublished as a conference paper at ICLR 2015\\nthe alignment is not considered to be a latent variable. Instead, the alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through.\\nThis gradient can be used to train the alignment model as well as the whole translation model jointly.\\nWe can understand the approach of taking a weighted sum of all the annotations as computing an\\nexpected annotation , where the expectation is over possible alignments. Let \\x0bijbe a probability that',\n",
       " 'We can understand the approach of taking a weighted sum of all the annotations as computing an\\nexpected annotation , where the expectation is over possible alignments. Let \\x0bijbe a probability that\\nthe target word yiis aligned to, or translated from, a source word xj. Then, thei-th context vector\\nciis the expected annotation over all the annotations with probabilities \\x0bij.\\nThe probability \\x0bij, or its associated energy eij, reﬂects the importance of the annotation hjwith\\nrespect to the previous hidden state si\\x001in deciding the next state siand generating yi. Intuitively,\\nthis implements a mechanism of attention in the decoder. The decoder decides parts of the source\\nsentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the\\nencoder from the burden of having to encode all information in the source sentence into a ﬁxedlength vector. With this new approach the information can be spread throughout the sequence of\\nannotations, which can be selectively retrieved by the decoder accordingly.\\n3.2 E NCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES\\nThe usual RNN, described in Eq. (1), reads an input sequence xin order starting from the ﬁrst',\n",
       " 'The usual RNN, described in Eq. (1), reads an input sequence xin order starting from the ﬁrst\\nsymbolx1to the last one xTx. However, in the proposed scheme, we would like the annotation\\nof each word to summarize not only the preceding words, but also the following words. Hence,\\nwe propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been\\nsuccessfully used recently in speech recognition (see, e.g., Graves et al. , 2013).\\nA BiRNN consists of forward and backward RNN’s. The forward RNN\\x00 !freads the input sequence\\nas it is ordered (from x1toxTx) and calculates a sequence of forward hidden states (\\x00 !h1;\\x01\\x01\\x01;\\x00 !hTx).\\nThe backward RNN \\x00freads the sequence in the reverse order (from xTxtox1), resulting in a\\nsequence of backward hidden states ( \\x00h1;\\x01\\x01\\x01; \\x00hTx).\\nWe obtain an annotation for each word xjby concatenating the forward hidden state\\x00 !hjand the\\nbackward one \\x00hj, i.e.,hj=h\\x00 !h>\\nj; \\x00h>\\nji>',\n",
       " 'backward one \\x00hj, i.e.,hj=h\\x00 !h>\\nj; \\x00h>\\nji>\\n. In this way, the annotation hjcontains the summaries\\nof both the preceding words and the following words. Due to the tendency of RNNs to better\\nrepresent recent inputs, the annotation hjwill be focused on the words around xj. This sequence\\nof annotations is used by the decoder and the alignment model later to compute the context vector\\n(Eqs. (5)–(6)).\\nSee Fig. 1 for the graphical illustration of the proposed model.\\n4 E XPERIMENT SETTINGS\\nWe evaluate the proposed approach on the task of English-to-French translation. We use the bilingual, parallel corpora provided by ACL WMT ’14.3As a comparison, we also report the performance of an RNN Encoder–Decoder which was proposed recently by Cho et al. (2014a). We use\\nthe same training procedures and the same dataset for both models.4\\n4.1 D ATASET\\nWMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news\\ncommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,',\n",
       " 'commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,\\ntotaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of\\nthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5\\nWe do not use any monolingual data other than the mentioned parallel corpora, although it may be\\npossible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test3http://www.statmt.org/wmt14/translation-task.html\\n4Implementations are available at https://github.com/lisa-groundhog/GroundHog .\\n5Available online at http://www-lium.univ-lemans.fr/ ˜schwenk/cslm_joint_paper/ .\\n4\\nPublished as a conference paper at ICLR 2015\\n0 10 20 30 40 50 60\\nSentence length051015202530BLEU score RNNsearch-50\\nRNNsearch-30\\nRNNenc-50\\nRNNenc-30\\nFigure 2: The BLEU scores\\nof the generated translations\\non the test set with respect\\nto the lengths of the sentences. The results are on',\n",
       " 'RNNenc-30\\nFigure 2: The BLEU scores\\nof the generated translations\\non the test set with respect\\nto the lengths of the sentences. The results are on\\nthe full test set which includes sentences having unknown words to the models.\\n2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test\\nset (news-test-2014) from WMT ’14, which consists of 3003 sentences not present in the training\\ndata.\\nAfter a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to\\ntrain our models. Any word not included in the shortlist is mapped to a special token ( [UNK ]). We\\ndo not apply any other special preprocessing, such as lowercasing or stemming, to the data.\\n4.2 M ODELS\\nWe train two types of models. The ﬁrst one is an RNN Encoder–Decoder (RNNencdec, Cho et al. ,\\n2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model\\ntwice: ﬁrst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then',\n",
       " 'twice: ﬁrst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then\\nwith the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).\\nThe encoder and decoder of the RNNencdec have 1000 hidden units each.7The encoder of the\\nRNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000\\nhidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a\\nsingle maxout (Goodfellow et al. , 2013) hidden layer to compute the conditional probability of each\\ntarget word (Pascanu et al. , 2014).\\nWe use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler,\\n2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sentences. We trained each model for approximately 5 days.\\nOnce a model is trained, we use a beam search to ﬁnd a translation that approximately maximizes the\\nconditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al. , 2013). Sutskever\\net al. (2014) used this approach to generate translations from their neural machine translation model.',\n",
       " \"et al. (2014) used this approach to generate translations from their neural machine translation model.\\nFor more details on the architectures of the models and training procedure used in the experiments,\\nsee Appendices A and B.\\n5 R ESULTS\\n5.1 Q UANTITATIVE RESULTS\\nIn Table 1, we list the translation performances measured in BLEU score. It is clear from the table\\nthat in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More\\nimportantly, the performance of the RNNsearch is as high as that of the conventional phrase-based\\ntranslation system (Moses), when only the sentences consisting of known words are considered.\\nThis is a signiﬁcant achievement, considering that Moses uses a separate monolingual corpus (418M\\nwords) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.\\n6We used the tokenization script from the open-source machine translation package, Moses.\\n7In this paper, by a ’hidden unit’, we always mean the gated hidden unit (see Appendix A.1.1).\\n5\\nPublished as a conference paper at ICLR 2015\\nThe\\nagreement\\non\\nthe\\nEuropean\\nEconomic\\nArea\\nwas\\nsigned\\nin\\nAugust\\n1992\\n.\\n<end>\\nL'\\naccord\",\n",
       " 'The\\nagreement\\non\\nthe\\nEuropean\\nEconomic\\nArea\\nwas\\nsigned\\nin\\nAugust\\n1992\\n.\\n<end>\\nL\\'\\naccord\\nsur\\nla\\nzone\\néconomique\\neuropéenne\\na\\nété\\nsigné\\nen\\naoût\\n1992\\n.\\n<end>\\nIt\\nshould\\nbe\\nnoted\\nthat\\nthe\\nmarine\\nenvironment\\nis\\nthe\\nleast\\nknown\\nof\\nenvironments\\n.\\n<end>\\nIl\\nconvient\\nde\\nnoter\\nque\\nl\\'\\nenvironnement\\nmarin\\nest\\nle\\nmoins\\nconnu\\nde\\nl\\'\\nenvironnement\\n.\\n<end>\\n(a) (b)\\nDestruction\\nof\\nthe\\nequipment\\nmeans\\nthat\\nSyria\\ncan\\nno\\nlonger\\nproduce\\nnew\\nchemical\\nweapons\\n.\\n<end>\\nLa\\ndestruction\\nde\\nl\\'\\néquipement\\nsignifie\\nque\\nla\\nSyrie\\nne\\npeut\\nplus\\nproduire\\nde\\nnouvelles\\narmes\\nchimiques\\n.\\n<end>\\n\"\\nThis\\nwill\\nchange\\nmy\\nfuture\\nwith\\nmy\\nfamily\\n,\\n\"\\nthe\\nman\\nsaid\\n.\\n<end>\\n\"\\nCela',\n",
       " '\"\\nThis\\nwill\\nchange\\nmy\\nfuture\\nwith\\nmy\\nfamily\\n,\\n\"\\nthe\\nman\\nsaid\\n.\\n<end>\\n\"\\nCela\\nva\\nchanger\\nmon\\navenir\\navec\\nma\\nfamille\\n\"\\n,\\na\\ndit\\nl\\'\\nhomme\\n.\\n<end>\\n(c) (d)\\nFigure 3: Four sample alignments found by RNNsearch-50. The x-axis and y-axis of each plot\\ncorrespond to the words in the source sentence (English) and the generated translation (French),\\nrespectively. Each pixel shows the weight \\x0bijof the annotation of the j-th source word for the i-th\\ntarget word (see Eq. (6)), in grayscale ( 0: black, 1: white). (a) an arbitrary sentence. (b–d) three\\nrandomly selected samples among the sentences without any unknown words and of length between\\n10 and 20 words from the test set.\\nOne of the motivations behind the proposed approach was the use of a ﬁxed-length context vector\\nin the basic encoder–decoder approach. We conjectured that this limitation may make the basic',\n",
       " 'in the basic encoder–decoder approach. We conjectured that this limitation may make the basic\\nencoder–decoder approach to underperform with long sentences. In Fig. 2, we see that the performance of RNNencdec dramatically drops as the length of the sentences increases. On the other hand,\\nboth RNNsearch-30 and RNNsearch-50 are more robust to the length of the sentences. RNNsearch50, especially, shows no performance deterioration even with sentences of length 50 or more. This\\nsuperiority of the proposed model over the basic encoder–decoder is further conﬁrmed by the fact\\nthat the RNNsearch-30 even outperforms RNNencdec-50 (see Table 1).\\n6\\nPublished as a conference paper at ICLR 2015\\nModel All No UNK\\x0e\\nRNNencdec-30 13.93 24.19\\nRNNsearch-30 21.50 31.44\\nRNNencdec-50 17.82 26.71\\nRNNsearch-50 26.75 34.16\\nRNNsearch-50?28.45 36.15\\nMoses 33.30 35.63Table 1: BLEU scores of the trained models computed on the test set. The second and third columns\\nshow respectively the scores on all the sentences and,',\n",
       " 'show respectively the scores on all the sentences and,\\non the sentences without any unknown word in themselves and in the reference translations. Note that\\nRNNsearch-50?was trained much longer until the\\nperformance on the development set stopped improving. (\\x0e) We disallowed the models to generate [UNK]\\ntokens when only the sentences having no unknown\\nwords were evaluated (last column).\\n5.2 Q UALITATIVE ANALYSIS\\n5.2.1 A LIGNMENT\\nThe proposed approach provides an intuitive way to inspect the (soft-)alignment between the words\\nin a generated translation and those in a source sentence. This is done by visualizing the annotation\\nweights\\x0bijfrom Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights\\nassociated with the annotations. From this we see which positions in the source sentence were\\nconsidered more important when generating the target word.\\nWe can see from the alignments in Fig. 3 that the alignment of words between English and French\\nis largely monotonic. We see strong weights along the diagonal of each matrix. However, we also\\nobserve a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically\\nordered differently between French and English, and we see an example in Fig. 3 (a). From this',\n",
       " 'ordered differently between French and English, and we see an example in Fig. 3 (a). From this\\nﬁgure, we see that the model correctly translates a phrase [European Economic Area] into [zone\\n´economique europ ´een]. The RNNsearch was able to correctly align [zone] with [Area], jumping\\nover the two words ([European] and [Economic]), and then looked one word back at a time to\\ncomplete the whole phrase [zone ´economique europ ´eenne].\\nThe strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from\\nFig. 3 (d). Consider the source phrase [the man] which was translated into [l’ homme]. Any hard\\nalignment will map [the] to [l’] and [man] to [homme]. This is not helpful for translation, as one\\nmust consider the word following [the] to determine whether it should be translated into [le], [la],\\n[les] or [l’]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and\\n[man], and in this example, we see that the model was able to correctly translate [the] into [l’]. We',\n",
       " '[man], and in this example, we see that the model was able to correctly translate [the] into [l’]. We\\nobserve similar behaviors in all the presented cases in Fig. 3. An additional beneﬁt of the soft alignment is that it naturally deals with source and target phrases of different lengths, without requiring a\\ncounter-intuitive way of mapping some words to or from nowhere ([NULL]) (see, e.g., Chapters 4\\nand 5 of Koehn, 2010).\\n5.2.2 L ONG SENTENCES\\nAs clearly visible from Fig. 2 the proposed model (RNNsearch) is much better than the conventional\\nmodel (RNNencdec) at translating long sentences. This is likely due to the fact that the RNNsearch\\ndoes not require encoding a long sentence into a ﬁxed-length vector perfectly, but only accurately\\nencoding the parts of the input sentence that surround a particular word.\\nAs an example, consider this source sentence from the test set:\\nAn admitting privilege is the right of a doctor to admit a patient to a hospital or\\na medical centre tocarry outadiagnosis oraprocedure, based onhisstatus asa\\nhealth care worker atahospital.\\nThe RNNencdec-50 translated this sentence into:',\n",
       " 'health care worker atahospital.\\nThe RNNencdec-50 translated this sentence into:\\nUn privil `ege d’admission est le droit d’un m ´edecin de reconna ˆıtre un patient `a\\nl’hˆopital ou un centre m ´edical d’un diagnostic oudeprendre undiagnostic en\\nfonction deson´etatdesant´e.\\n7\\nPublished as a conference paper at ICLR 2015\\nThe RNNencdec-50 correctly translated the source sentence until [a medical center]. However, from\\nthere on (underlined), it deviated from the original meaning of the source sentence. For instance, it\\nreplaced [based on his status as a health care worker at a hospital] in the source sentence with [en\\nfonction de son ´etat de sant ´e] (“based on his state of health”).\\nOn the other hand, the RNNsearch-50 generated the following correct translation, preserving the\\nwhole meaning of the input sentence without omitting any details:\\nUn privil `ege d’admission est le droit d’un m ´edecin d’admettre un patient `a un',\n",
       " 'Un privil `ege d’admission est le droit d’un m ´edecin d’admettre un patient `a un\\nhˆopital ou un centre m ´edical pour effectuer undiagnostic ouuneproc´edure, selon\\nsonstatut detravailleur dessoins desant´e`al’hˆopital.\\nLet us consider another sentence from the test set:\\nThis kind of experience is part of Disney’s efforts to ”extend the lifetime of its\\nseries and build new relationships with audiences viadigital platforms that are\\nbecoming ever more important, ” headded.\\nThe translation by the RNNencdec-50 is\\nCe type d’exp ´erience fait partie des initiatives du Disney pour ”prolonger la dur ´ee\\nde vie de ses nouvelles et de d ´evelopper des liens avec les lecteurs num´eriques qui\\ndeviennent plus complexes.\\nAs with the previous example, the RNNencdec began deviating from the actual meaning of the\\nsource sentence after generating approximately 30 words (see the underlined phrase). After that',\n",
       " 'As with the previous example, the RNNencdec began deviating from the actual meaning of the\\nsource sentence after generating approximately 30 words (see the underlined phrase). After that\\npoint, the quality of the translation deteriorates, with basic mistakes such as the lack of a closing\\nquotation mark.\\nAgain, the RNNsearch-50 was able to translate this long sentence correctly:\\nCe genre d’exp ´erience fait partie des efforts de Disney pour ”prolonger la dur ´ee\\nde vie de ses s ´eries et cr ´eer de nouvelles relations avec des publics viades\\nplateformes num´eriques deplus enplus importantes”, a-t-il ajout ´e.\\nIn conjunction with the quantitative results presented already, these qualitative observations conﬁrm our hypotheses that the RNNsearch architecture enables far more reliable translation of long\\nsentences than the standard RNNencdec model.\\nIn Appendix C, we provide a few more sample translations of long source sentences generated by\\nthe RNNencdec-50, RNNsearch-50 and Google Translate along with the reference translations.\\n6 R ELATED WORK\\n6.1 L EARNING TO ALIGN\\nA similar approach of aligning an output symbol with an input symbol was proposed recently by',\n",
       " '6 R ELATED WORK\\n6.1 L EARNING TO ALIGN\\nA similar approach of aligning an output symbol with an input symbol was proposed recently by\\nGraves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the\\nmodel is asked to generate handwriting of a given sequence of characters. In his work, he used a\\nmixture of Gaussian kernels to compute the weights of the annotations, where the location, width\\nand mixture coefﬁcient of each kernel was predicted from an alignment model. More speciﬁcally,\\nhis alignment was restricted to predict the location such that the location increases monotonically.\\nThe main difference from our approach is that, in (Graves, 2013), the modes of the weights of the\\nannotations only move in one direction. In the context of machine translation, this is a severe limitation, as (long-distance) reordering is often needed to generate a grammatically correct translation\\n(for instance, English-to-German).\\nOur approach, on the other hand, requires computing the annotation weight of every word in the\\nsource sentence for each word in the translation. This drawback is not severe with the task of\\ntranslation in which most of input and output sentences are only 15–40 words. However, this may\\nlimit the applicability of the proposed scheme to other tasks.\\n8',\n",
       " 'translation in which most of input and output sentences are only 15–40 words. However, this may\\nlimit the applicability of the proposed scheme to other tasks.\\n8\\nPublished as a conference paper at ICLR 2015\\n6.2 N EURAL NETWORKS FOR MACHINE TRANSLATION\\nSince Bengio et al. (2003) introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a ﬁxed number of the preceding words,\\nneural networks have widely been used in machine translation. However, the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine\\ntranslation system or to re-rank a list of candidate translations provided by an existing system.\\nFor instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of\\na pair of source and target phrases and to use the score as an additional feature in the phrase-based\\nstatistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin\\net al. (2014) reported the successful use of the neural networks as a sub-component of the existing\\ntranslation system. Traditionally, a neural network trained as a target-side language model has been\\nused to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al. , 2006).',\n",
       " 'used to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al. , 2006).\\nAlthough the above approaches were shown to improve the translation performance over the stateof-the-art machine translation systems, we are more interested in a more ambitious objective of\\ndesigning a completely new translation system based on neural networks. The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works.\\nRather than using a neural network as a part of the existing system, our model works on its own and\\ngenerates a translation from a source sentence directly.\\n7 C ONCLUSION\\nThe conventional approach to neural machine translation, called an encoder–decoder approach, encodes a whole input sentence into a ﬁxed-length vector from which a translation will be decoded.\\nWe conjectured that the use of a ﬁxed-length context vector is problematic for translating long sentences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al.\\n(2014).\\nIn this paper, we proposed a novel architecture that addresses this issue. We extended the basic\\nencoder–decoder by letting a model (soft-)search for a set of input words, or their annotations computed by an encoder, when generating each target word. This frees the model from having to encode',\n",
       " 'a whole source sentence into a ﬁxed-length vector, and also lets the model focus only on information\\nrelevant to the generation of the next target word. This has a major positive impact on the ability\\nof the neural machine translation system to yield good results on longer sentences. Unlike with\\nthe traditional machine translation systems, all of the pieces of the translation system, including\\nthe alignment mechanism, are jointly trained towards a better log-probability of producing correct\\ntranslations.\\nWe tested the proposed model, called RNNsearch, on the task of English-to-French translation. The\\nexperiment revealed that the proposed RNNsearch outperforms the conventional encoder–decoder\\nmodel (RNNencdec) signiﬁcantly, regardless of the sentence length and that it is much more robust to the length of a source sentence. From the qualitative analysis where we investigated the\\n(soft-)alignment generated by the RNNsearch, we were able to conclude that the model can correctly align each target word with the relevant words, or their annotations, in the source sentence as\\nit generated a correct translation.\\nPerhaps more importantly, the proposed approach achieved a translation performance comparable to\\nthe existing phrase-based statistical machine translation. It is a striking result, considering that the\\nproposed architecture, or the whole family of neural machine translation, has only been proposed',\n",
       " 'the existing phrase-based statistical machine translation. It is a striking result, considering that the\\nproposed architecture, or the whole family of neural machine translation, has only been proposed\\nas recently as this year. We believe the architecture proposed here is a promising step toward better\\nmachine translation and a better understanding of natural languages in general.\\nOne of challenges left for the future is to better handle unknown, or rare words. This will be required\\nfor the model to be more widely used and to match the performance of current state-of-the-art\\nmachine translation systems in all contexts.\\n9\\nPublished as a conference paper at ICLR 2015\\nACKNOWLEDGMENTS\\nThe authors would like to thank the developers of Theano (Bergstra et al. , 2010; Bastien et al. ,\\n2012). We acknowledge the support of the following agencies for research funding and computing\\nsupport: NSERC, Calcul Qu ´ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bahdanau thanks the support from Planet Intelligent Systems GmbH. We also thank Felix Hill, Bart van\\nMerri ´enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim.\\nREFERENCES\\nAxelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection.',\n",
       " 'REFERENCES\\nAxelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection.\\nInProceedings of the ACL Conference on Empirical Methods in Natural Language Processing\\n(EMNLP) , pages 355–362. Association for Computational Linguistics.\\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N.,\\nand Bengio, Y . (2012). Theano: new features and speed improvements. Deep Learning and\\nUnsupervised Feature Learning NIPS 2012 Workshop.\\nBengio, Y ., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient\\ndescent is difﬁcult. IEEE Transactions on Neural Networks ,5(2), 157–166.\\nBengio, Y ., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model.\\nJ. Mach. Learn. Res. ,3, 1137–1155.',\n",
       " 'J. Mach. Learn. Res. ,3, 1137–1155.\\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., WardeFarley, D., and Bengio, Y . (2010). Theano: a CPU and GPU math expression compiler. In\\nProceedings of the Python for Scientiﬁc Computing Conference (SciPy) . Oral Presentation.\\nBoulanger-Lewandowski, N., Bengio, Y ., and Vincent, P. (2013). Audio chord recognition with\\nrecurrent neural networks. In ISMIR .\\nCho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y . (2014a).\\nLearning phrase representations using RNN encoder-decoder for statistical machine translation.\\nInProceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) . to\\nappear.\\nCho, K., van Merri ¨enboer, B., Bahdanau, D., and Bengio, Y . (2014b). On the properties of neural\\nmachine translation: Encoder–Decoder approaches. In Eighth Workshop on Syntax, Semantics',\n",
       " 'machine translation: Encoder–Decoder approaches. In Eighth Workshop on Syntax, Semantics\\nand Structure in Statistical Translation . to appear.\\nDevlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust\\nneural network joint models for statistical machine translation. In Association for Computational\\nLinguistics .\\nForcada, M. L. and ˜Neco, R. P. (1997). Recursive hetero-associative memories for translation. In\\nJ. Mira, R. Moreno-D ´ıaz, and J. Cabestany, editors, Biological and Artiﬁcial Computation: From\\nNeuroscience to Technology , volume 1240 of Lecture Notes in Computer Science , pages 453–462.\\nSpringer Berlin Heidelberg.\\nGoodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013). Maxout networks. In Proceedings of The 30th International Conference on Machine Learning , pages 1319–\\n1327.\\nGraves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\\n29th International Conference on Machine Learning (ICML 2012) .',\n",
       " '1327.\\nGraves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\\n29th International Conference on Machine Learning (ICML 2012) .\\nGraves, A. (2013). Generating sequences with recurrent neural networks. arXiv: 1308.0850\\n[cs.NE] .\\nGraves, A., Jaitly, N., and Mohamed, A.-R. (2013). Hybrid speech recognition with deep bidirectional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on , pages 273–278.\\n10\\nPublished as a conference paper at ICLR 2015\\nHermann, K. and Blunsom, P. (2014). Multilingual distributed representations without word alignment. In Proceedings of the Second International Conference on Learning Representations (ICLR\\n2014) .\\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut\\nf¨ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit ¨at M ¨unchen.\\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation ,9(8),\\n1735–1780.',\n",
       " 'Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation ,9(8),\\n1735–1780.\\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Proceedings\\nof the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages\\n1700–1709. Association for Computational Linguistics.\\nKoehn, P. (2010). Statistical Machine Translation . Cambridge University Press, New York, NY ,\\nUSA.\\nKoehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase-based translation. In Proceedings\\nof the 2003 Conference of the North American Chapter of the Association for Computational\\nLinguistics on Human Language Technology - Volume 1 , NAACL ’03, pages 48–54, Stroudsburg,\\nPA, USA. Association for Computational Linguistics.\\nPascanu, R., Mikolov, T., and Bengio, Y . (2013a). On the difﬁculty of training recurrent neural\\nnetworks. In ICML’2013 .\\nPascanu, R., Mikolov, T., and Bengio, Y . (2013b). On the difﬁculty of training recurrent neural',\n",
       " 'Pascanu, R., Mikolov, T., and Bengio, Y . (2013b). On the difﬁculty of training recurrent neural\\nnetworks. In Proceedings of the 30th International Conference on Machine Learning (ICML\\n2013) .\\nPascanu, R., Gulcehre, C., Cho, K., and Bengio, Y . (2014). How to construct deep recurrent neural\\nnetworks. In Proceedings of the Second International Conference on Learning Representations\\n(ICLR 2014) .\\nPouget-Abadie, J., Bahdanau, D., van Merri ¨enboer, B., Cho, K., and Bengio, Y . (2014). Overcoming\\nthe curse of sentence length for neural machine translation using automatic segmentation. In\\nEighth Workshop on Syntax, Semantics and Structure in Statistical Translation . to appear.\\nSchuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. Signal Processing,\\nIEEE Transactions on ,45(11), 2673–2681.\\nSchwenk, H. (2012). Continuous space translation models for phrase-based statistical machine\\ntranslation. In M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on',\n",
       " 'translation. In M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on\\nComputational Linguistics (COLIN) , pages 1071–1080. Indian Institute of Technology Bombay.\\nSchwenk, H., Dchelotte, D., and Gauvain, J.-L. (2006). Continuous space language models for\\nstatistical machine translation. In Proceedings of the COLING/ACL on Main conference poster\\nsessions , pages 723–730. Association for Computational Linguistics.\\nSutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks.\\nInAdvances in Neural Information Processing Systems (NIPS 2014) .\\nZeiler, M. D. (2012). ADADELTA: An adaptive learning rate method. arXiv: 1212.5701\\n[cs.LG] .\\n11\\nPublished as a conference paper at ICLR 2015\\nA M ODEL ARCHITECTURE\\nA.1 A RCHITECTURAL CHOICES\\nThe proposed scheme in Section 3 is a general framework where one can freely deﬁne, for instance,\\nthe activation functions fof recurrent neural networks (RNN) and the alignment model a. Here, we\\ndescribe the choices we made for the experiments in this paper.',\n",
       " 'the activation functions fof recurrent neural networks (RNN) and the alignment model a. Here, we\\ndescribe the choices we made for the experiments in this paper.\\nA.1.1 R ECURRENT NEURAL NETWORK\\nFor the activation function fof an RNN, we use the gated hidden unit recently proposed by Cho\\net al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an\\nelement-wise tanh . This gated unit is similar to a long short-term memory (LSTM) unit proposed\\nearlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn\\nlong-term dependencies. This is made possible by having computation paths in the unfolded RNN\\nfor which the product of derivatives is close to 1. These paths allow gradients to ﬂow backward\\neasily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al. , 1994;\\nPascanu et al. , 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit\\ndescribed here, as was done in a similar context by Sutskever et al. (2014).\\nThe new state siof the RNN employing ngated hidden units8is computed by',\n",
       " 'described here, as was done in a similar context by Sutskever et al. (2014).\\nThe new state siof the RNN employing ngated hidden units8is computed by\\nsi=f(si\\x001;yi\\x001;ci) = (1\\x00zi)\\x0esi\\x001+zi\\x0e~si;\\nwhere\\x0eis an element-wise multiplication, and ziis the output of the update gates (see below). The\\nproposed updated state ~siis computed by\\n~si= tanh (We(yi\\x001) +U[ri\\x0esi\\x001] +Cci);\\nwheree(yi\\x001)2Rmis anm-dimensional embedding of a word yi\\x001, andriis the output of the\\nreset gates (see below). When yiis represented as a 1-of-Kvector,e(yi)is simply a column of an\\nembedding matrix E2Rm\\x02K. Whenever possible, we omit bias terms to make the equations less\\ncluttered.\\nThe update gates ziallow each hidden unit to maintain its previous activation, and the reset gates ri\\ncontrol how much and what information from the previous state should be reset. We compute them\\nby\\nzi=\\x1b(Wze(yi\\x001) +Uzsi\\x001+Czci);',\n",
       " 'by\\nzi=\\x1b(Wze(yi\\x001) +Uzsi\\x001+Czci);\\nri=\\x1b(Wre(yi\\x001) +Ursi\\x001+Crci);\\nwhere\\x1b(\\x01)is a logistic sigmoid function.\\nAt each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered function (Pascanu et al. , 2014). We use a single hidden layer of maxout units (Goodfellow et al. , 2013)\\nand normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)).\\nA.1.2 A LIGNMENT MODEL\\nThe alignment model should be designed considering that the model needs to be evaluated Tx\\x02Ty\\ntimes for each sentence pair of lengths TxandTy. In order to reduce computation, we use a singlelayer multilayer perceptron such that\\na(si\\x001;hj) =v>\\natanh (Wasi\\x001+Uahj);\\nwhereWa2Rn\\x02n;Ua2Rn\\x022nandva2Rnare the weight matrices. Since Uahjdoes not\\ndepend oni, we can pre-compute it in advance to minimize the computational cost.',\n",
       " 'depend oni, we can pre-compute it in advance to minimize the computational cost.\\n8Here, we show the formula of the decoder. The same formula can be used in the encoder by simply\\nignoring the context vector ciand the related terms.\\n12\\nPublished as a conference paper at ICLR 2015\\nA.2 D ETAILED DESCRIPTION OF THE MODEL\\nA.2.1 E NCODER\\nIn this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the\\nexperiments (see Sec. 4–5). From here on, we omit all bias terms in order to increase readability.\\nThe model takes a source sentence of 1-of-K coded word vectors as input\\nx= (x1;:::;x Tx); xi2RKx\\nand outputs a translated sentence of 1-of-K coded word vectors\\ny= (y1;:::;y Ty); yi2RKy;\\nwhereKxandKyare the vocabulary sizes of source and target languages, respectively. TxandTy\\nrespectively denote the lengths of source and target sentences.\\nFirst, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:\\n\\x00 !hi=(',\n",
       " 'respectively denote the lengths of source and target sentences.\\nFirst, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:\\n\\x00 !hi=(\\n(1\\x00\\x00 !zi)\\x0e\\x00 !hi\\x001+\\x00 !zi\\x0e\\x00 !hi, ifi>0\\n0 , ifi= 0\\nwhere\\n\\x00 !hi= tanh\\x10\\x00 !WExi+\\x00 !Uh\\x00 !ri\\x0e\\x00 !hi\\x001i\\x11\\n\\x00 !zi=\\x1b\\x10\\x00 !WzExi+\\x00 !Uz\\x00 !hi\\x001\\x11\\n\\x00 !ri=\\x1b\\x10\\x00 !WrExi+\\x00 !Ur\\x00 !hi\\x001\\x11\\n:\\nE2Rm\\x02Kxis the word embedding matrix.\\x00 !W;\\x00 !Wz;\\x00 !Wr2Rn\\x02m,\\x00 !U;\\x00 !Uz;\\x00 !Ur2Rn\\x02nare\\nweight matrices. mandnare the word embedding dimensionality and the number of hidden units,\\nrespectively. \\x1b(\\x01)is as usual a logistic sigmoid function.\\nThe backward states ( \\x00h1;\\x01\\x01\\x01; \\x00hTx)are computed similarly. We share the word embedding matrix\\nEbetween the forward and backward RNNs, unlike the weight matrices.',\n",
       " 'Ebetween the forward and backward RNNs, unlike the weight matrices.\\nWe concatenate the forward and backward states to to obtain the annotations (h1;h2;\\x01\\x01\\x01;hTx),\\nwhere\\nhi=\"\\x00 !hi \\x00hi#\\n(7)\\nA.2.2 D ECODER\\nThe hidden state siof the decoder given the annotations from the encoder is computed by\\nsi=(1\\x00zi)\\x0esi\\x001+zi\\x0e~si;\\nwhere\\n~si= tanh (WEy i\\x001+U[ri\\x0esi\\x001] +Cci)\\nzi=\\x1b(WzEyi\\x001+Uzsi\\x001+Czci)\\nri=\\x1b(WrEyi\\x001+Ursi\\x001+Crci)\\nEis the word embedding matrix for the target language. W;W z;Wr2Rn\\x02m,U;Uz;Ur2Rn\\x02n,\\nandC;C z;Cr2Rn\\x022nare weights. Again, mandnare the word embedding dimensionality\\nand the number of hidden units, respectively. The initial hidden state s0is computed by s0=\\ntanh\\x10\\nWs \\x00h1\\x11\\n;whereWs2Rn\\x02n.',\n",
       " 'tanh\\x10\\nWs \\x00h1\\x11\\n;whereWs2Rn\\x02n.\\nThe context vector ciare recomputed at each step by the alignment model:\\nci=TxX\\nj=1\\x0bijhj;\\n13\\nPublished as a conference paper at ICLR 2015\\nModel Updates (\\x02105)Epochs Hours GPU Train NLL Dev. NLL\\nRNNenc-30 8.46 6.4 109 TITAN BLACK 28.1 53.0\\nRNNenc-50 6.00 4.5 108 Quadro K-6000 44.0 43.6\\nRNNsearch-30 4.71 3.6 113 TITAN BLACK 26.7 47.2\\nRNNsearch-50 2.88 2.2 111 Quadro K-6000 40.7 38.1\\nRNNsearch-50?6.67 5.0 252 Quadro K-6000 36.7 35.2\\nTable 2: Learning statistics and relevant information. Each update corresponds to updating the\\nparameters once using a single minibatch. One epoch is one pass through the training set. NLL is\\nthe average conditional log-probabilities of the sentences in either the training set or the development\\nset. Note that the lengths of the sentences differ.\\nwhere\\n\\x0bij=exp (eij)PTx\\nk=1exp (eik)\\neij=v>',\n",
       " 'set. Note that the lengths of the sentences differ.\\nwhere\\n\\x0bij=exp (eij)PTx\\nk=1exp (eik)\\neij=v>\\natanh (Wasi\\x001+Uahj);\\nandhjis thej-th annotation in the source sentence (see Eq. (7)). va2Rn0;Wa2Rn0\\x02nand\\nUa2Rn0\\x022nare weight matrices. Note that the model becomes RNN Encoder–Decoder (Cho\\net al. , 2014a), if we ﬁx cito\\x00 !hTx.\\nWith the decoder state si\\x001, the context ciand the last generated word yi\\x001, we deﬁne the probability\\nof a target word yias\\np(yijsi;yi\\x001;ci)/exp\\x00\\ny>\\niWoti\\x01\\n;\\nwhere\\nti=\\x02\\nmax\\x08~ti;2j\\x001;~ti;2j\\t\\x03>\\nj=1;:::;l\\nand~ti;kis thek-th element of a vector ~tiwhich is computed by\\n~ti=Uosi\\x001+VoEyi\\x001+Coci:',\n",
       " 'and~ti;kis thek-th element of a vector ~tiwhich is computed by\\n~ti=Uosi\\x001+VoEyi\\x001+Coci:\\nWo2RKy\\x02l,Uo2R2l\\x02n,Vo2R2l\\x02mandCo2R2l\\x022nare weight matrices. This can be understood as having a deep output (Pascanu et al. , 2014) with a single maxout hidden layer (Goodfellow\\net al. , 2013).\\nA.2.3 M ODEL SIZE\\nFor all the models used in this paper, the size of a hidden layer nis 1000, the word embedding\\ndimensionality mis 620 and the size of the maxout hidden layer in the deep output lis 500. The\\nnumber of hidden units in the alignment model n0is 1000.\\nB T RAINING PROCEDURE\\nB.1 P ARAMETER INITIALIZATION\\nWe initialized the recurrent weight matrices U;Uz;Ur; \\x00U; \\x00Uz; \\x00Ur;\\x00 !U;\\x00 !Uzand\\x00 !Uras random orthogonal matrices. For WaandUa, we initialized them by sampling each element from the Gaussian',\n",
       " 'distribution of mean 0and variance 0:0012. All the elements of Vaand all the bias vectors were initialized to zero. Any other weight matrix was initialized by sampling from the Gaussian distribution\\nof mean 0and variance 0:012.\\nB.2 T RAINING\\nWe used the stochastic gradient descent (SGD) algorithm. Adadelta (Zeiler, 2012) was used to\\nautomatically adapt the learning rate of each parameter ( \\x0f= 10\\x006and\\x1a= 0:95). We explicitly\\n14\\nPublished as a conference paper at ICLR 2015\\nnormalized the L2-norm of the gradient of the cost function each time to be at most a predeﬁned\\nthreshold of 1, when the norm was larger than the threshold (Pascanu et al. , 2013b). Each SGD\\nupdate direction was computed with a minibatch of 80 sentences.\\nAt each update our implementation requires time proportional to the length of the longest sentence in\\na minibatch. Hence, to minimize the waste of computation, before every 20-th update, we retrieved\\n1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches. The\\ntraining data was shufﬂed once before training and was traversed sequentially in this manner.\\nIn Tables 2 we present the statistics related to training all the models used in the experiments.',\n",
       " 'In Tables 2 we present the statistics related to training all the models used in the experiments.\\nC T RANSLATIONS OF LONG SENTENCES\\nSource An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre\\nto carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.\\nReference Le privil `ege d’admission est le droit d’un m ´edecin, en vertu de son statut de membre soignant\\nd’un h ˆopital, d’admettre un patient dans un h ˆopital ou un centre m ´edical aﬁn d’y d ´elivrer un\\ndiagnostic ou un traitement.\\nRNNenc-50 Un privil `ege d’admission est le droit d’un m ´edecin de reconna ˆıtre un patient `a l’h ˆopital ou un\\ncentre m ´edical d’un diagnostic ou de prendre un diagnostic en fonction de son ´etat de sant ´e.',\n",
       " 'centre m ´edical d’un diagnostic ou de prendre un diagnostic en fonction de son ´etat de sant ´e.\\nRNNsearch-50 Un privil `ege d’admission est le droit d’un m ´edecin d’admettre un patient `a un h ˆopital ou un\\ncentre m ´edical pour effectuer un diagnostic ou une proc ´edure, selon son statut de travailleur des\\nsoins de sant ´e`a l’h ˆopital.\\nGoogle\\nTranslateUn privil `ege admettre est le droit d’un m ´edecin d’admettre un patient dans un h ˆopital ou un\\ncentre m ´edical pour effectuer un diagnostic ou une proc ´edure, fond ´ee sur sa situation en tant\\nque travailleur de soins de sant ´e dans un h ˆopital.\\nSource This kind of experience is part of Disney’s efforts to ”extend the lifetime of its series and build\\nnew relationships with audiences via digital platforms that are becoming ever more important,”\\nhe added.',\n",
       " 'new relationships with audiences via digital platforms that are becoming ever more important,”\\nhe added.\\nReference Ce type d’exp ´erience entre dans le cadre des efforts de Disney pour ” ´etendre la dur ´ee de\\nvie de ses s ´eries et construire de nouvelles relations avec son public gr ˆace`a des plateformes\\nnum´eriques qui sont de plus en plus importantes”, a-t-il ajout ´e.\\nRNNenc-50 Ce type d’exp ´erience fait partie des initiatives du Disney pour ”prolonger la dur ´ee de vie de\\nses nouvelles et de d ´evelopper des liens avec les lecteurs num ´eriques qui deviennent plus complexes.\\nRNNsearch-50 Ce genre d’exp ´erience fait partie des efforts de Disney pour ”prolonger la dur ´ee de vie de ses\\ns´eries et cr ´eer de nouvelles relations avec des publics via des plateformes num ´eriques de plus\\nen plus importantes”, a-t-il ajout ´e.\\nGoogle',\n",
       " 'en plus importantes”, a-t-il ajout ´e.\\nGoogle\\nTranslateCe genre d’exp ´erience fait partie des efforts de Disney `a “´etendre la dur ´ee de vie de sa s ´erie et\\nconstruire de nouvelles relations avec le public par le biais des plates-formes num ´eriques qui\\ndeviennent de plus en plus important”, at-il ajout ´e.\\nSource In a press conference on Thursday, Mr Blair stated that there was nothing in this video that might\\nconstitute a ”reasonable motive” that could lead to criminal charges being brought against the\\nmayor.\\nReference En conf ´erence de presse, jeudi, M. Blair a afﬁrm ´e qu’il n’y avait rien dans cette vid ´eo qui puisse\\nconstituer des ”motifs raisonnables” pouvant mener au d ´epˆot d’une accusation criminelle contre\\nle maire.',\n",
       " 'le maire.\\nRNNenc-50 Lors de la conf ´erence de presse de jeudi, M. Blair a dit qu’il n’y avait rien dans cette vid ´eo qui\\npourrait constituer une ”motivation raisonnable” pouvant entra ˆıner des accusations criminelles\\nport´ees contre le maire.\\nRNNsearch-50 Lors d’une conf ´erence de presse jeudi, M. Blair a d ´eclar ´e qu’il n’y avait rien dans cette vid ´eo qui\\npourrait constituer un ”motif raisonnable” qui pourrait conduire `a des accusations criminelles\\ncontre le maire.\\nGoogle\\nTranslateLors d’une conf ´erence de presse jeudi, M. Blair a d ´eclar ´e qu’il n’y avait rien dans cette vido\\nqui pourrait constituer un ”motif raisonnable” qui pourrait mener `a des accusations criminelles\\nportes contre le maire.',\n",
       " 'qui pourrait constituer un ”motif raisonnable” qui pourrait mener `a des accusations criminelles\\nportes contre le maire.\\nTable 3: The translations generated by RNNenc-50 and RNNsearch-50 from long source sentences\\n(30 words or more) selected from the test set. For each source sentence, we also show the goldstandard translation. The translations by Google Translate were made on 27 August 2014.\\n15',\n",
       " 'arXiv:1409.1556v6  [cs.CV]  10 Apr 2015Publishedasa conferencepaperat ICLR2015\\nVERYDEEPCONVOLUTIONAL NETWORKS\\nFORLARGE-SCALEIMAGERECOGNITION\\nKarenSimonyan∗& AndrewZisserman+\\nVisualGeometryGroup,DepartmentofEngineeringScience, UniversityofOxford\\n{karen,az }@robots.ox.ac.uk\\nABSTRACT\\nIn this work we investigate the effect of the convolutional n etwork depth on its\\naccuracy in the large-scale image recognition setting. Our main contribution is\\na thorough evaluation of networks of increasing depth using an architecture with\\nverysmall( 3×3)convolutionﬁlters,whichshowsthatasigniﬁcantimprove ment\\non the prior-art conﬁgurations can be achieved by pushing th e depth to 16–19\\nweight layers. These ﬁndings were the basis of our ImageNet C hallenge 2014\\nsubmission,whereourteamsecuredtheﬁrstandthesecondpl acesinthelocalisation and classiﬁcation tracks respectively. We also show th at our representations\\ngeneralise well to other datasets, where they achieve state -of-the-art results. We',\n",
       " 'generalise well to other datasets, where they achieve state -of-the-art results. We\\nhave made our two best-performingConvNet models publicly a vailable to facilitate furtherresearchontheuse ofdeepvisualrepresentati onsincomputervision.\\n1 INTRODUCTION\\nConvolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition (Krizhevskyetal., 2012; Zeiler& Fergus, 2013; Sermanetet al., 2014;\\nSimonyan& Zisserman, 2014) which has become possible due to the large public image repositories,suchasImageNet(Denget al.,2009),andhigh-perform ancecomputingsystems,suchasGPUs\\norlarge-scaledistributedclusters(Deanet al., 2012). In particular,animportantroleintheadvance\\nofdeepvisualrecognitionarchitectureshasbeenplayedby theImageNetLarge-ScaleVisualRecognition Challenge (ILSVRC) (Russakovskyet al., 2014), whic h has served as a testbed for a few',\n",
       " 'generationsof large-scale image classiﬁcation systems, f rom high-dimensionalshallow feature encodings(Perronninetal.,2010)(thewinnerofILSVRC-2011 )todeepConvNets(Krizhevskyet al.,\\n2012)(thewinnerofILSVRC-2012).\\nWith ConvNets becoming more of a commodity in the computer vi sion ﬁeld, a number of attempts have been made to improve the original architecture o f Krizhevskyet al. (2012) in a\\nbid to achieve better accuracy. For instance, the best-perf orming submissions to the ILSVRC2013 (Zeiler&Fergus, 2013; Sermanetetal., 2014) utilised smaller receptive window size and\\nsmaller stride of the ﬁrst convolutional layer. Another lin e of improvements dealt with training\\nand testing the networks densely over the whole image and ove r multiple scales (Sermanetet al.,\\n2014; Howard, 2014). In this paper, we address another impor tant aspect of ConvNet architecture\\ndesign–itsdepth. Tothisend,we ﬁxotherparametersofthea rchitecture,andsteadilyincreasethe\\ndepth of the network by adding more convolutionallayers, wh ich is feasible due to the use of very',\n",
       " 'depth of the network by adding more convolutionallayers, wh ich is feasible due to the use of very\\nsmall (3×3)convolutionﬁltersinall layers.\\nAs a result, we come up with signiﬁcantly more accurate ConvN et architectures, which not only\\nachieve the state-of-the-art accuracy on ILSVRC classiﬁca tion and localisation tasks, but are also\\napplicabletootherimagerecognitiondatasets,wherethey achieveexcellentperformanceevenwhen\\nusedasa partofa relativelysimple pipelines(e.g.deepfea turesclassiﬁed byalinearSVM without\\nﬁne-tuning). We havereleasedourtwobest-performingmode ls1tofacilitatefurtherresearch.\\nThe rest of the paper is organised as follows. In Sect. 2, we de scribe our ConvNet conﬁgurations.\\nThe details of the image classiﬁcation trainingand evaluat ionare then presented in Sect. 3, and the\\n∗current afﬁliation: Google DeepMind+current afﬁliation: Universityof Oxfordand Google DeepMi nd\\n1http://www.robots.ox.ac.uk/ ˜vgg/research/very_deep/\\n1',\n",
       " '1http://www.robots.ox.ac.uk/ ˜vgg/research/very_deep/\\n1\\nPublishedasa conferencepaperat ICLR2015\\nconﬁgurations are compared on the ILSVRC classiﬁcation tas k in Sect. 4. Sect. 5 concludes the\\npaper. For completeness,we also describeand assess our ILS VRC-2014object localisationsystem\\ninAppendixA,anddiscussthegeneralisationofverydeepfe aturestootherdatasetsinAppendixB.\\nFinally,AppendixCcontainsthelist ofmajorpaperrevisio ns.\\n2 CONVNETCONFIGURATIONS\\nTo measure the improvement brought by the increased ConvNet depth in a fair setting, all our\\nConvNet layer conﬁgurations are designed using the same pri nciples, inspired by Ciresan etal.\\n(2011); Krizhevskyet al. (2012). In this section, we ﬁrst de scribe a generic layout of our ConvNet\\nconﬁgurations(Sect.2.1)andthendetailthespeciﬁcconﬁg urationsusedintheevaluation(Sect.2.2).',\n",
       " 'Ourdesignchoicesarethendiscussedandcomparedtothepri orart inSect. 2.3.\\n2.1 A RCHITECTURE\\nDuring training, the input to our ConvNets is a ﬁxed-size 224×224RGB image. The only preprocessingwedoissubtractingthemeanRGBvalue,computed onthetrainingset,fromeachpixel.\\nTheimageispassedthroughastackofconvolutional(conv.) layers,whereweuseﬁlterswithavery\\nsmall receptive ﬁeld: 3×3(which is the smallest size to capture the notion of left/rig ht, up/down,\\ncenter). In one of the conﬁgurationswe also utilise 1×1convolutionﬁlters, which can be seen as\\na linear transformationof the input channels (followed by n on-linearity). The convolutionstride is\\nﬁxedto1pixel;thespatialpaddingofconv.layerinputissuchthatt hespatialresolutionispreserved\\nafterconvolution,i.e. the paddingis 1pixel for3×3conv.layers. Spatial poolingis carriedoutby',\n",
       " 'afterconvolution,i.e. the paddingis 1pixel for3×3conv.layers. Spatial poolingis carriedoutby\\nﬁvemax-poolinglayers,whichfollowsomeoftheconv.layer s(notalltheconv.layersarefollowed\\nbymax-pooling). Max-poolingisperformedovera 2×2pixelwindow,withstride 2.\\nAstackofconvolutionallayers(whichhasadifferentdepth indifferentarchitectures)isfollowedby\\nthree Fully-Connected(FC) layers: the ﬁrst two have4096ch annelseach,the thirdperforms1000way ILSVRC classiﬁcation and thus contains1000channels(o ne foreach class). The ﬁnal layer is\\nthesoft-maxlayer. Theconﬁgurationofthefullyconnected layersis thesameinall networks.\\nAllhiddenlayersareequippedwiththerectiﬁcation(ReLU( Krizhevskyetal.,2012))non-linearity.\\nWe note that none of our networks (except for one) contain Loc al Response Normalisation\\n(LRN) normalisation (Krizhevskyet al., 2012): as will be sh own in Sect. 4, such normalisation',\n",
       " '(LRN) normalisation (Krizhevskyet al., 2012): as will be sh own in Sect. 4, such normalisation\\ndoes not improve the performance on the ILSVRC dataset, but l eads to increased memory consumption and computation time. Where applicable, the param eters for the LRN layer are those\\nof(Krizhevskyetal., 2012).\\n2.2 C ONFIGURATIONS\\nThe ConvNet conﬁgurations, evaluated in this paper, are out lined in Table 1, one per column. In\\nthe following we will refer to the nets by their names (A–E). A ll conﬁgurationsfollow the generic\\ndesign presented in Sect. 2.1, and differ only in the depth: f rom 11 weight layers in the network A\\n(8conv.and3FClayers)to19weightlayersinthenetworkE(1 6conv.and3FClayers). Thewidth\\nof conv.layers (the number of channels) is rather small, sta rting from 64in the ﬁrst layer and then\\nincreasingbyafactorof 2aftereachmax-poolinglayer,untilit reaches 512.\\nIn Table 2 we reportthe numberof parametersfor each conﬁgur ation. In spite of a large depth, the',\n",
       " 'In Table 2 we reportthe numberof parametersfor each conﬁgur ation. In spite of a large depth, the\\nnumberof weights in our netsis not greater thanthe numberof weightsin a moreshallow net with\\nlargerconv.layerwidthsandreceptiveﬁelds(144Mweights in(Sermanetet al., 2014)).\\n2.3 D ISCUSSION\\nOur ConvNet conﬁgurations are quite different from the ones used in the top-performing entries\\nof the ILSVRC-2012 (Krizhevskyetal., 2012) and ILSVRC-201 3 competitions (Zeiler& Fergus,\\n2013;Sermanetet al.,2014). Ratherthanusingrelativelyl argereceptiveﬁeldsintheﬁrstconv.layers(e.g.11×11withstride 4in(Krizhevskyet al.,2012),or 7×7withstride 2in(Zeiler& Fergus,\\n2013; Sermanetet al., 2014)), we use very small 3×3receptive ﬁelds throughout the whole net,\\nwhichareconvolvedwiththeinputateverypixel(withstrid e1). Itiseasytoseethatastackoftwo',\n",
       " 'whichareconvolvedwiththeinputateverypixel(withstrid e1). Itiseasytoseethatastackoftwo\\n3×3conv.layers(withoutspatialpoolinginbetween)hasaneff ectivereceptiveﬁeldof 5×5;three\\n2\\nPublishedasa conferencepaperat ICLR2015\\nTable 1:ConvNet conﬁgurations (shown in columns). The depth of the conﬁgurations increase s\\nfromtheleft(A)totheright(E),asmorelayersareadded(th eaddedlayersareshowninbold). The\\nconvolutional layer parameters are denoted as “conv /an}bracketle{treceptive ﬁeld size /an}bracketri}ht-/an}bracketle{tnumber of channels /an}bracketri}ht”.\\nTheReLU activationfunctionisnotshownforbrevity.\\nConvNet Conﬁguration\\nA A-LRN B C D E\\n11weight 11weight 13 weight 16weight 16weight 19 weight\\nlayers layers layers layers layers layers\\ninput (224×224RGBimage)\\nconv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64',\n",
       " 'input (224×224RGBimage)\\nconv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64\\nLRN conv3-64 conv3-64 conv3-64 conv3-64\\nmaxpool\\nconv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128\\nconv3-128 conv3-128 conv3-128 conv3-128\\nmaxpool\\nconv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256\\nconv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256\\nconv1-256 conv3-256 conv3-256\\nconv3-256\\nmaxpool\\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\\nconv1-512 conv3-512 conv3-512\\nconv3-512\\nmaxpool\\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\\nconv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512',\n",
       " 'conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512\\nconv1-512 conv3-512 conv3-512\\nconv3-512\\nmaxpool\\nFC-4096\\nFC-4096\\nFC-1000\\nsoft-max\\nTable2:Number ofparameters (inmillions).\\nNetwork A,A-LRN BCDE\\nNumber of parameters 133 133134138144\\nsuch layers have a 7×7effectivereceptive ﬁeld. So what have we gainedby using, fo r instance, a\\nstackofthree 3×3conv.layersinsteadofasingle 7×7layer? First,weincorporatethreenon-linear\\nrectiﬁcation layers instead of a single one, which makes the decision functionmore discriminative.\\nSecond, we decrease the number of parameters: assuming that both the input and the output of a\\nthree-layer 3×3convolutionstack has Cchannels,the stack is parametrisedby 3/parenleftbig\\n32C2/parenrightbig\\n= 27C2\\nweights; at the same time, a single 7×7conv. layer would require 72C2= 49C2parameters, i.e.',\n",
       " '= 27C2\\nweights; at the same time, a single 7×7conv. layer would require 72C2= 49C2parameters, i.e.\\n81%more. Thiscan be seen as imposinga regularisationon the 7×7conv.ﬁlters, forcingthemto\\nhaveadecompositionthroughthe 3×3ﬁlters(withnon-linearityinjectedin between).\\nThe incorporation of 1×1conv. layers (conﬁguration C, Table 1) is a way to increase th e nonlinearity of the decision function without affecting the re ceptive ﬁelds of the conv. layers. Even\\nthoughinourcasethe 1×1convolutionisessentiallyalinearprojectionontothespa ceofthesame\\ndimensionality(thenumberofinputandoutputchannelsist hesame),anadditionalnon-linearityis\\nintroducedbytherectiﬁcationfunction. Itshouldbenoted that1×1conv.layershaverecentlybeen\\nutilisedin the“NetworkinNetwork”architectureofLinet a l.(2014).\\nSmall-size convolution ﬁlters have been previously used by Ciresan etal. (2011), but their nets',\n",
       " 'Small-size convolution ﬁlters have been previously used by Ciresan etal. (2011), but their nets\\nare signiﬁcantly less deep than ours, and they did not evalua te on the large-scale ILSVRC\\ndataset. Goodfellowet al. (2014) applied deep ConvNets ( 11weight layers) to the task of\\nstreet number recognition, and showed that the increased de pth led to better performance.\\nGoogLeNet(Szegedyet al., 2014), a top-performingentryof the ILSVRC-2014classiﬁcation task,\\nwas developed independentlyof our work, but is similar in th at it is based on very deep ConvNets\\n3\\nPublishedasa conferencepaperat ICLR2015\\n(22 weight layers) and small convolution ﬁlters (apart from 3×3, they also use 1×1and5×5\\nconvolutions). Their network topology is, however, more co mplex than ours, and the spatial resolution of the feature maps is reduced more aggressively in th e ﬁrst layers to decrease the amount\\nof computation. As will be shown in Sect. 4.5, our model is out performing that of Szegedyetal.',\n",
       " 'of computation. As will be shown in Sect. 4.5, our model is out performing that of Szegedyetal.\\n(2014)intermsofthesingle-networkclassiﬁcationaccura cy.\\n3 CLASSIFICATION FRAMEWORK\\nIn the previous section we presented the details of our netwo rk conﬁgurations. In this section, we\\ndescribethe detailsofclassiﬁcationConvNettrainingand evaluation.\\n3.1 T RAINING\\nThe ConvNet training procedure generally follows Krizhevs kyetal. (2012) (except for sampling\\ntheinputcropsfrommulti-scaletrainingimages,asexplai nedlater). Namely,thetrainingiscarried\\nout by optimising the multinomial logistic regression obje ctive using mini-batch gradient descent\\n(based on back-propagation(LeCunet al., 1989)) with momen tum. The batch size was set to 256,\\nmomentum to 0.9. The training was regularised by weight decay (the L2penalty multiplier set to\\n5·10−4)anddropoutregularisationfortheﬁrsttwofully-connect edlayers(dropoutratiosetto 0.5).',\n",
       " '5·10−4)anddropoutregularisationfortheﬁrsttwofully-connect edlayers(dropoutratiosetto 0.5).\\nThelearningrate wasinitially setto 10−2,andthendecreasedbyafactorof 10whenthevalidation\\nset accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning\\nwas stopped after 370K iterations (74 epochs). We conjecture that in spite of the l arger number of\\nparametersandthegreaterdepthofournetscomparedto(Kri zhevskyetal.,2012),thenetsrequired\\nlessepochstoconvergedueto(a)implicitregularisationi mposedbygreaterdepthandsmallerconv.\\nﬁlter sizes; (b)pre-initialisationofcertainlayers.\\nThe initialisation of the networkweightsis important,sin ce bad initialisation can stall learningdue\\nto the instability of gradient in deep nets. To circumvent th is problem, we began with training\\nthe conﬁgurationA (Table 1), shallow enoughto be trained wi th randominitialisation. Then,when',\n",
       " 'the conﬁgurationA (Table 1), shallow enoughto be trained wi th randominitialisation. Then,when\\ntrainingdeeperarchitectures,weinitialisedtheﬁrstfou rconvolutionallayersandthelastthreefullyconnectedlayerswiththelayersofnetA(theintermediatel ayerswereinitialisedrandomly). Wedid\\nnotdecreasethelearningrateforthepre-initialisedlaye rs,allowingthemtochangeduringlearning.\\nFor random initialisation (where applicable), we sampled t he weights from a normal distribution\\nwith thezeromeanand 10−2variance. The biaseswere initialisedwith zero. It isworth notingthat\\nafter the paper submission we found that it is possible to ini tialise the weights without pre-training\\nbyusingthe randominitialisationprocedureofGlorot&Ben gio(2010).\\nToobtaintheﬁxed-size 224×224ConvNetinputimages,theywererandomlycroppedfromresca led\\ntraining images (one crop per image per SGD iteration). To fu rther augment the training set, the\\ncropsunderwentrandomhorizontalﬂippingandrandomRGBco lourshift(Krizhevskyet al.,2012).\\nTrainingimagerescalingisexplainedbelow.',\n",
       " 'Trainingimagerescalingisexplainedbelow.\\nTraining image size. LetSbe the smallest side of an isotropically-rescaledtraining image, from\\nwhich the ConvNet input is cropped (we also refer to Sas the training scale). While the crop size\\nis ﬁxed to 224×224, in principle Scan take on any value not less than 224: forS= 224the crop\\nwill capture whole-image statistics, completely spanning the smallest side of a training image; for\\nS≫224thecropwillcorrespondtoasmallpartoftheimage,contain ingasmallobjectoranobject\\npart.\\nWe considertwoapproachesforsettingthetrainingscale S. Theﬁrst istoﬁx S,whichcorresponds\\nto single-scale training (note that image content within th e sampled crops can still represent multiscale image statistics). In our experiments, we evaluated m odels trained at two ﬁxed scales: S=\\n256(which has been widely used in the prior art (Krizhevskyet al ., 2012; Zeiler&Fergus, 2013;\\nSermanetet al., 2014)) and S= 384. Given a ConvNet conﬁguration,we ﬁrst trained the network',\n",
       " 'Sermanetet al., 2014)) and S= 384. Given a ConvNet conﬁguration,we ﬁrst trained the network\\nusingS= 256. To speed-up training of the S= 384network, it was initialised with the weights\\npre-trainedwith S= 256,andwe useda smallerinitiallearningrateof 10−3.\\nThe second approachto setting Sis multi-scale training, where each training image is indiv idually\\nrescaled by randomly sampling Sfrom a certain range [Smin,Smax](we used Smin= 256and\\nSmax= 512). Sinceobjectsinimagescanbeofdifferentsize,itisbene ﬁcialtotakethisintoaccount\\nduringtraining. Thiscanalso beseen astrainingset augmen tationbyscale jittering,wherea single\\n4\\nPublishedasa conferencepaperat ICLR2015\\nmodel is trained to recognise objects over a wide range of sca les. For speed reasons, we trained\\nmulti-scale models by ﬁne-tuning all layers of a single-sca le model with the same conﬁguration,\\npre-trainedwithﬁxed S= 384.\\n3.2 T ESTING',\n",
       " 'pre-trainedwithﬁxed S= 384.\\n3.2 T ESTING\\nAttest time,givena trainedConvNetandaninputimage,itis classiﬁed inthefollowingway. First,\\nit is isotropically rescaled to a pre-deﬁned smallest image side, denoted as Q(we also refer to it\\nas the test scale). We note that Qis not necessarily equal to the training scale S(as we will show\\nin Sect. 4, usingseveralvaluesof QforeachSleadsto improvedperformance). Then,the network\\nis applied densely overthe rescaled test image in a way simil ar to (Sermanetet al., 2014). Namely,\\nthe fully-connected layers are ﬁrst converted to convoluti onal layers (the ﬁrst FC layer to a 7×7\\nconv. layer, the last two FC layers to 1×1conv. layers). The resulting fully-convolutional net is\\nthen applied to the whole (uncropped) image. The result is a c lass score map with the number of\\nchannels equal to the number of classes, and a variable spati al resolution, dependent on the input\\nimagesize. Finally,toobtainaﬁxed-sizevectorofclasssc oresfortheimage,theclassscoremapis',\n",
       " 'imagesize. Finally,toobtainaﬁxed-sizevectorofclasssc oresfortheimage,theclassscoremapis\\nspatially averaged(sum-pooled). We also augmentthe test s et by horizontalﬂippingof the images;\\nthesoft-maxclassposteriorsoftheoriginalandﬂippedima gesareaveragedtoobtaintheﬁnalscores\\nfortheimage.\\nSince the fully-convolutionalnetwork is applied over the w hole image, there is no need to sample\\nmultiple crops at test time (Krizhevskyetal., 2012), which is less efﬁcient as it requires network\\nre-computationforeachcrop. Atthesametime,usingalarge setofcrops,asdonebySzegedyetal.\\n(2014),canleadtoimprovedaccuracy,asit resultsin aﬁner samplingoftheinputimagecompared\\ntothefully-convolutionalnet. Also,multi-cropevaluati oniscomplementarytodenseevaluationdue\\nto different convolution boundary conditions: when applyi ng a ConvNet to a crop, the convolved\\nfeature mapsare paddedwith zeros, while in the case of dense evaluationthe paddingfor the same',\n",
       " 'feature mapsare paddedwith zeros, while in the case of dense evaluationthe paddingfor the same\\ncrop naturally comes from the neighbouring parts of an image (due to both the convolutions and\\nspatial pooling), which substantially increases the overa ll network receptive ﬁeld, so more context\\niscaptured. Whilewebelievethatinpracticetheincreased computationtimeofmultiplecropsdoes\\nnotjustifythepotentialgainsinaccuracy,forreferencew ealsoevaluateournetworksusing 50crops\\nperscale( 5×5regulargridwith 2ﬂips),foratotalof 150cropsover 3scales,whichiscomparable\\nto144cropsover 4scalesusedbySzegedyetal. (2014).\\n3.3 IMPLEMENTATION DETAILS\\nOurimplementationisderivedfromthepubliclyavailableC ++ Caffetoolbox(Jia,2013)(branched\\nout in December 2013), but contains a number of signiﬁcant mo diﬁcations, allowing us to perform\\ntrainingandevaluationonmultipleGPUsinstalledinasing lesystem,aswellastrainandevaluateon\\nfull-size (uncropped) images at multiple scales (as descri bed above). Multi-GPU training exploits',\n",
       " 'full-size (uncropped) images at multiple scales (as descri bed above). Multi-GPU training exploits\\ndata parallelism, and is carried out by splitting each batch of training images into several GPU\\nbatches, processed in parallel on each GPU. After the GPU bat ch gradientsare computed, they are\\naveraged to obtain the gradient of the full batch. Gradient c omputation is synchronous across the\\nGPUs, sothe resultisexactlythesame aswhentrainingona si ngleGPU.\\nWhile more sophisticated methods of speeding up ConvNet tra ining have been recently proposed (Krizhevsky, 2014), which employmodeland data paral lelism for differentlayersof the net,\\nwehavefoundthatourconceptuallymuchsimplerschemealre adyprovidesaspeedupof 3.75times\\non an off-the-shelf4-GPU system, as comparedto using a sing le GPU. On a system equippedwith\\nfourNVIDIATitanBlackGPUs,trainingasinglenettook2–3w eeksdependingonthearchitecture.\\n4 CLASSIFICATION EXPERIMENTS\\nDataset. In this section, we present the image classiﬁcation results achieved by the described',\n",
       " '4 CLASSIFICATION EXPERIMENTS\\nDataset. In this section, we present the image classiﬁcation results achieved by the described\\nConvNetarchitecturesontheILSVRC-2012dataset(whichwa susedforILSVRC2012–2014challenges). The dataset includes images of 1000 classes, and is split into three sets: training ( 1.3M\\nimages), validation ( 50K images), and testing ( 100K images with held-out class labels). The classiﬁcation performanceis evaluated using two measures: the top-1 and top-5 error. The former is a\\nmulti-class classiﬁcation error, i.e. the proportion of in correctly classiﬁed images; the latter is the\\n5\\nPublishedasa conferencepaperat ICLR2015\\nmain evaluation criterion used in ILSVRC, and is computed as the proportion of images such that\\ntheground-truthcategoryisoutsidethetop-5predictedca tegories.\\nForthemajorityofexperiments,weusedthevalidationseta sthetestset. Certainexperimentswere\\nalso carried out on the test set and submitted to the ofﬁcial I LSVRC server as a “VGG” team entry\\ntothe ILSVRC-2014competition(Russakovskyet al., 2014).',\n",
       " 'tothe ILSVRC-2014competition(Russakovskyet al., 2014).\\n4.1 SINGLESCALEEVALUATION\\nWe begin with evaluating the performanceof individual Conv Net models at a single scale with the\\nlayerconﬁgurationsdescribedin Sect. 2.2. The test images ize was set as follows: Q=Sforﬁxed\\nS,andQ= 0.5(Smin+Smax)forjittered S∈[Smin,Smax]. Theresultsofareshownin Table3.\\nFirst, we note that using local response normalisation (A-L RN network) does not improve on the\\nmodel A without any normalisation layers. We thus do not empl oy normalisation in the deeper\\narchitectures(B–E).\\nSecond, we observe that the classiﬁcation error decreases w ith the increased ConvNet depth: from\\n11 layers in A to 19 layers in E. Notably, in spite of the same de pth, the conﬁguration C (which\\ncontainsthree 1×1conv.layers),performsworsethantheconﬁgurationD,whic huses3×3conv.\\nlayersthroughoutthenetwork. Thisindicatesthatwhileth e additionalnon-linearitydoeshelp(Cis',\n",
       " 'layersthroughoutthenetwork. Thisindicatesthatwhileth e additionalnon-linearitydoeshelp(Cis\\nbetter than B), it is also important to capture spatial conte xt by using conv. ﬁlters with non-trivial\\nreceptive ﬁelds (D is better than C). The error rate of our arc hitecture saturates when the depth\\nreaches19layers,butevendeepermodelsmightbebeneﬁcialforlarger datasets. Wealsocompared\\nthe net B with a shallow net with ﬁve 5×5conv. layers, which was derived from B by replacing\\neachpairof 3×3conv. layerswithasingle 5×5conv. layer(whichhasthesamereceptiveﬁeldas\\nexplained in Sect. 2.3). The top-1 error of the shallow net wa s measured to be 7%higher than that\\nof B (on a center crop),which conﬁrmsthat a deepnet with smal l ﬁlters outperformsa shallow net\\nwithlargerﬁlters.\\nFinally, scale jittering at training time ( S∈[256;512] ) leads to signiﬁcantly better results than',\n",
       " 'Finally, scale jittering at training time ( S∈[256;512] ) leads to signiﬁcantly better results than\\ntraining on images with ﬁxed smallest side ( S= 256orS= 384), even though a single scale is\\nusedattesttime. Thisconﬁrmsthattrainingsetaugmentati onbyscalejitteringisindeedhelpfulfor\\ncapturingmulti-scaleimagestatistics.\\nTable3:ConvNetperformanceatasingle testscale.\\nConvNet conﬁg. (Table 1) smallest image side top-1 val.error (%) top-5 val.error (%)\\ntrain(S)test (Q)\\nA 256 256 29.6 10.4\\nA-LRN 256 256 29.7 10.5\\nB 256 256 28.7 9.9\\nC256 256 28.1 9.4\\n384 384 28.1 9.3\\n[256;512] 384 27.3 8.8\\nD256 256 27.0 8.8\\n384 384 26.8 8.7\\n[256;512] 384 25.6 8.1\\nE256 256 27.3 9.0\\n384 384 26.9 8.7\\n[256;512] 384 25.5 8.0\\n4.2 M ULTI-SCALEEVALUATION',\n",
       " '384 384 26.9 8.7\\n[256;512] 384 25.5 8.0\\n4.2 M ULTI-SCALEEVALUATION\\nHavingevaluatedtheConvNetmodelsatasinglescale,wenow assesstheeffectofscalejitteringat\\ntesttime. Itconsistsofrunningamodeloverseveralrescal edversionsofatestimage(corresponding\\nto different values of Q), followed by averaging the resulting class posteriors. Co nsidering that a\\nlarge discrepancy between training and testing scales lead s to a drop in performance, the models\\ntrained with ﬁxed Swere evaluated over three test image sizes, close to the trai ning one: Q=\\n{S−32,S,S+ 32}. At the same time, scale jittering at training time allows th e network to be\\nappliedto a widerrangeofscales at test time,so the modeltr ainedwithvariable S∈[Smin;Smax]\\nwasevaluatedoveralargerrangeofsizes Q={Smin,0.5(Smin+Smax),Smax}.\\n6\\nPublishedasa conferencepaperat ICLR2015\\nTheresults,presentedinTable4,indicatethatscalejitte ringattest timeleadstobetterperformance',\n",
       " '6\\nPublishedasa conferencepaperat ICLR2015\\nTheresults,presentedinTable4,indicatethatscalejitte ringattest timeleadstobetterperformance\\n(as compared to evaluating the same model at a single scale, s hown in Table 3). As before, the\\ndeepest conﬁgurations(D and E) perform the best, and scale j ittering is better than training with a\\nﬁxed smallest side S. Our best single-network performance on the validation set is24.8%/7.5%\\ntop-1/top-5error(highlightedinboldinTable4). Onthete stset,theconﬁgurationEachieves 7.3%\\ntop-5error.\\nTable4:ConvNetperformanceatmultiple test scales.\\nConvNet conﬁg. (Table 1) smallest image side top-1val. error (%) top-5val. error (%)\\ntrain(S)test(Q)\\nB 256 224,256,288 28.2 9.6\\nC256 224,256,288 27.7 9.2\\n384 352,384,416 27.8 9.2\\n[256;512] 256,384,512 26.3 8.2\\nD256 224,256,288 26.6 8.6',\n",
       " '[256;512] 256,384,512 26.3 8.2\\nD256 224,256,288 26.6 8.6\\n384 352,384,416 26.5 8.6\\n[256;512] 256,384,512 24.8 7.5\\nE256 224,256,288 26.9 8.7\\n384 352,384,416 26.7 8.6\\n[256;512] 256,384,512 24.8 7.5\\n4.3 M ULTI-CROP EVALUATION\\nIn Table 5 we compare dense ConvNet evaluation with mult-cro p evaluation (see Sect. 3.2 for details). We also assess the complementarityof thetwo evalua tiontechniquesbyaveragingtheirsoftmax outputs. As can be seen, using multiple crops performs sl ightly better than dense evaluation,\\nandthe two approachesareindeedcomplementary,astheir co mbinationoutperformseach ofthem.\\nAs noted above, we hypothesize that this is due to a different treatment of convolution boundary\\nconditions.\\nTable 5:ConvNetevaluationtechniques comparison. Inall experimentsthe trainingscale Swas\\nsampledfrom [256;512] ,andthreetest scales Qwereconsidered: {256,384,512}.',\n",
       " 'sampledfrom [256;512] ,andthreetest scales Qwereconsidered: {256,384,512}.\\nConvNet conﬁg. (Table 1) Evaluationmethod top-1 val. error(%) top-5 val. error (%)\\nDdense 24.8 7.5\\nmulti-crop 24.6 7.5\\nmulti-crop &dense 24.4 7.2\\nEdense 24.8 7.5\\nmulti-crop 24.6 7.4\\nmulti-crop &dense 24.4 7.1\\n4.4 C ONVNETFUSION\\nUpuntilnow,weevaluatedtheperformanceofindividualCon vNetmodels. Inthispartoftheexperiments,wecombinetheoutputsofseveralmodelsbyaveragin gtheirsoft-maxclassposteriors. This\\nimprovesthe performancedueto complementarityof the mode ls, andwas used in the top ILSVRC\\nsubmissions in 2012 (Krizhevskyet al., 2012) and 2013 (Zeil er&Fergus, 2013; Sermanetet al.,\\n2014).\\nThe results are shown in Table 6. By the time of ILSVRC submiss ion we had only trained the\\nsingle-scale networks, as well as a multi-scale model D (by ﬁ ne-tuning only the fully-connected',\n",
       " 'single-scale networks, as well as a multi-scale model D (by ﬁ ne-tuning only the fully-connected\\nlayers rather than all layers). The resulting ensemble of 7 n etworks has 7.3%ILSVRC test error.\\nAfter the submission, we considered an ensemble of only two b est-performing multi-scale models\\n(conﬁgurations D and E), which reduced the test error to 7.0%using dense evaluation and 6.8%\\nusing combined dense and multi-crop evaluation. For refere nce, our best-performingsingle model\\nachieves7.1%error(modelE, Table5).\\n4.5 C OMPARISON WITH THE STATE OF THE ART\\nFinally, we compare our results with the state of the art in Ta ble 7. In the classiﬁcation task of\\nILSVRC-2014 challenge (Russakovskyet al., 2014), our “VGG ” team secured the 2nd place with\\n7\\nPublishedasa conferencepaperat ICLR2015\\nTable6:Multiple ConvNetfusion results.\\nCombined ConvNet modelsError\\ntop-1 val top-5val top-5test\\nILSVRCsubmission',\n",
       " 'Table6:Multiple ConvNetfusion results.\\nCombined ConvNet modelsError\\ntop-1 val top-5val top-5test\\nILSVRCsubmission\\n(D/256/224,256,288), (D/384/352,384,416), (D/[256;512 ]/256,384,512)\\n(C/256/224,256,288), (C/384/352,384,416)\\n(E/256/224,256,288), (E/384/352,384,416)24.7 7.5 7.3\\npost-submission\\n(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,dense eval. 24.0 7.1 7.0\\n(D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,multi-crop 23.9 7.2 (D/[256;512]/256,384,512), (E/[256;512]/256,384,512) ,multi-crop &dense eval. 23.7 6.8 6.8\\n7.3%test errorusinganensembleof7 models. Afterthesubmissio n,we decreasedtheerrorrateto',\n",
       " '7.3%test errorusinganensembleof7 models. Afterthesubmissio n,we decreasedtheerrorrateto\\n6.8%usinganensembleof2models.\\nAs can be seen from Table 7, our very deep ConvNetssigniﬁcant ly outperformthe previousgeneration of models, which achieved the best results in the ILSVR C-2012 and ILSVRC-2013 competitions. Our result is also competitivewith respect to the cla ssiﬁcation task winner(GoogLeNetwith\\n6.7%error) and substantially outperforms the ILSVRC-2013 winn ing submission Clarifai, which\\nachieved 11.2%with outside training data and 11.7%without it. This is remarkable, considering\\nthat our best result is achievedby combiningjust two models – signiﬁcantly less than used in most\\nILSVRC submissions. In terms of the single-net performance , our architecture achieves the best\\nresult (7.0%test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart\\nfrom the classical ConvNet architecture of LeCunetal. (198 9), but improved it by substantially\\nincreasingthedepth.',\n",
       " 'from the classical ConvNet architecture of LeCunetal. (198 9), but improved it by substantially\\nincreasingthedepth.\\nTable 7:Comparison with the state of the art in ILSVRC classiﬁcation . Our methodis denoted\\nas“VGG”.Onlytheresultsobtainedwithoutoutsidetrainin gdataarereported.\\nMethod top-1 val. error(%) top-5val. error (%) top-5testerror (%)\\nVGG(2nets, multi-crop& dense eval.) 23.7 6.8 6.8\\nVGG(1net, multi-crop& dense eval.) 24.4 7.1 7.0\\nVGG(ILSVRCsubmission, 7nets, dense eval.) 24.7 7.5 7.3\\nGoogLeNet (Szegedy et al., 2014) (1net) - 7.9\\nGoogLeNet (Szegedy et al., 2014) (7nets) - 6.7\\nMSRA(He et al., 2014) (11nets) - - 8.1\\nMSRA(He et al., 2014) (1net) 27.9 9.1 9.1\\nClarifai(Russakovsky et al., 2014) (multiplenets) - - 11.7',\n",
       " 'Clarifai(Russakovsky et al., 2014) (multiplenets) - - 11.7\\nClarifai(Russakovsky et al., 2014) (1net) - - 12.5\\nZeiler& Fergus (Zeiler&Fergus, 2013) (6nets) 36.0 14.7 14.8\\nZeiler& Fergus (Zeiler&Fergus, 2013) (1net) 37.5 16.0 16.1\\nOverFeat (Sermanetet al.,2014) (7nets) 34.0 13.2 13.6\\nOverFeat (Sermanetet al.,2014) (1net) 35.7 14.2 Krizhevsky et al.(Krizhevsky et al., 2012) (5nets) 38.1 16.4 16.4\\nKrizhevsky et al.(Krizhevsky et al., 2012) (1net) 40.7 18.2 5 CONCLUSION\\nIn this work we evaluated very deep convolutional networks ( up to 19 weight layers) for largescale image classiﬁcation. It was demonstrated that the rep resentation depth is beneﬁcial for the\\nclassiﬁcationaccuracy,andthatstate-of-the-artperfor manceontheImageNetchallengedatasetcan',\n",
       " 'classiﬁcationaccuracy,andthatstate-of-the-artperfor manceontheImageNetchallengedatasetcan\\nbeachievedusingaconventionalConvNetarchitecture(LeC unet al.,1989;Krizhevskyet al.,2012)\\nwithsubstantiallyincreaseddepth. Intheappendix,weals oshowthatourmodelsgeneralisewellto\\na wide range of tasks and datasets, matchingor outperformin gmore complexrecognitionpipelines\\nbuiltaroundlessdeepimagerepresentations. Ourresultsy etagainconﬁrmtheimportanceof depth\\ninvisualrepresentations.\\nACKNOWLEDGEMENTS\\nThisworkwassupportedbyERCgrantVisRecno.228180. Wegra tefullyacknowledgethesupport\\nofNVIDIACorporationwiththedonationoftheGPUsusedfort hisresearch.\\n8\\nPublishedasa conferencepaperat ICLR2015\\nREFERENCES\\nBell, S., Upchurch, P.,Snavely, N., and Bala, K. Material re cognition inthe wild withthe materials in context\\ndatabase. CoRR,abs/1412.0623, 2014.',\n",
       " 'database. CoRR,abs/1412.0623, 2014.\\nChatﬁeld, K., Simonyan, K., Vedaldi, A., and Zisserman, A. R eturn of the devil in the details: Delving deep\\nintoconvolutional nets. In Proc.BMVC. ,2014.\\nCimpoi,M.,Maji,S.,andVedaldi,A. Deepconvolutionalﬁlt erbanksfortexturerecognitionandsegmentation.\\nCoRR,abs/1411.6836, 2014.\\nCiresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance\\nconvolutional neural networks for image classiﬁcation. In IJCAI,pp. 1237–1242, 2011.\\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M. , Ranzato, M., Senior, A., Tucker, P., Yang,\\nK.,Le,Q. V.,andNg, A.Y. Large scale distributeddeepnetwo rks. InNIPS,pp. 1232–1240, 2012.',\n",
       " 'K.,Le,Q. V.,andNg, A.Y. Large scale distributeddeepnetwo rks. InNIPS,pp. 1232–1240, 2012.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. Imagenet: A large-scale hierarchical image\\ndatabase. In Proc.CVPR ,2009.\\nDonahue,J.,Jia,Y.,Vinyals,O.,Hoffman,J.,Zhang,N.,Tz eng,E.,andDarrell,T.Decaf: Adeepconvolutional\\nactivation feature for generic visual recognition. CoRR,abs/1310.1531, 2013.\\nEveringham, M., Eslami, S.M. A., Van Gool, L., Williams,C., Winn, J., and Zisserman, A. The Pascal visual\\nobject classes challenge: Aretrospective. IJCV,111(1):98–136, 2015.\\nFei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An\\nincremental bayesian approach tested on 101 object categor ies. InIEEE CVPR Workshop of Generative\\nModel BasedVision , 2004.',\n",
       " 'incremental bayesian approach tested on 101 object categor ies. InIEEE CVPR Workshop of Generative\\nModel BasedVision , 2004.\\nGirshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection\\nand semantic segmentation. CoRR,abs/1311.2524v5, 2014. PublishedinProc.CVPR,2014.\\nGkioxari, G.,Girshick, R.,and Malik, J. Actions and attrib utes from wholes and parts. CoRR,abs/1412.2604,\\n2014.\\nGlorot, X. andBengio, Y. Understanding the difﬁcultyof tra iningdeep feedforward neural networks. In Proc.\\nAISTATS,volume 9, pp. 249–256, 2010.\\nGoodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Sh et, V. Multi-digit number recognition from street\\nview imagery usingdeep convolutional neural networks. In Proc.ICLR ,2014.\\nGrifﬁn, G., Holub, A., and Perona, P. Caltech-256 object cat egory dataset. Technical Report 7694, California\\nInstitute of Technology, 2007.',\n",
       " 'Institute of Technology, 2007.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid poolin g in deep convolutional networks for visual\\nrecognition. CoRR,abs/1406.4729v2, 2014.\\nHoai, M. Regularizedmax pooling forimage categorization. InProc. BMVC. ,2014.\\nHoward, A.G. Someimprovements ondeepconvolutional neura l networkbasedimageclassiﬁcation. In Proc.\\nICLR,2014.\\nJia, Y. Caffe: An open source convolutional architecture fo r fast feature embedding.\\nhttp://caffe.berkeleyvision.org/ ,2013.\\nKarpathy, A. and Fei-Fei, L. Deep visual-semantic alignmen ts for generating image descriptions. CoRR,\\nabs/1412.2306, 2014.\\nKiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visu al-semantic embeddings with multimodal neural\\nlanguage models. CoRR,abs/1411.2539, 2014.\\nKrizhevsky, A. One weirdtrickfor parallelizingconvoluti onal neural networks. CoRR,abs/1404.5997, 2014.',\n",
       " 'Krizhevsky, A. One weirdtrickfor parallelizingconvoluti onal neural networks. CoRR,abs/1404.5997, 2014.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet cl assiﬁcation with deep convolutional neural networks. In NIPS,pp. 1106–1114, 2012.\\nLeCun,Y.,Boser, B.,Denker, J.S.,Henderson, D.,Howard, R .E.,Hubbard, W.,andJackel, L.D. Backpropagationapplied tohandwrittenzipcode recognition. Neural Computation , 1(4):541–551, 1989.\\nLin,M., Chen, Q.,andYan, S. Networkinnetwork. In Proc.ICLR ,2014.\\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutiona l networks for semantic segmentation. CoRR,\\nabs/1411.4038, 2014.\\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations\\nusing Convolutional Neural Networks. In Proc.CVPR ,2014.',\n",
       " 'using Convolutional Neural Networks. In Proc.CVPR ,2014.\\nPerronnin, F.,S´ anchez, J.,andMensink, T. Improving theF isherkernel forlarge-scale image classiﬁcation. In\\nProc.ECCV ,2010.\\nRazavian, A.,Azizpour, H.,Sullivan, J.,andCarlsson,S. C NNFeaturesoff-the-shelf: anAstounding Baseline\\nfor Recognition. CoRR,abs/1403.6382, 2014.\\n9\\nPublishedasa conferencepaperat ICLR2015\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,\\nBernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large sc ale visual recognition challenge. CoRR,\\nabs/1409.0575, 2014.\\nSermanet,P.,Eigen, D.,Zhang, X.,Mathieu, M.,Fergus,R., andLeCun,Y. OverFeat: IntegratedRecognition,\\nLocalizationand Detectionusing Convolutional Networks. InProc.ICLR ,2014.',\n",
       " 'Localizationand Detectionusing Convolutional Networks. InProc.ICLR ,2014.\\nSimonyan, K. and Zisserman, A. Two-stream convolutional ne tworks for action recognition in videos. CoRR,\\nabs/1406.2199, 2014. Published inProc.NIPS,2014.\\nSzegedy, C., Liu, W.,Jia, Y., Sermanet, P.,Reed, S.,Anguel ov, D.,Erhan, D., Vanhoucke, V., and Rabinovich,\\nA. Goingdeeper withconvolutions. CoRR,abs/1409.4842, 2014.\\nWei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan , S. CNN: Single-label to multi-label. CoRR,\\nabs/1406.5726, 2014.\\nZeiler, M. D. and Fergus, R. Visualizing and understanding c onvolutional networks. CoRR, abs/1311.2901,\\n2013. PublishedinProc. ECCV,2014.\\nA LOCALISATION\\nIn the main bodyof the paper we have consideredthe classiﬁca tion task of the ILSVRC challenge,',\n",
       " 'A LOCALISATION\\nIn the main bodyof the paper we have consideredthe classiﬁca tion task of the ILSVRC challenge,\\nand performed a thorough evaluation of ConvNet architectur es of different depth. In this section,\\nwe turn to the localisation task of the challenge, which we ha ve won in 2014 with 25.3%error. It\\ncan be seen as a special case of object detection, where a sing le object bounding box should be\\npredictedforeach ofthe top-5classes, irrespectiveof the actual numberofobjectsof the class. For\\nthiswe adoptthe approachof Sermanetet al. (2014), the winn ersof the ILSVRC-2013localisation\\nchallenge,withafewmodiﬁcations. Ourmethodisdescribed inSect.A.1andevaluatedinSect.A.2.\\nA.1 L OCALISATION CONVNET\\nTo perform object localisation, we use a very deep ConvNet, w here the last fully connected layer\\npredicts the bounding box location instead of the class scor es. A bounding box is represented by\\na 4-D vector storing its center coordinates, width, and heig ht. There is a choice of whether the',\n",
       " 'a 4-D vector storing its center coordinates, width, and heig ht. There is a choice of whether the\\nboundingbox prediction is shared across all classes (singl e-class regression, SCR (Sermanetet al.,\\n2014))orisclass-speciﬁc(per-classregression,PCR).In theformercase,thelastlayeris4-D,while\\nin the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding\\nboxpredictionlayer,weuse theConvNetarchitectureD (Tab le1),whichcontains16weightlayers\\nandwasfoundtobe thebest-performingin theclassiﬁcation task (Sect.4).\\nTraining. Training of localisation ConvNets is similar to that of the c lassiﬁcation ConvNets\\n(Sect.3.1). Themaindifferenceisthatwereplacethelogis ticregressionobjectivewithaEuclidean\\nloss,whichpenalisesthedeviationofthepredictedboundi ngboxparametersfromtheground-truth.\\nWe trainedtwo localisation models, each on a single scale: S= 256andS= 384(due to the time',\n",
       " 'We trainedtwo localisation models, each on a single scale: S= 256andS= 384(due to the time\\nconstraints,we didnot use trainingscale jitteringforour ILSVRC-2014submission). Trainingwas\\ninitialised with the correspondingclassiﬁcation models ( trained on the same scales), and the initial\\nlearning rate was set to 10−3. We exploredboth ﬁne-tuningall layers and ﬁne-tuningonly the ﬁrst\\ntwo fully-connected layers, as done in (Sermanetetal., 201 4). The last fully-connected layer was\\ninitialisedrandomlyandtrainedfromscratch.\\nTesting. We consider two testing protocols. The ﬁrst is used for compa ring different network\\nmodiﬁcations on the validation set, and considers only the b oundingbox prediction for the ground\\ntruth class (to factor out the classiﬁcation errors). The bo unding box is obtained by applying the\\nnetworkonlyto thecentralcropoftheimage.\\nThe second, fully-ﬂedged, testing procedure is based on the dense application of the localisation\\nConvNet to the whole image, similarly to the classiﬁcation t ask (Sect. 3.2). The difference is that',\n",
       " 'ConvNet to the whole image, similarly to the classiﬁcation t ask (Sect. 3.2). The difference is that\\ninstead of the class score map, the output of the last fully-c onnected layer is a set of bounding\\nbox predictions. To come up with the ﬁnal prediction, we util ise the greedy merging procedure\\nof Sermanetetal. (2014), which ﬁrst merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obta ined from the classiﬁcation ConvNet.\\nWhen several localisation ConvNets are used, we ﬁrst take th e union of their sets of boundingbox\\npredictions, and then run the mergingprocedureon the union . We did not use the multiple pooling\\n10\\nPublishedasa conferencepaperat ICLR2015\\noffsets technique of Sermanetetal. (2014), which increase s the spatial resolution of the bounding\\nboxpredictionsandcanfurtherimprovetheresults.\\nA.2 L OCALISATION EXPERIMENTS',\n",
       " 'boxpredictionsandcanfurtherimprovetheresults.\\nA.2 L OCALISATION EXPERIMENTS\\nIn this section we ﬁrst determine the best-performinglocal isation setting (using the ﬁrst test protocol), and then evaluate it in a fully-ﬂedged scenario (the se cond protocol). The localisation error\\nis measured according to the ILSVRC criterion (Russakovsky et al., 2014), i.e. the bounding box\\npredictionis deemed correctif its intersectionoverunion ratio with the ground-truthboundingbox\\nisabove0.5.\\nSettings comparison. As can be seen from Table 8, per-class regression (PCR) outpe rforms the\\nclass-agnostic single-class regression (SCR), which diff ers from the ﬁndings of Sermanetetal.\\n(2014), where PCR was outperformed by SCR. We also note that ﬁ ne-tuning all layers for the localisation task leads to noticeablybetter results than ﬁne -tuningonly the fully-connectedlayers(as\\ndonein(Sermanetet al.,2014)). Intheseexperiments,thes mallestimagessidewassetto S= 384;\\ntheresultswith S= 256exhibitthesamebehaviourandarenotshownforbrevity.',\n",
       " 'theresultswith S= 256exhibitthesamebehaviourandarenotshownforbrevity.\\nTable 8:Localisation error for different modiﬁcations with the simpliﬁed testing protocol: the\\nboundingbox is predictedfrom a single central image crop, a nd the ground-truthclass is used. All\\nConvNet layers (except for the last one) have the conﬁgurati on D (Table 1), while the last layer\\nperformseithersingle-classregression(SCR) orper-clas sregression(PCR).\\nFine-tunedlayers regression type GTclass localisationerror\\n1st and2nd FCSCR 36.4\\nPCR 34.3\\nall PCR 33.1\\nFully-ﬂedgedevaluation. Havingdeterminedthebestlocalisationsetting(PCR,ﬁne- tuningofall\\nlayers),we nowapply it in the fully-ﬂedgedscenario,where the top-5class labelsare predictedusing our best-performingclassiﬁcation system (Sect. 4.5), and multiple densely-computedbounding',\n",
       " 'box predictions are merged using the method of Sermanetetal . (2014). As can be seen from Table 9, applicationof the localisationConvNetto the whole i magesubstantiallyimprovesthe results\\ncompared to using a center crop (Table 8), despite using the t op-5 predicted class labels instead of\\nthegroundtruth. Similarlytotheclassiﬁcationtask(Sect .4),testingatseveralscalesandcombining\\nthepredictionsofmultiplenetworksfurtherimprovesthep erformance.\\nTable9:Localisationerror\\nsmallestimage side top-5localisationerror (%)\\ntrain(S) test(Q) val. test.\\n256 256 29.5 384 384 28.2 26.7\\n384 352,384 27.5 fusion: 256/256 and 384/352,384 26.9 25.3\\nComparison with the state of the art. We compare our best localisation result with the state\\nof the art in Table 10. With 25.3%test error, our “VGG” team won the localisation challenge of\\nILSVRC-2014 (Russakovskyet al., 2014). Notably, our resul ts are considerably better than those\\nof the ILSVRC-2013winnerOverfeat(Sermanetet al., 2014), even thoughwe used less scales and',\n",
       " 'of the ILSVRC-2013winnerOverfeat(Sermanetet al., 2014), even thoughwe used less scales and\\ndid not employ their resolution enhancement technique. We e nvisage that better localisation performance can be achieved if this technique is incorporated i nto our method. This indicates the\\nperformanceadvancementbroughtbyourverydeepConvNets– wegotbetterresultswithasimpler\\nlocalisationmethod,buta morepowerfulrepresentation.\\nB GENERALISATION OF VERYDEEPFEATURES\\nIn the previous sections we have discussed training and eval uation of very deep ConvNets on the\\nILSVRC dataset. In this section, we evaluate our ConvNets, p re-trained on ILSVRC, as feature\\n11\\nPublishedasa conferencepaperat ICLR2015\\nTable 10: Comparison with the state of the art in ILSVRC localisation . Our methodis denoted\\nas“VGG”.\\nMethod top-5val. error (%) top-5 testerror (%)\\nVGG 26.9 25.3\\nGoogLeNet (Szegedyet al., 2014) - 26.7\\nOverFeat (Sermanet etal.,2014) 30.0 29.9\\nKrizhevsky et al.(Krizhevsky et al.,2012) - 34.2',\n",
       " 'Krizhevsky et al.(Krizhevsky et al.,2012) - 34.2\\nextractors on other, smaller, datasets, where training lar ge models from scratch is not feasible due\\nto over-ﬁtting. Recently, there has been a lot of interest in such a use case (Zeiler&Fergus, 2013;\\nDonahueet al., 2013; Razavianet al., 2014; Chatﬁeldet al., 2014), as it turns out that deep image\\nrepresentations,learntonILSVRC,generalisewelltoothe rdatasets,wheretheyhaveoutperformed\\nhand-crafted representations by a large margin. Following that line of work, we investigate if our\\nmodelsleadtobetterperformancethanmoreshallowmodelsu tilisedinthestate-of-the-artmethods.\\nIn this evaluation, we consider two models with the best clas siﬁcation performance on ILSVRC\\n(Sect.4)–conﬁgurations“Net-D”and“Net-E”(whichwemade publiclyavailable).\\nTo utilise the ConvNets, pre-trained on ILSVRC, for image cl assiﬁcation on other datasets, we',\n",
       " 'To utilise the ConvNets, pre-trained on ILSVRC, for image cl assiﬁcation on other datasets, we\\nremove the last fully-connected layer (which performs 1000 -way ILSVRC classiﬁcation), and use\\n4096-Dactivationsofthepenultimatelayerasimagefeatur es,whichareaggregatedacrossmultiple\\nlocations and scales. The resulting image descriptor is L2-normalised and combined with a linear\\nSVM classiﬁer, trained on the target dataset. For simplicit y, pre-trained ConvNet weights are kept\\nﬁxed(noﬁne-tuningisperformed).\\nAggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure\\n(Sect. 3.2). Namely, an image is ﬁrst rescaled so that its sma llest side equals Q, and then the network is densely applied over the image plane (which is possib le when all weight layers are treated\\nas convolutional). We then perform global average pooling o n the resulting feature map, which\\nproducesa 4096-Dimage descriptor. The descriptor is then a veraged with the descriptor of a horizontally ﬂipped image. As was shown in Sect. 4.2, evaluation over multiple scales is beneﬁcial, so',\n",
       " 'we extract features over several scales Q. The resulting multi-scale features can be either stacked\\nor pooled across scales. Stacking allows a subsequent class iﬁer to learn how to optimally combine\\nimage statistics over a range of scales; this, however, come s at the cost of the increased descriptor\\ndimensionality. We returntothediscussionofthisdesignc hoicein theexperimentsbelow. We also\\nassess late fusion of features, computed using two networks , which is performed by stacking their\\nrespectiveimagedescriptors.\\nTable11: Comparisonwiththestateoftheartinimageclassiﬁcationo nVOC-2007,VOC-2012,\\nCaltech-101, and Caltech-256 . Our models are denoted as “VGG”. Results marked with * were\\nachievedusingConvNetspre-trainedonthe extended ILSVRCdataset(2000classes).\\nMethodVOC-2007 VOC-2012 Caltech-101 Caltech-256\\n(meanAP) (mean AP) (meanclass recall) (mean class recall)\\nZeiler& Fergus (Zeiler&Fergus, 2013) - 79.0 86.5±0.5 74.2±0.3',\n",
       " 'Zeiler& Fergus (Zeiler&Fergus, 2013) - 79.0 86.5±0.5 74.2±0.3\\nChatﬁeldetal. (Chatﬁeldet al., 2014) 82.4 83.2 88.4±0.6 77.6±0.1\\nHe etal. (Heet al.,2014) 82.4 - 93.4±0.5 Weiet al.(Weiet al., 2014) 81.5(85.2∗)81.7 (90.3∗) - VGGNet-D (16layers) 89.3 89.0 91.8±1.0 85.0±0.2\\nVGGNet-E(19 layers) 89.3 89.0 92.3±0.5 85.1±0.3\\nVGGNet-D & Net-E 89.7 89.3 92.7±0.5 86.2±0.3\\nImage Classiﬁcation on VOC-2007and VOC-2012. We beginwith the evaluationon the image\\nclassiﬁcation task of PASCAL VOC-2007 and VOC-2012 benchma rks (Everinghametal., 2015).',\n",
       " 'classiﬁcation task of PASCAL VOC-2007 and VOC-2012 benchma rks (Everinghametal., 2015).\\nThese datasets contain 10K and 22.5K images respectively, a nd each image is annotated with one\\nor several labels, correspondingto 20 object categories. T he VOC organisersprovidea pre-deﬁned\\nsplit into training, validation, and test data (the test dat a for VOC-2012 is not publicly available;\\ninstead,anofﬁcialevaluationserverisprovided). Recogn itionperformanceismeasuredusingmean\\naverageprecision(mAP)acrossclasses.\\nNotably, by examining the performance on the validation set s of VOC-2007 and VOC-2012, we\\nfoundthat aggregatingimage descriptors,computedat mult iple scales, by averagingperformssim12\\nPublishedasa conferencepaperat ICLR2015\\nilarly to the aggregation by stacking. We hypothesize that t his is due to the fact that in the VOC',\n",
       " 'Publishedasa conferencepaperat ICLR2015\\nilarly to the aggregation by stacking. We hypothesize that t his is due to the fact that in the VOC\\ndataset the objects appear over a variety of scales, so there is no particular scale-speciﬁc semantics which a classiﬁer could exploit. Since averaging has a b eneﬁt of not inﬂating the descriptor dimensionality, we were able to aggregated image descri ptors over a wide range of scales:\\nQ∈ {256,384,512,640,768}. It is worth noting though that the improvement over a smalle r\\nrangeof{256,384,512}wasrathermarginal( 0.3%).\\nThetestsetperformanceisreportedandcomparedwithother approachesinTable11. Ournetworks\\n“Net-D”and“Net-E”exhibitidenticalperformanceonVOCda tasets,andtheircombinationslightly\\nimproves the results. Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, outperformingthe previousb est result of Chatﬁeldet al. (2014) by\\nmore than 6%. It should be noted that the method of Wei et al. (2014), which achieves1%better',\n",
       " 'more than 6%. It should be noted that the method of Wei et al. (2014), which achieves1%better\\nmAP on VOC-2012, is pre-trained on an extended 2000-class IL SVRC dataset, which includes\\nadditional 1000 categories, semantically close to those in VOC datasets. It also beneﬁts from the\\nfusionwith anobjectdetection-assistedclassiﬁcation pi peline.\\nImageClassiﬁcationonCaltech-101andCaltech-256. InthissectionweevaluateverydeepfeaturesonCaltech-101(Fei-Feiet al.,2004)andCaltech-256 (Grifﬁnet al.,2007)imageclassiﬁcation\\nbenchmarks. Caltech-101contains9Kimageslabelledinto1 02classes(101objectcategoriesanda\\nbackgroundclass), while Caltech-256 is larger with 31K ima ges and 257 classes. A standard evaluation protocolon these datasets is to generateseveral ran domsplits into training and test data and\\nreport the average recognition performance across the spli ts, which is measured by the mean class\\nrecall(whichcompensatesforadifferentnumberoftestima gesperclass). FollowingChatﬁeld etal.',\n",
       " 'recall(whichcompensatesforadifferentnumberoftestima gesperclass). FollowingChatﬁeld etal.\\n(2014); Zeiler&Fergus(2013); He etal. (2014),onCaltech- 101we generated3 randomsplits into\\ntraining and test data, so that each split contains 30 traini ng images per class, and up to 50 test\\nimages per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training\\nimages per class (and the rest is used for testing). In each sp lit, 20% of training images were used\\nasa validationset forhyper-parameterselection.\\nWe found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or max-pooling. This can be explained by the fact that\\nin Caltech images objects typically occupy the whole image, so multi-scale image features are semanticallydifferent(capturingthe wholeobject vs. object parts), andstacking allows a classiﬁer to\\nexploitsuchscale-speciﬁcrepresentations. We usedthree scalesQ∈ {256,384,512}.\\nOurmodelsarecomparedtoeachotherandthestateofthearti nTable11. Ascanbeseen,thedeeper',\n",
       " 'Ourmodelsarecomparedtoeachotherandthestateofthearti nTable11. Ascanbeseen,thedeeper\\n19-layerNet-Eperformsbetterthanthe16-layerNet-D,and theircombinationfurtherimprovesthe\\nperformance. On Caltech-101, our representations are comp etitive with the approach of He etal.\\n(2014),which,however,performssigniﬁcantlyworsethano urnetsonVOC-2007. OnCaltech-256,\\nourfeaturesoutperformthestate oftheart (Chatﬁeldetal. , 2014)byalargemargin( 8.6%).\\nAction Classiﬁcation on VOC-2012. We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classiﬁcation\\ntask (Everinghamet al., 2015), which consists in predictin g an action class from a single image,\\ngiven a bounding box of the person performing the action. The dataset contains 4.6K training images,labelledinto11classes. SimilarlytotheVOC-2012ob jectclassiﬁcationtask,theperformance',\n",
       " 'is measured using the mAP. We considered two training settin gs: (i) computing the ConvNet featuresonthewholeimageandignoringtheprovidedboundingb ox;(ii)computingthefeaturesonthe\\nwholeimageandontheprovidedboundingbox,andstackingth emtoobtaintheﬁnalrepresentation.\\nTheresultsarecomparedtootherapproachesinTable12.\\nOurrepresentationachievesthestateofartontheVOCactio nclassiﬁcationtaskevenwithoutusing\\nthe provided bounding boxes, and the results are further imp roved when using both images and\\nbounding boxes. Unlike other approaches, we did not incorpo rate any task-speciﬁc heuristics, but\\nreliedontherepresentationpowerofverydeepconvolution alfeatures.\\nOther Recognition Tasks. Since the public release of our models, they have been active ly used\\nby the research community for a wide range of image recogniti on tasks, consistently outperforming more shallow representations. For instance, Girshicke t al. (2014) achieve the state of the\\nobject detection results by replacing the ConvNet of Krizhe vskyet al. (2012) with our 16-layer',\n",
       " 'object detection results by replacing the ConvNet of Krizhe vskyet al. (2012) with our 16-layer\\nmodel. Similar gains over a more shallow architecture of Kri zhevskyet al. (2012) have been ob13\\nPublishedasa conferencepaperat ICLR2015\\nTable 12: Comparison with the state of the art in single-image action c lassiﬁcation on VOC2012. Our models are denoted as “VGG”. Results marked with * were a chieved using ConvNets\\npre-trainedonthe extended ILSVRCdataset (1512classes).\\nMethod VOC-2012 (meanAP)\\n(Oquab et al., 2014) 70.2∗\\n(Gkioxari etal.,2014) 73.6\\n(Hoai,2014) 76.3\\nVGG Net-D& Net-E,image-only 79.2\\nVGG Net-D& Net-E,image and bounding box 84.0\\nserved in semantic segmentation (Longet al., 2014), image c aption generation (Kirosetal., 2014;\\nKarpathy& Fei-Fei, 2014),textureandmaterialrecognitio n(Cimpoiet al., 2014; Bell etal., 2014).\\nC PAPERREVISIONS',\n",
       " 'C PAPERREVISIONS\\nHere we present the list of major paper revisions, outlining the substantial changes for the convenienceofthe reader.\\nv1Initialversion. Presentstheexperimentscarriedoutbefo rethe ILSVRCsubmission.\\nv2Addspost-submissionILSVRCexperimentswithtrainingset augmentationusingscalejittering,\\nwhichimprovestheperformance.\\nv3Addsgeneralisationexperiments(AppendixB) on PASCAL VOC andCaltech image classiﬁcationdatasets. Themodelsusedforthese experimentsarepub liclyavailable.\\nv4The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple\\ncropsforclassiﬁcation.\\nv6Camera-readyICLR-2015conferencepaper. Addsa compariso nof the net B with a shallow net\\nandtheresultsonPASCAL VOCactionclassiﬁcationbenchmar k.\\n14',\n",
       " 'Fully Convolutional Networks for Semantic Segmentation\\nJonathan Long\\x03Evan Shelhamer\\x03Trevor Darrell\\nUC Berkeley\\nfjonlong,shelhamer,trevor g@cs.berkeley.edu\\nAbstract\\nConvolutional networks are powerful visual models that\\nyield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional”\\nnetworks that take input of arbitrary size and produce\\ncorrespondingly-sized output with efﬁcient inference and\\nlearning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense\\nprediction tasks, and draw connections to prior models. We\\nadapt contemporary classiﬁcation networks (AlexNet [19],\\nthe VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations\\nby ﬁne-tuning [4] to the segmentation task. We then deﬁne a novel architecture that combines semantic information from a deep, coarse layer with appearance information\\nfrom a shallow, ﬁne layer to produce accurate and detailed',\n",
       " 'from a shallow, ﬁne layer to produce accurate and detailed\\nsegmentations. Our fully convolutional network achieves\\nstate-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2,\\nand SIFT Flow, while inference takes less than one ﬁfth of a\\nsecond for a typical image.\\n1. Introduction\\nConvolutional networks are driving advances in recognition. Convnets are not only improving for whole-image\\nclassiﬁcation [19, 31, 32], but also making progress on local tasks with structured output. These include advances in\\nbounding box object detection [29, 12, 17], part and keypoint prediction [39, 24], and local correspondence [24, 9].\\nThe natural next step in the progression from coarse to\\nﬁne inference is to make a prediction at every pixel. Prior\\napproaches have used convnets for semantic segmentation\\n[27, 2, 8, 28, 16, 14, 11], in which each pixel is labeled with\\nthe class of its enclosing object or region, but with shortcomings that this work addresses.\\n\\x03Authors contributed equally\\n963842564096409621\\n21backward/learningforward/inference\\npixelwise predictionsegmentation g.t.',\n",
       " '\\x03Authors contributed equally\\n963842564096409621\\n21backward/learningforward/inference\\npixelwise predictionsegmentation g.t.\\n256384Figure 1. Fully convolutional networks can efﬁciently learn to\\nmake dense predictions for per-pixel tasks like semantic segmentation.\\nWe show that a fully convolutional network (FCN),\\ntrained end-to-end, pixels-to-pixels on semantic segmentation exceeds the state-of-the-art without further machinery. To our knowledge, this is the ﬁrst work to train FCNs\\nend-to-end (1) for pixelwise prediction and (2) from supervised pre-training. Fully convolutional versions of existing\\nnetworks predict dense outputs from arbitrary-sized inputs.\\nBoth learning and inference are performed whole-image-ata-time by dense feedforward computation and backpropagation. In-network upsampling layers enable pixelwise prediction and learning in nets with subsampled pooling.\\nThis method is efﬁcient, both asymptotically and absolutely, and precludes the need for the complications in other\\nworks. Patchwise training is common [27, 2, 8, 28, 11], but',\n",
       " 'works. Patchwise training is common [27, 2, 8, 28, 11], but\\nlacks the efﬁciency of fully convolutional training. Our approach does not make use of pre- and post-processing complications, including superpixels [8, 16], proposals [16, 14],\\nor post-hoc reﬁnement by random ﬁelds or local classiﬁers\\n[8, 16]. Our model transfers recent success in classiﬁcation [19, 31, 32] to dense prediction by reinterpreting classiﬁcation nets as fully convolutional and ﬁne-tuning from\\ntheir learned representations. In contrast, previous works\\nhave applied small convnets without supervised pre-training\\n[8, 28, 27].\\nSemantic segmentation faces an inherent tension between semantics and location: global information resolves\\nwhat while local information resolves where. Deep feature\\n1arXiv:1411.4038v2  [cs.CV]  8 Mar 2015\\nhierarchies jointly encode location and semantics in a localto-global pyramid. We deﬁne a novel “skip” architecture\\nto combine deep, coarse, semantic information and shallow,\\nﬁne, appearance information in Section 4.2 (see Figure 3).',\n",
       " 'to combine deep, coarse, semantic information and shallow,\\nﬁne, appearance information in Section 4.2 (see Figure 3).\\nIn the next section, we review related work on deep classiﬁcation nets, FCNs, and recent approaches to semantic\\nsegmentation using convnets. The following sections explain FCN design and dense prediction tradeoffs, introduce\\nour architecture with in-network upsampling and multilayer combinations, and describe our experimental framework. Finally, we demonstrate state-of-the-art results on\\nPASCAL VOC 2011-2, NYUDv2, and SIFT Flow.\\n2. Related work\\nOur approach draws on recent successes of deep nets\\nfor image classiﬁcation [19, 31, 32] and transfer learning\\n[4, 38]. Transfer was ﬁrst demonstrated on various visual\\nrecognition tasks [4, 38], then on detection, and on both\\ninstance and semantic segmentation in hybrid proposalclassiﬁer models [12, 16, 14]. We now re-architect and ﬁnetune classiﬁcation nets to direct, dense prediction of semantic segmentation. We chart the space of FCNs and situate\\nprior models, both historical and recent, in this framework.\\nFully convolutional networks To our knowledge, the',\n",
       " 'prior models, both historical and recent, in this framework.\\nFully convolutional networks To our knowledge, the\\nidea of extending a convnet to arbitrary-sized inputs ﬁrst\\nappeared in Matan et al. [25], which extended the classic\\nLeNet [21] to recognize strings of digits. Because their net\\nwas limited to one-dimensional input strings, Matan et al.\\nused Viterbi decoding to obtain their outputs. Wolf and Platt\\n[37] expand convnet outputs to 2-dimensional maps of detection scores for the four corners of postal address blocks.\\nBoth of these historical works do inference and learning\\nfully convolutionally for detection. Ning et al. [27] deﬁne\\na convnet for coarse multiclass segmentation of C. elegans\\ntissues with fully convolutional inference.\\nFully convolutional computation has also been exploited\\nin the present era of many-layered nets. Sliding window\\ndetection by Sermanet et al . [29], semantic segmentation\\nby Pinheiro and Collobert [28], and image restoration by\\nEigen et al. [5] do fully convolutional inference. Fully convolutional training is rare, but used effectively by Tompson\\net al. [35] to learn an end-to-end part detector and spatial',\n",
       " 'et al. [35] to learn an end-to-end part detector and spatial\\nmodel for pose estimation, although they do not exposit on\\nor analyze this method.\\nAlternatively, He et al . [17] discard the nonconvolutional portion of classiﬁcation nets to make a\\nfeature extractor. They combine proposals and spatial\\npyramid pooling to yield a localized, ﬁxed-length feature\\nfor classiﬁcation. While fast and effective, this hybrid\\nmodel cannot be learned end-to-end.\\nDense prediction with convnets Several recent works\\nhave applied convnets to dense prediction problems, including semantic segmentation by Ning et al. [27], Farabet et al.[8], and Pinheiro and Collobert [28]; boundary prediction\\nfor electron microscopy by Ciresan et al. [2] and for natural\\nimages by a hybrid neural net/nearest neighbor model by\\nGanin and Lempitsky [11]; and image restoration and depth\\nestimation by Eigen et al. [5, 6]. Common elements of these\\napproaches include\\n\\x0fsmall models restricting capacity and receptive ﬁelds;\\n\\x0fpatchwise training [27, 2, 8, 28, 11];\\n\\x0fpost-processing by superpixel projection, random ﬁeld',\n",
       " '\\x0fpatchwise training [27, 2, 8, 28, 11];\\n\\x0fpost-processing by superpixel projection, random ﬁeld\\nregularization, ﬁltering, or local classiﬁcation [8, 2,\\n11];\\n\\x0finput shifting and output interlacing for dense output\\n[28, 11] as introduced by OverFeat [29];\\n\\x0fmulti-scale pyramid processing [8, 28, 11];\\n\\x0fsaturating tanh nonlinearities [8, 5, 28]; and\\n\\x0fensembles [2, 11],\\nwhereas our method does without this machinery. However,\\nwe do study patchwise training 3.4 and “shift-and-stitch”\\ndense output 3.2 from the perspective of FCNs. We also\\ndiscuss in-network upsampling 3.3, of which the fully connected prediction by Eigen et al. [6] is a special case.\\nUnlike these existing methods, we adapt and extend deep\\nclassiﬁcation architectures, using image classiﬁcation as supervised pre-training, and ﬁne-tune fully convolutionally to\\nlearn simply and efﬁciently from whole image inputs and\\nwhole image ground thruths.',\n",
       " 'learn simply and efﬁciently from whole image inputs and\\nwhole image ground thruths.\\nHariharan et al. [16] and Gupta et al. [14] likewise adapt\\ndeep classiﬁcation nets to semantic segmentation, but do\\nso in hybrid proposal-classiﬁer models. These approaches\\nﬁne-tune an R-CNN system [12] by sampling bounding\\nboxes and/or region proposals for detection, semantic segmentation, and instance segmentation. Neither method is\\nlearned end-to-end.\\nThey achieve state-of-the-art results on PASCAL VOC\\nsegmentation and NYUDv2 segmentation respectively, so\\nwe directly compare our standalone, end-to-end FCN to\\ntheir semantic segmentation results in Section 5.\\n3. Fully convolutional networks\\nEach layer of data in a convnet is a three-dimensional\\narray of size h\\x02w\\x02d, wherehandware spatial dimensions, anddis the feature or channel dimension. The ﬁrst\\nlayer is the image, with pixel size h\\x02w, anddcolor channels. Locations in higher layers correspond to the locations\\nin the image they are path-connected to, which are called\\ntheir receptive ﬁelds .',\n",
       " 'in the image they are path-connected to, which are called\\ntheir receptive ﬁelds .\\nConvnets are built on translation invariance. Their basic components (convolution, pooling, and activation functions) operate on local input regions, and depend only on\\nrelative spatial coordinates. Writing xijfor the data vector\\nat location (i;j)in a particular layer, and yijfor the following layer, these functions compute outputs yijby\\nyij=fks(fxsi+\\x0ei;sj +\\x0ejg0\\x14\\x0ei;\\x0ej\\x14k)\\nwherekis called the kernel size, sis the stride or subsampling factor, and fksdetermines the layer type: a matrix\\nmultiplication for convolution or average pooling, a spatial\\nmax for max pooling, or an elementwise nonlinearity for an\\nactivation function, and so on for other types of layers.\\nThis functional form is maintained under composition,\\nwith kernel size and stride obeying the transformation rule\\nfks\\x0egk0s0= (f\\x0eg)k0+(k\\x001)s0;ss0:\\nWhile a general deep net computes a general nonlinear\\nfunction, a net with only layers of this form computes a',\n",
       " 'While a general deep net computes a general nonlinear\\nfunction, a net with only layers of this form computes a\\nnonlinear ﬁlter , which we call a deep ﬁlter orfully convolutional network . An FCN naturally operates on an input of\\nany size, and produces an output of corresponding (possibly\\nresampled) spatial dimensions.\\nA real-valued loss function composed with an FCN deﬁnes a task. If the loss function is a sum over the spatial\\ndimensions of the ﬁnal layer, `(x;\\x12) =P\\nij`0(xij;\\x12), its\\ngradient will be a sum over the gradients of each of its spatial components. Thus stochastic gradient descent on `computed on whole images will be the same as stochastic gradient descent on `0, taking all of the ﬁnal layer receptive ﬁelds\\nas a minibatch.\\nWhen these receptive ﬁelds overlap signiﬁcantly, both\\nfeedforward computation and backpropagation are much\\nmore efﬁcient when computed layer-by-layer over an entire\\nimage instead of independently patch-by-patch.\\nWe next explain how to convert classiﬁcation nets into\\nfully convolutional nets that produce coarse output maps.',\n",
       " 'image instead of independently patch-by-patch.\\nWe next explain how to convert classiﬁcation nets into\\nfully convolutional nets that produce coarse output maps.\\nFor pixelwise prediction, we need to connect these coarse\\noutputs back to the pixels. Section 3.2 describes a trick that\\nOverFeat [29] introduced for this purpose. We gain insight\\ninto this trick by reinterpreting it as an equivalent network\\nmodiﬁcation. As an efﬁcient, effective alternative, we introduce deconvolution layers for upsampling in Section 3.3.\\nIn Section 3.4 we consider training by patchwise sampling,\\nand give evidence in Section 4.3 that our whole image training is faster and equally effective.\\n3.1. Adapting classiﬁers for dense prediction\\nTypical recognition nets, including LeNet [21], AlexNet\\n[19], and its deeper successors [31, 32], ostensibly take\\nﬁxed-sized inputs and produce nonspatial outputs. The fully\\nconnected layers of these nets have ﬁxed dimensions and\\nthrow away spatial coordinates. However, these fully connected layers can also be viewed as convolutions with kernels that cover their entire input regions. Doing so casts\\nthem into fully convolutional networks that take input of\\nany size and output classiﬁcation maps. This transformation',\n",
       " 'them into fully convolutional networks that take input of\\nany size and output classiﬁcation maps. This transformation\\n`tabby cat\" `\\n96256384384256409640961000\\n96384256409640961000\\n256384tabby cat heatmapconvolutionalizationFigure 2. Transforming fully connected layers into convolution\\nlayers enables a classiﬁcation net to output a heatmap. Adding\\nlayers and a spatial loss (as in Figure 1) produces an efﬁcient machine for end-to-end dense learning.\\nis illustrated in Figure 2. (By contrast, nonconvolutional\\nnets, such as the one by Le et al. [20], lack this capability.)\\nFurthermore, while the resulting maps are equivalent to\\nthe evaluation of the original net on particular input patches,\\nthe computation is highly amortized over the overlapping\\nregions of those patches. For example, while AlexNet takes\\n1:2ms (on a typical GPU) to produce the classiﬁcation\\nscores of a 227\\x02227image, the fully convolutional version takes 22ms to produce a 10\\x0210grid of outputs from\\na500\\x02500image, which is more than 5times faster than\\nthe na ¨ıve approach1.',\n",
       " 'a500\\x02500image, which is more than 5times faster than\\nthe na ¨ıve approach1.\\nThe spatial output maps of these convolutionalized models make them a natural choice for dense problems like semantic segmentation. With ground truth available at every output cell, both the forward and backward passes are\\nstraightforward, and both take advantage of the inherent\\ncomputational efﬁciency (and aggressive optimization) of\\nconvolution.\\nThe corresponding backward times for the AlexNet example are 2:4ms for a single image and 37ms for a fully\\nconvolutional 10\\x0210output map, resulting in a speedup\\nsimilar to that of the forward pass. This dense backpropagation is illustrated in Figure 1.\\nWhile our reinterpretation of classiﬁcation nets as fully\\nconvolutional yields output maps for inputs of any size, the\\noutput dimensions are typically reduced by subsampling.\\nThe classiﬁcation nets subsample to keep ﬁlters small and\\ncomputational requirements reasonable. This coarsens the\\noutput of a fully convolutional version of these nets, reducing it from the size of the input by a factor equal to the pixel\\nstride of the receptive ﬁelds of the output units.\\n1Assuming efﬁcient batching of single image inputs. The classiﬁcation',\n",
       " 'stride of the receptive ﬁelds of the output units.\\n1Assuming efﬁcient batching of single image inputs. The classiﬁcation\\nscores for a single image by itself take 5.4 ms to produce, which is nearly\\n25times slower than the fully convolutional version.\\n3.2. Shift-and-stitch is ﬁlter rarefaction\\nInput shifting and output interlacing is a trick that yields\\ndense predictions from coarse outputs without interpolation, introduced by OverFeat [29]. If the outputs are downsampled by a factor of f, the input is shifted (by left and top\\npadding)xpixels to the right and ypixels down, once for\\nevery value of (x;y)2f0;:::;f\\x001g\\x02f 0;:::;f\\x001g.\\nThesef2inputs are each run through the convnet, and the\\noutputs are interlaced so that the predictions correspond to\\nthe pixels at the centers of their receptive ﬁelds.\\nChanging only the ﬁlters and layer strides of a convnet\\ncan produce the same output as this shift-and-stitch trick.\\nConsider a layer (convolution or pooling) with input stride',\n",
       " 'can produce the same output as this shift-and-stitch trick.\\nConsider a layer (convolution or pooling) with input stride\\ns, and a following convolution layer with ﬁlter weights fij\\n(eliding the feature dimensions, irrelevant here). Setting the\\nlower layer’s input stride to 1upsamples its output by a factor ofs, just like shift-and-stitch. However, convolving the\\noriginal ﬁlter with the upsampled output does not produce\\nthe same result as the trick, because the original ﬁlter only\\nsees a reduced portion of its (now upsampled) input. To\\nreproduce the trick, rarefy the ﬁlter by enlarging it as\\nf0\\nij=\\x1afi=s;j=s ifsdivides both iandj;\\n0 otherwise,\\n(withiandjzero-based). Reproducing the full net output\\nof the trick involves repeating this ﬁlter enlargement layerby-layer until all subsampling is removed.\\nSimply decreasing subsampling within a net is a tradeoff:\\nthe ﬁlters see ﬁner information, but have smaller receptive\\nﬁelds and take longer to compute. We have seen that the\\nshift-and-stitch trick is another kind of tradeoff: the output',\n",
       " 'ﬁelds and take longer to compute. We have seen that the\\nshift-and-stitch trick is another kind of tradeoff: the output\\nis made denser without decreasing the receptive ﬁeld sizes\\nof the ﬁlters, but the ﬁlters are prohibited from accessing\\ninformation at a ﬁner scale than their original design.\\nAlthough we have done preliminary experiments with\\nshift-and-stitch, we do not use it in our model. We ﬁnd\\nlearning through upsampling, as described in the next section, to be more effective and efﬁcient, especially when\\ncombined with the skip layer fusion described later on.\\n3.3. Upsampling is backwards strided convolution\\nAnother way to connect coarse outputs to dense pixels\\nis interpolation. For instance, simple bilinear interpolation\\ncomputes each output yijfrom the nearest four inputs by a\\nlinear map that depends only on the relative positions of the\\ninput and output cells.\\nIn a sense, upsampling with factor fis convolution with\\nafractional input stride of 1=f. So long as fis integral, a\\nnatural way to upsample is therefore backwards convolution\\n(sometimes called deconvolution ) with an output stride of\\nf. Such an operation is trivial to implement, since it simply',\n",
       " 'natural way to upsample is therefore backwards convolution\\n(sometimes called deconvolution ) with an output stride of\\nf. Such an operation is trivial to implement, since it simply\\nreverses the forward and backward passes of convolution.Thus upsampling is performed in-network for end-to-end\\nlearning by backpropagation from the pixelwise loss.\\nNote that the deconvolution ﬁlter in such a layer need not\\nbe ﬁxed (e.g., to bilinear upsampling), but can be learned.\\nA stack of deconvolution layers and activation functions can\\neven learn a nonlinear upsampling.\\nIn our experiments, we ﬁnd that in-network upsampling\\nis fast and effective for learning dense prediction. Our best\\nsegmentation architecture uses these layers to learn to upsample for reﬁned prediction in Section 4.2.\\n3.4. Patchwise training is loss sampling\\nIn stochastic optimization, gradient computation is\\ndriven by the training distribution. Both patchwise training and fully-convolutional training can be made to produce any distribution, although their relative computational\\nefﬁciency depends on overlap and minibatch size. Whole\\nimage fully convolutional training is identical to patchwise\\ntraining where each batch consists of all the receptive ﬁelds',\n",
       " 'image fully convolutional training is identical to patchwise\\ntraining where each batch consists of all the receptive ﬁelds\\nof the units below the loss for an image (or collection of\\nimages). While this is more efﬁcient than uniform sampling\\nof patches, it reduces the number of possible batches. However, random selection of patches within an image may be\\nrecovered simply. Restricting the loss to a randomly sampled subset of its spatial terms (or, equivalently applying a\\nDropConnect mask [36] between the output and the loss)\\nexcludes patches from the gradient computation.\\nIf the kept patches still have signiﬁcant overlap, fully\\nconvolutional computation will still speed up training. If\\ngradients are accumulated over multiple backward passes,\\nbatches can include patches from several images.2\\nSampling in patchwise training can correct class imbalance [27, 8, 2] and mitigate the spatial correlation of dense\\npatches [28, 16]. In fully convolutional training, class balance can also be achieved by weighting the loss, and loss\\nsampling can be used to address spatial correlation.\\nWe explore training with sampling in Section 4.3, and do\\nnot ﬁnd that it yields faster or better convergence for dense\\nprediction. Whole image training is effective and efﬁcient.\\n4. Segmentation Architecture',\n",
       " 'not ﬁnd that it yields faster or better convergence for dense\\nprediction. Whole image training is effective and efﬁcient.\\n4. Segmentation Architecture\\nWe cast ILSVRC classiﬁers into FCNs and augment\\nthem for dense prediction with in-network upsampling and\\na pixelwise loss. We train for segmentation by ﬁne-tuning.\\nNext, we build a novel skip architecture that combines\\ncoarse, semantic and local, appearance information to reﬁne prediction.\\nFor this investigation, we train and validate on the PASCAL VOC 2011 segmentation challenge [7]. We train with\\n2Note that not every possible patch is included this way, since the receptive ﬁelds of the ﬁnal layer units lie on a ﬁxed, strided grid. However,\\nby shifting the image left and down by a random value up to the stride,\\nrandom selection from all possible patches may be recovered.\\na per-pixel multinomial logistic loss and validate with the\\nstandard metric of mean pixel intersection over union, with\\nthe mean taken over all classes, including background. The\\ntraining ignores pixels that are masked out (as ambiguous\\nor difﬁcult) in the ground truth.\\n4.1. From classiﬁer to dense FCN',\n",
       " 'training ignores pixels that are masked out (as ambiguous\\nor difﬁcult) in the ground truth.\\n4.1. From classiﬁer to dense FCN\\nWe begin by convolutionalizing proven classiﬁcation architectures as in Section 3. We consider the AlexNet3architecture [19] that won ILSVRC12, as well as the VGG\\nnets [31] and the GoogLeNet4[32] which did exceptionally well in ILSVRC14. We pick the VGG 16-layer net5,\\nwhich we found to be equivalent to the 19-layer net on this\\ntask. For GoogLeNet, we use only the ﬁnal loss layer, and\\nimprove performance by discarding the ﬁnal average pooling layer. We decapitate each net by discarding the ﬁnal\\nclassiﬁer layer, and convert all fully connected layers to\\nconvolutions. We append a 1\\x021convolution with channel dimension 21to predict scores for each of the PASCAL classes (including background) at each of the coarse\\noutput locations, followed by a deconvolution layer to bilinearly upsample the coarse outputs to pixel-dense outputs\\nas described in Section 3.3. Table 1 compares the preliminary validation results along with the basic characteristics',\n",
       " 'as described in Section 3.3. Table 1 compares the preliminary validation results along with the basic characteristics\\nof each net. We report the best results achieved after convergence at a ﬁxed learning rate (at least 175 epochs).\\nFine-tuning from classiﬁcation to segmentation gave reasonable predictions for each net. Even the worst model\\nachieved\\x1875% of state-of-the-art performance. The\\nsegmentation-equippped VGG net (FCN-VGG16) already\\nappears to be state-of-the-art at 56.0 mean IU on val, compared to 52.6 on test [16]. Training on extra data raises\\nperformance to 59.4 mean IU on a subset of val7. Training\\ndetails are given in Section 4.3.\\nDespite similar classiﬁcation accuracy, our implementation of GoogLeNet did not match this segmentation result.\\n4.2. Combining what and where\\nWe deﬁne a new fully convolutional net (FCN) for segmentation that combines layers of the feature hierarchy and\\nreﬁnes the spatial precision of the output. See Figure 3.\\nWhile fully convolutionalized classiﬁers can be ﬁnetuned to segmentation as shown in 4.1, and even score',\n",
       " 'While fully convolutionalized classiﬁers can be ﬁnetuned to segmentation as shown in 4.1, and even score\\nhighly on the standard metric, their output is dissatisfyingly\\ncoarse (see Figure 4). The 32 pixel stride at the ﬁnal prediction layer limits the scale of detail in the upsampled output.\\nWe address this by adding links that combine the ﬁnal\\nprediction layer with lower layers with ﬁner strides. This\\n3Using the publicly available CaffeNet reference model.\\n4Since there is no publicly available version of GoogLeNet, we use\\nour own reimplementation. Our version is trained with less extensive data\\naugmentation, and gets 68.5% top-1 and 88.4% top-5 ILSVRC accuracy.\\n5Using the publicly available version from the Caffe model zoo.Table 1. We adapt and extend three classiﬁcation convnets to segmentation. We compare performance by mean intersection over\\nunion on the validation set of PASCAL VOC 2011 and by inference time (averaged over 20 trials for a 500\\x02500 input on an\\nNVIDIA Tesla K40c). We detail the architecture of the adapted\\nnets as regards dense prediction: number of parameter layers, receptive ﬁeld size of output units, and the coarsest stride within the',\n",
       " 'nets as regards dense prediction: number of parameter layers, receptive ﬁeld size of output units, and the coarsest stride within the\\nnet. (These numbers give the best performance obtained at a ﬁxed\\nlearning rate, not best performance possible.)\\nFCNAlexNetFCNVGG16FCNGoogLeNet4\\nmean IU 39.8 56.0 42.5\\nforward time 50 ms 210 ms 59 ms\\nconv. layers 8 16 22\\nparameters 57M 134M 6M\\nrf size 355 404 907\\nmax stride 32 32 32\\nturns a line topology into a DAG, with edges that skip ahead\\nfrom lower layers to higher ones (Figure 3). As they see\\nfewer pixels, the ﬁner scale predictions should need fewer\\nlayers, so it makes sense to make them from shallower net\\noutputs. Combining ﬁne layers and coarse layers lets the\\nmodel make local predictions that respect global structure.\\nBy analogy to the multiscale local jet of Florack et al. [10],\\nwe call our nonlinear local feature hierarchy the deep jet .\\nWe ﬁrst divide the output stride in half by predicting\\nfrom a 16 pixel stride layer. We add a 1\\x021convolution\\nlayer on top of pool4 to produce additional class predictions. We fuse this output with the predictions computed',\n",
       " 'from a 16 pixel stride layer. We add a 1\\x021convolution\\nlayer on top of pool4 to produce additional class predictions. We fuse this output with the predictions computed\\non top of conv7 (convolutionalized fc7) at stride 32 by\\nadding a 2\\x02upsampling layer and summing6both predictions. (See Figure 3). We initialize the 2\\x02upsampling to\\nbilinear interpolation, but allow the parameters to be learned\\nas described in Section 3.3. Finally, the stride 16 predictions\\nare upsampled back to the image. We call this net FCN-16s.\\nFCN-16s is learned end-to-end, initialized with the parameters of the last, coarser net, which we now call FCN-32s.\\nThe new parameters acting on pool4 are zero-initialized so\\nthat the net starts with unmodiﬁed predictions. The learning\\nrate is decreased by a factor of 100.\\nLearning this skip net improves performance on the validation set by 3.0 mean IU to 62.4. Figure 4 shows improvement in the ﬁne structure of the output. We compared\\nthis fusion with learning only from the pool4 layer (which\\nresulted in poor performance), and simply decreasing the\\nlearning rate without adding the extra link (which results',\n",
       " 'this fusion with learning only from the pool4 layer (which\\nresulted in poor performance), and simply decreasing the\\nlearning rate without adding the extra link (which results\\nin an insigniﬁcant performance improvement, without improving the quality of the output).\\nWe continue in this fashion by fusing predictions from\\npool3 with a 2\\x02upsampling of predictions fused from\\npool4 andconv7 , building the net FCN-8s. We obtain\\n6Max fusion made learning difﬁcult due to gradient switching.\\nFCN-32s FCN-16s FCN-8s Ground truth\\nFigure 4. Reﬁning fully convolutional nets by fusing information\\nfrom layers with different strides improves segmentation detail.\\nThe ﬁrst three images show the output from our 32, 16, and 8\\npixel stride nets (see Figure 3).\\nTable 2. Comparison of skip FCNs on a subset of PASCAL\\nVOC2011 validation7. Learning is end-to-end, except for FCN32s-ﬁxed, where only the last layer is ﬁne-tuned. Note that FCN32s is FCN-VGG16, renamed to highlight stride.\\npixel\\nacc.mean\\nacc.mean\\nIUf.w.\\nIU',\n",
       " 'pixel\\nacc.mean\\nacc.mean\\nIUf.w.\\nIU\\nFCN-32s-ﬁxed 83.0 59.7 45.4 72.0\\nFCN-32s 89.1 73.3 59.4 81.4\\nFCN-16s 90.0 75.7 62.4 83.0\\nFCN-8s 90.3 75.9 62.7 83.2\\na minor additional improvement to 62.7 mean IU, and ﬁnd\\na slight improvement in the smoothness and detail of our\\noutput. At this point our fusion improvements have met diminishing returns, both with respect to the IU metric which\\nemphasizes large-scale correctness, and also in terms of the\\nimprovement visible e.g. in Figure 4, so we do not continue\\nfusing even lower layers.\\nReﬁnement by other means Decreasing the stride of\\npooling layers is the most straightforward way to obtain\\nﬁner predictions. However, doing so is problematic for our\\nVGG16-based net. Setting the pool5 layer to have stride 1\\nrequires our convolutionalized fc6 to have a kernel size of14\\x0214in order to maintain its receptive ﬁeld size. In addition to their computational cost, we had difﬁculty learning',\n",
       " 'such large ﬁlters. We made an attempt to re-architect the\\nlayers above pool5 with smaller ﬁlters, but were not successful in achieving comparable performance; one possible\\nexplanation is that the initialization from ImageNet-trained\\nweights in the upper layers is important.\\nAnother way to obtain ﬁner predictions is to use the shiftand-stitch trick described in Section 3.2. In limited experiments, we found the cost to improvement ratio from this\\nmethod to be worse than layer fusion.\\n4.3. Experimental framework\\nOptimization We train by SGD with momentum. We\\nuse a minibatch size of 20 images and ﬁxed learning rates of\\n10\\x003,10\\x004, and 5\\x005for FCN-AlexNet, FCN-VGG16, and\\nFCN-GoogLeNet, respectively, chosen by line search. We\\nuse momentum 0:9, weight decay of 5\\x004or2\\x004, and doubled the learning rate for biases, although we found training\\nto be insensitive to these parameters (but sensitive to the\\nlearning rate). We zero-initialize the class scoring convolution layer, ﬁnding random initialization to yield neither\\nbetter performance nor faster convergence. Dropout was included where used in the original classiﬁer nets.',\n",
       " 'better performance nor faster convergence. Dropout was included where used in the original classiﬁer nets.\\nFine-tuning We ﬁne-tune all layers by backpropagation through the whole net. Fine-tuning the\\noutput classiﬁer alone yields only 70% of the full ﬁnetuning performance as compared in Table 2. Training from\\nscratch is not feasible considering the time required to\\nlearn the base classiﬁcation nets. (Note that the VGG net is\\ntrained in stages, while we initialize from the full 16-layer\\nversion.) Fine-tuning takes three days on a single GPU for\\nthe coarse FCN-32s version, and about one day each to\\nupgrade to the FCN-16s and FCN-8s versions.\\nPatch Sampling As explained in Section 3.4, our full\\nimage training effectively batches each image into a reguimage pool4 pool5 pool1 pool2 pool332x upsampled\\nprediction (FCN-32s)2x upsampled\\nprediction16x upsampled\\nprediction (FCN-16s)8x upsampled\\nprediction (FCN-8s)\\npool4\\nprediction2x upsampled\\nprediction\\npool3\\npredictionP P',\n",
       " 'prediction (FCN-8s)\\npool4\\nprediction2x upsampled\\nprediction\\npool3\\npredictionP P\\nFigure 3. Our DAG nets learn to combine coarse, high layer information with ﬁne, low layer information. Layers are shown as grids that\\nreveal relative spatial coarseness. Only pooling and prediction layers are shown; intermediate convolution layers (including our converted\\nfully connected layers) are omitted. Solid line (FCN-32s): Our single-stream net, described in Section 4.1, upsamples stride 32 predictions\\nback to pixels in a single step. Dashed line (FCN-16s): Combining predictions from both the ﬁnal layer and the pool4 layer, at stride\\n16, lets our net predict ﬁner details, while retaining high-level semantic information. Dotted line (FCN-8s): Additional predictions from\\npool3 , at stride 8, provide further precision.\\n500 1000 1500\\niteration number0.40.60.81.01.2lossfull images\\n50% sampling\\n25% sampling\\n10000 20000 30000\\nrelative time (num. images processed)0.40.60.81.01.2lossFigure 5. Training on whole images is just as effective as sampling\\npatches, but results in faster (wall time) convergence by making',\n",
       " 'patches, but results in faster (wall time) convergence by making\\nmore efﬁcient use of data. Left shows the effect of sampling on\\nconvergence rate for a ﬁxed expected batch size, while right plots\\nthe same by relative wall time.\\nlar grid of large, overlapping patches. By contrast, prior\\nwork randomly samples patches over a full dataset [27, 2,\\n8, 28, 11], potentially resulting in higher variance batches\\nthat may accelerate convergence [22]. We study this tradeoff by spatially sampling the loss in the manner described\\nearlier, making an independent choice to ignore each ﬁnal\\nlayer cell with some probability 1\\x00p. To avoid changing the\\neffective batch size, we simultaneously increase the number\\nof images per batch by a factor 1=p. Note that due to the efﬁciency of convolution, this form of rejection sampling is\\nstill faster than patchwise training for large enough values\\nofp(e.g., at least for p > 0:2according to the numbers\\nin Section 3.1). Figure 5 shows the effect of this form of\\nsampling on convergence. We ﬁnd that sampling does not\\nhave a signiﬁcant effect on convergence rate compared to\\nwhole image training, but takes signiﬁcantly more time due',\n",
       " 'have a signiﬁcant effect on convergence rate compared to\\nwhole image training, but takes signiﬁcantly more time due\\nto the larger number of images that need to be considered\\nper batch. We therefore choose unsampled, whole image\\ntraining in our other experiments.\\nClass Balancing Fully convolutional training can balance classes by weighting or sampling the loss. Although\\nour labels are mildly unbalanced (about 3=4are background), we ﬁnd class balancing unnecessary.\\nDense Prediction The scores are upsampled to the input dimensions by deconvolution layers within the net. Final layer deconvolutional ﬁlters are ﬁxed to bilinear interpolation, while intermediate upsampling layers are initialized to bilinear upsampling, and then learned. Shift-andstitch (Section 3.2), or the ﬁlter rarefaction equivalent, are\\nnot used.\\nAugmentation We tried augmenting the training data\\nby randomly mirroring and “jittering” the images by translating them up to 32 pixels (the coarsest scale of prediction)\\nin each direction. This yielded no noticeable improvement.\\nMore Training Data The PASCAL VOC 2011 segmentation challenge training set, which we used for Table 1,',\n",
       " 'in each direction. This yielded no noticeable improvement.\\nMore Training Data The PASCAL VOC 2011 segmentation challenge training set, which we used for Table 1,\\nlabels 1112 images. Hariharan et al . [15] have collectedlabels for a much larger set of 8498 PASCAL training images, which was used to train the previous state-of-the-art\\nsystem, SDS [16]. This training data improves the FCNVGG16 validation score7by 3.4 points to 59.4 mean IU.\\nImplementation All models are trained and tested with\\nCaffe [18] on a single NVIDIA Tesla K40c. The models\\nand code will be released open-source on publication.\\n5. Results\\nWe test our FCN on semantic segmentation and scene\\nparsing, exploring PASCAL VOC, NYUDv2, and SIFT\\nFlow. Although these tasks have historically distinguished\\nbetween objects and regions, we treat both uniformly as\\npixel prediction. We evaluate our FCN skip architecture8\\non each of these datasets, and then extend it to multi-modal\\ninput for NYUDv2 and multi-task prediction for the semantic and geometric labels of SIFT Flow.\\nMetrics We report four metrics from common semantic\\nsegmentation and scene parsing evaluations that are variations on pixel accuracy and region intersection over union',\n",
       " 'Metrics We report four metrics from common semantic\\nsegmentation and scene parsing evaluations that are variations on pixel accuracy and region intersection over union\\n(IU). Letnijbe the number of pixels of class ipredicted to\\nbelong to class j, where there are ncldifferent classes, and\\nletti=P\\njnijbe the total number of pixels of class i. We\\ncompute:\\n\\x0fpixel accuracy:P\\ninii=P\\niti\\n\\x0fmean accuraccy: (1=ncl)P\\ninii=ti\\n\\x0fmean IU: (1=ncl)P\\ninii=\\x10\\nti+P\\njnji\\x00nii\\x11\\n\\x0ffrequency weighted IU:\\n(P\\nktk)\\x001P\\nitinii=\\x10\\nti+P\\njnji\\x00nii\\x11\\nPASCAL VOC Table 3 gives the performance of our\\nFCN-8s on the test sets of PASCAL VOC 2011 and 2012,\\nand compares it to the previous state-of-the-art, SDS [16],\\nand the well-known R-CNN [12]. We achieve the best results on mean IU9by a relative margin of 20%. Inference\\ntime is reduced 114\\x02(convnet only, ignoring proposals and\\nreﬁnement) or 286\\x02(overall).',\n",
       " 'time is reduced 114\\x02(convnet only, ignoring proposals and\\nreﬁnement) or 286\\x02(overall).\\nTable 3. Our fully convolutional net gives a 20% relative improvement over the state-of-the-art on the PASCAL VOC 2011 and 2012\\ntest sets, and reduces inference time.\\nmean IU mean IU inference\\nVOC2011 test VOC2012 test time\\nR-CNN [12] 47.9 - SDS [16] 52.6 51.6 \\x1850 s\\nFCN-8s 62.7 62.2 \\x18175 ms\\nNYUDv2 [30] is an RGB-D dataset collected using the\\n7There are training images from [15] included in the PASCAL VOC\\n2011 val set, so we validate on the non-intersecting set of 736 images. An\\nearlier version of this paper mistakenly evaluated on the entire val set.\\n8Our models and code are publicly available at\\nhttps://github.com/BVLC/caffe/wiki/Model-Zoo#fcn .\\n9This is the only metric provided by the test server.\\nTable 4. Results on NYUDv2. RGBD is early-fusion of the\\nRGB and depth channels at the input. HHA is the depth embedding of [14] as horizontal disparity, height above ground, and',\n",
       " 'RGB and depth channels at the input. HHA is the depth embedding of [14] as horizontal disparity, height above ground, and\\nthe angle of the local surface normal with the inferred gravity\\ndirection. RGB-HHA is the jointly trained late fusion model\\nthat sums RGB and HHA predictions.\\npixel\\nacc.mean\\nacc.mean\\nIUf.w.\\nIU\\nGupta et al. [14] 60.3 - 28.6 47.0\\nFCN-32s RGB 60.0 42.2 29.2 43.9\\nFCN-32s RGBD 61.5 42.4 30.5 45.5\\nFCN-32s HHA 57.1 35.2 24.2 40.4\\nFCN-32s RGB-HHA 64.3 44.9 32.8 48.0\\nFCN-16s RGB-HHA 65.4 46.1 34.0 49.5\\nMicrosoft Kinect. It has 1449 RGB-D images, with pixelwise labels that have been coalesced into a 40 class semantic segmentation task by Gupta et al. [13]. We report results\\non the standard split of 795 training images and 654 testing\\nimages. (Note: all model selection is performed on PASCAL 2011 val.) Table 4 gives the performance of our model',\n",
       " 'on the standard split of 795 training images and 654 testing\\nimages. (Note: all model selection is performed on PASCAL 2011 val.) Table 4 gives the performance of our model\\nin several variations. First we train our unmodiﬁed coarse\\nmodel (FCN-32s) on RGB images. To add depth information, we train on a model upgraded to take four-channel\\nRGB-D input (early fusion). This provides little beneﬁt,\\nperhaps due to the difﬁcultly of propagating meaningful\\ngradients all the way through the model. Following the success of Gupta et al. [14], we try the three-dimensional HHA\\nencoding of depth, training nets on just this information, as\\nwell as a “late fusion” of RGB and HHA where the predictions from both nets are summed at the ﬁnal layer, and the\\nresulting two-stream net is learned end-to-end. Finally we\\nupgrade this late fusion net to a 16-stride version.\\nSIFT Flow is a dataset of 2,688 images with pixel labels\\nfor 33 semantic categories (“bridge”, “mountain”, “sun”),',\n",
       " 'SIFT Flow is a dataset of 2,688 images with pixel labels\\nfor 33 semantic categories (“bridge”, “mountain”, “sun”),\\nas well as three geometric categories (“horizontal”, “vertical”, and “sky”). An FCN can naturally learn a joint representation that simultaneously predicts both types of labels.\\nWe learn a two-headed version of FCN-16s with semantic and geometric prediction layers and losses. The learned\\nmodel performs as well on both tasks as two independently\\ntrained models, while learning and inference are essentially\\nas fast as each independent model by itself. The results in\\nTable 5, computed on the standard split into 2,488 training\\nand 200 test images,10show state-of-the-art performance on\\nboth tasks.\\n10Three of the SIFT Flow categories are not present in the test set. We\\nmade predictions across all 33 categories, but only included categories actually present in the test set in our evaluation. (An earlier version of this paper reported a lower mean IU, which included all categories either present\\nor predicted in the evaluation.)Table 5. Results on SIFT Flow10with class segmentation\\n(center) and geometric segmentation (right). Tighe [33] is\\na non-parametric transfer method. Tighe 1 is an exemplar',\n",
       " '(center) and geometric segmentation (right). Tighe [33] is\\na non-parametric transfer method. Tighe 1 is an exemplar\\nSVM while 2 is SVM + MRF. Farabet is a multi-scale convnet trained on class-balanced samples (1) or natural frequency\\nsamples (2). Pinheiro is a multi-scale, recurrent convnet, denoted RCNN 3(\\x0e3). The metric for geometry is pixel accuracy.\\npixel\\nacc.mean\\nacc.mean\\nIUf.w.\\nIUgeom.\\nacc.\\nLiuet al. [23] 76.7 - - - Tighe et al. [33] - - - - 90.8\\nTighe et al. [34] 1 75.6 41.1 - - Tighe et al. [34] 2 78.6 39.2 - - Farabet et al. [8] 1 72.3 50.8 - - Farabet et al. [8] 2 78.5 29.6 - - Pinheiro et al. [28] 77.7 29.8 - - FCN-16s 85.2 51.7 39.5 76.1 94.3\\nFCN-8s SDS [16] Ground Truth Image',\n",
       " 'FCN-8s SDS [16] Ground Truth Image\\nFigure 6. Fully convolutional segmentation nets produce stateof-the-art performance on PASCAL. The left column shows the\\noutput of our highest performing net, FCN-8s. The second shows\\nthe segmentations produced by the previous state-of-the-art system\\nby Hariharan et al. [16]. Notice the ﬁne structures recovered (ﬁrst\\nrow), ability to separate closely interacting objects (second row),\\nand robustness to occluders (third row). The fourth row shows a\\nfailure case: the net sees lifejackets in a boat as people.\\n6. Conclusion\\nFully convolutional networks are a rich class of models, of which modern classiﬁcation convnets are a special case. Recognizing this, extending these classiﬁcation\\nnets to segmentation, and improving the architecture with\\nmulti-resolution layer combinations dramatically improves\\nthe state-of-the-art, while simultaneously simplifying and\\nspeeding up learning and inference.\\nAcknowledgements This work was supported in part\\nby DARPA’s MSEE and SMISC programs, NSF awards IIS1427425, IIS-1212798, IIS-1116411, and the NSF GRFP,',\n",
       " 'Toyota, and the Berkeley Vision and Learning Center. We\\ngratefully acknowledge NVIDIA for GPU donation. We\\nthank Bharath Hariharan and Saurabh Gupta for their advice and dataset tools. We thank Sergio Guadarrama for\\nreproducing GoogLeNet in Caffe. We thank Jitendra Malik\\nfor his helpful comments. Thanks to Wei Liu for pointing\\nout an issue wth our SIFT Flow mean IU computation and\\nan error in our frequency weighted mean IU formula.\\nA. Upper Bounds on IU\\nIn this paper, we have achieved good performance on\\nthe mean IU segmentation metric even with coarse semantic\\nprediction. To better understand this metric and the limits\\nof this approach with respect to it, we compute approximate\\nupper bounds on performance with prediction at various\\nscales. We do this by downsampling ground truth images\\nand then upsampling them again to simulate the best results\\nobtainable with a particular downsampling factor. The following table gives the mean IU on a subset of PASCAL\\n2011 val for various downsampling factors.\\nfactor mean IU\\n128 50.9\\n64 73.3\\n32 86.1\\n16 92.8\\n896.4\\n498.5\\nPixel-perfect prediction is clearly not necessary to',\n",
       " '128 50.9\\n64 73.3\\n32 86.1\\n16 92.8\\n896.4\\n498.5\\nPixel-perfect prediction is clearly not necessary to\\nachieve mean IU well above state-of-the-art, and, conversely, mean IU is a not a good measure of ﬁne-scale accuracy.\\nB. More Results\\nWe further evaluate our FCN for semantic segmentation.\\nPASCAL-Context [26] provides whole scene annotations of PASCAL VOC 2010. While there are over 400 distinct classes, we follow the 59 class task deﬁned by [26] that\\npicks the most frequent classes. We train and evaluate on\\nthe training and val sets respectively. In Table 6, we compare to the joint object + stuff variation of Convolutional\\nFeature Masking [3] which is the previous state-of-the-art\\non this task. FCN-8s scores 35.1 mean IU for an 11% relative improvement.\\nChangelog\\nThe arXiv version of this paper is kept up-to-date with\\ncorrections and additional relevant material. The following\\ngives a brief history of changes.Table 6. Results on PASCAL-Context. CFM is the best result of\\n[3] by convolutional feature masking and segment pursuit with the',\n",
       " '[3] by convolutional feature masking and segment pursuit with the\\nVGG net. O2Pis the second order pooling method [1] as reported\\nin the errata of [26]. The 59 class task includes the 59 most frequent classes while the 33 class task consists of an easier subset\\nidentiﬁed by [26].\\n59 classpixel\\nacc.mean\\nacc.mean\\nIUf.w.\\nIU\\nO2P - - 18.1 CFM - - 31.5 FCN-32s 63.8 42.7 31.8 48.3\\nFCN-16s 65.7 46.2 34.8 50.7\\nFCN-8s 65.9 46.5 35.1 51.0\\n33 class\\nO2P - - 29.2 CFM - - 46.1 FCN-32s 69.8 65.1 50.4 54.9\\nFCN-16s 71.8 68.0 53.4 57.5\\nFCN-8s 71.8 67.6 53.5 57.7\\nv2Add Appendix A giving upper bounds on mean IU and\\nAppendix B with PASCAL-Context results. Correct PASCAL validation numbers (previously, some val images were\\nincluded in train), SIFT Flow mean IU (which used an inappropriately strict metric), and an error in the frequency',\n",
       " 'included in train), SIFT Flow mean IU (which used an inappropriately strict metric), and an error in the frequency\\nweighted mean IU formula. Add link to models and update\\ntiming numbers to reﬂect improved implementation (which\\nis publicly available).\\nReferences\\n[1] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV ,\\n2012. 9\\n[2] D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Deep neural networks segment neuronal membranes\\nin electron microscopy images. In NIPS , pages 2852–2860,\\n2012. 1, 2, 4, 7\\n[3] J. Dai, K. He, and J. Sun. Convolutional feature masking for joint object and stuff segmentation. arXiv preprint\\narXiv:1412.1283 , 2014. 9\\n[4] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML , 2014.\\n1, 2',\n",
       " 'E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML , 2014.\\n1, 2\\n[5] D. Eigen, D. Krishnan, and R. Fergus. Restoring an image\\ntaken through a window covered with dirt or rain. In Computer Vision (ICCV), 2013 IEEE International Conference\\non, pages 633–640. IEEE, 2013. 2\\n[6] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction\\nfrom a single image using a multi-scale deep network. arXiv\\npreprint arXiv:1406.2283 , 2014. 2\\n[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman. The PASCAL Visual Object Classes\\nChallenge 2011 (VOC2011) Results. http://www.pascalnetwork.org/challenges/VOC/voc2011/workshop/index.html.\\n4\\n[8] C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\\nhierarchical features for scene labeling. Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on , 2013. 1, 2, 4,\\n7, 8',\n",
       " 'hierarchical features for scene labeling. Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on , 2013. 1, 2, 4,\\n7, 8\\n[9] P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor matching\\nwith convolutional neural networks: a comparison to SIFT.\\nCoRR , abs/1405.5769, 2014. 1\\n[10] L. Florack, B. T. H. Romeny, M. Viergever, and J. Koenderink. The gaussian scale-space paradigm and the multiscale local jet. International Journal of Computer Vision ,\\n18(1):61–75, 1996. 5\\n[11] Y . Ganin and V . Lempitsky. N4-ﬁelds: Neural network nearest neighbor ﬁelds for image transforms. In ACCV , 2014. 1,\\n2, 7\\n[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic\\nsegmentation. In Computer Vision and Pattern Recognition ,\\n2014. 1, 2, 7\\n[13] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organization\\nand recognition of indoor scenes from RGB-D images. In',\n",
       " '[13] S. Gupta, P. Arbelaez, and J. Malik. Perceptual organization\\nand recognition of indoor scenes from RGB-D images. In\\nCVPR , 2013. 8\\n[14] S. Gupta, R. Girshick, P. Arbelaez, and J. Malik. Learning\\nrich features from RGB-D images for object detection and\\nsegmentation. In ECCV . Springer, 2014. 1, 2, 8\\n[15] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.\\nSemantic contours from inverse detectors. In International\\nConference on Computer Vision (ICCV) , 2011. 7\\n[16] B. Hariharan, P. Arbel ´aez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In European Conference on Computer Vision (ECCV) , 2014. 1, 2, 4, 5, 7, 8\\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\\nin deep convolutional networks for visual recognition. In\\nECCV , 2014. 1, 2',\n",
       " 'in deep convolutional networks for visual recognition. In\\nECCV , 2014. 1, 2\\n[18] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093 , 2014. 7\\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nNIPS , 2012. 1, 2, 3, 5\\n[20] Q. V . Le, R. Monga, M. Devin, K. Chen, G. S. Corrado,\\nJ. Dean, and A. Y . Ng. Building high-level features using\\nlarge scale unsupervised learning. In ICML , 2012. 3\\n[21] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard,\\nW. Hubbard, and L. D. Jackel. Backpropagation applied to\\nhand-written zip code recognition. In Neural Computation ,\\n1989. 2, 3',\n",
       " 'W. Hubbard, and L. D. Jackel. Backpropagation applied to\\nhand-written zip code recognition. In Neural Computation ,\\n1989. 2, 3\\n[22] Y . A. LeCun, L. Bottou, G. B. Orr, and K.-R. M ¨uller. Efﬁcient backprop. In Neural networks: Tricks of the trade ,\\npages 9–48. Springer, 1998. 7\\n[23] C. Liu, J. Yuen, and A. Torralba. Sift ﬂow: Dense correspondence across scenes and its applications. Pattern Analysis\\nand Machine Intelligence, IEEE Transactions on , 33(5):978–\\n994, 2011. 8[24] J. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence? In NIPS , 2014. 1\\n[25] O. Matan, C. J. Burges, Y . LeCun, and J. S. Denker. Multidigit recognition using a space displacement neural network.\\nInNIPS , pages 488–495. Citeseer, 1991. 2',\n",
       " 'InNIPS , pages 488–495. Citeseer, 1991. 2\\n[26] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object\\ndetection and semantic segmentation in the wild. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE\\nConference on , pages 891–898. IEEE, 2014. 9\\n[27] F. Ning, D. Delhomme, Y . LeCun, F. Piano, L. Bottou, and\\nP. E. Barbano. Toward automatic phenotyping of developing\\nembryos from videos. Image Processing, IEEE Transactions\\non, 14(9):1360–1371, 2005. 1, 2, 4, 7\\n[28] P. H. Pinheiro and R. Collobert. Recurrent convolutional\\nneural networks for scene labeling. In ICML , 2014. 1, 2,\\n4, 7, 8\\n[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localization',\n",
       " 'and Y . LeCun. Overfeat: Integrated recognition, localization\\nand detection using convolutional networks. In ICLR , 2014.\\n1, 2, 3, 4\\n[30] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor\\nsegmentation and support inference from rgbd images. In\\nECCV , 2012. 7\\n[31] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR ,\\nabs/1409.1556, 2014. 1, 2, 3, 5\\n[32] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.\\nGoing deeper with convolutions. CoRR , abs/1409.4842,\\n2014. 1, 2, 3, 5\\n[33] J. Tighe and S. Lazebnik. Superparsing: scalable nonparametric image parsing with superpixels. In ECCV , pages 352–\\n365. Springer, 2010. 8\\n[34] J. Tighe and S. Lazebnik. Finding things: Image parsing with',\n",
       " '365. Springer, 2010. 8\\n[34] J. Tighe and S. Lazebnik. Finding things: Image parsing with\\nregions and per-exemplar detectors. In CVPR , 2013. 8\\n[35] J. Tompson, A. Jain, Y . LeCun, and C. Bregler. Joint training\\nof a convolutional network and a graphical model for human\\npose estimation. CoRR , abs/1406.2984, 2014. 2\\n[36] L. Wan, M. Zeiler, S. Zhang, Y . L. Cun, and R. Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine\\nLearning (ICML-13) , pages 1058–1066, 2013. 4\\n[37] R. Wolf and J. C. Platt. Postal address block location using\\na convolutional locator network. Advances in Neural Information Processing Systems , pages 745–745, 1994. 2\\n[38] M. D. Zeiler and R. Fergus. Visualizing and understanding\\nconvolutional networks. In Computer Vision–ECCV 2014 ,\\npages 818–833. Springer, 2014. 2',\n",
       " 'convolutional networks. In Computer Vision–ECCV 2014 ,\\npages 818–833. Springer, 2014. 2\\n[39] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Partbased r-cnns for ﬁne-grained category detection. In Computer Vision–ECCV 2014 , pages 834–849. Springer, 2014.\\n1',\n",
       " 'Deep Filter Banks for Texture Recognition and Segmentation\\nMircea Cimpoi\\nUniversity of Oxford\\nmircea@robots.ox.ac.ukSubhransu Maji\\nUniversity of Massachusetts, Amherst\\nsmaji@cs.umass.eduAndrea Vedaldi\\nUniversity of Oxford\\nvedaldi@robots.ox.ac.uk\\nAbstract\\nResearch in texture recognition often concentrates on the\\nproblem of material recognition in uncluttered conditions,\\nan assumption rarely met by applications. In this work we\\nconduct a ﬁrst study of material and describable texture attributes recognition in clutter, using a new dataset derived\\nfrom the OpenSurface texture repository. Motivated by the\\nchallenge posed by this problem, we propose a new texture\\ndescriptor, FV-CNN, obtained by Fisher Vector pooling of a\\nConvolutional Neural Network (CNN) ﬁlter bank. FV-CNN\\nsubstantially improves the state-of-the-art in texture, material and scene recognition. Our approach achieves 79.8%\\naccuracy on Flickr material dataset and 81% accuracy on\\nMIT indoor scenes, providing absolute gains of more than\\n10% over existing approaches. FV-CNN easily transfers\\nacross domains without requiring feature adaptation as for\\nmethods that build on the fully-connected layers of CNNs.',\n",
       " '10% over existing approaches. FV-CNN easily transfers\\nacross domains without requiring feature adaptation as for\\nmethods that build on the fully-connected layers of CNNs.\\nFurthermore, FV-CNN can seamlessly incorporate multiscale information and describe regions of arbitrary shapes\\nand sizes. Our approach is particularly suited at localizing “stuff” categories and obtains state-of-the-art results\\non MSRC segmentation dataset, as well as promising results\\non recognizing materials and surface attributes in clutter on\\nthe OpenSurfaces dataset.\\n1. Introduction\\nTexture is ubiquitous and provides useful cues of material properties of objects and their identity, especially when\\nshape is not useful. Hence, a signiﬁcant amount of effort in\\nthe computer vision community has gone into recognizing\\ntexture via tasks such as texture perception [1, 2, 12, 13]\\nand description [7, 11], material recognition [26, 36, 37],\\nsegmentation [20, 29], and even synthesis [9, 43].\\nPerhaps the most studied task in texture understanding\\nis the one of material recognition, as captured in benchmarks such as CuRET [8], KTH-TIPS [5], and, more recently, FMD [38]. However, while at least the FMD dataset\\ncontains images collected from the Internet, vividly dubbed',\n",
       " 'contains images collected from the Internet, vividly dubbed\\n“images in the wild”, all these datasets make the simplifying\\nbanded\\n blotchy\\n chequered\\n grid\\nmarbled\\n paisley\\n paisley\\n wrinkled\\nbrick\\n ceramic\\n carpet\\n fabric\\nFigure 1. Texture recognition in clutter . Example of top retrieved texture segments by attributes (top two rows) and materials\\n(bottom) in the OpenSurfaces dataset.\\nassumption that textures ﬁll images. Thus, they are not necessarily representative of the signiﬁcantly harder problem\\nof recognising materials in natural images, where textures\\nappear in clutter. Building on a recent dataset collected by\\nthe computer graphics community, the ﬁrst contribution of\\nthis paper is a large-scale analysis of material and perceptual texture attribute recognition and segmentation in clutter(Fig. 1 and Sect. 2).\\nMotivated by the challenge posed by recognising texture\\nin clutter, we develop a new texture descriptor. In the simplest terms a texture is characterized by the arrangement of\\nlocal patterns, as captured in early works [26, 41] by the\\ndistribution of local “ﬁlter bank” responses. These ﬁlter\\nbanks were designed to capture edges, spots and bars at\\ndifferent scales and orientations. Typical combinations of',\n",
       " 'banks were designed to capture edges, spots and bars at\\ndifferent scales and orientations. Typical combinations of\\nthe ﬁlter responses, identiﬁed by vector quantisation, were\\nused as the computational basis of the “textons” proposed\\nby Julesz [22]. Texton distributions were the early versions\\nof “bag-of-words” representations, a dominant approach in\\nrecognition in the early 2000s, since then improved by new\\npooling schemes such as soft-assignment [27, 42, 48] and\\nFisher Vectors (FVs) [32]. Until recently, FV with SIFT\\nfeatures [28] as a local representation was the state-of-theart method for recognition, not only for textures, but for\\nobjects and scenes too.\\nLater, however, Convolutional Neural Networks (CNNs)\\n1arXiv:1411.6836v2  [cs.CV]  9 Jul 2015\\nhave emerged as the new state-of-the-art for recognition, exempliﬁed by remarkable results in image classiﬁcation [23],\\ndetection [14] and segmentation [16] on a number of widely\\nused benchmarks. Key to their success is the ability to leverage large labelled datasets to learn increasingly complex',\n",
       " 'detection [14] and segmentation [16] on a number of widely\\nused benchmarks. Key to their success is the ability to leverage large labelled datasets to learn increasingly complex\\ntransformations of the input to capture invariances. Importantly, CNNs pre-trained on such large datasets have been\\nshown [6, 14, 30] to contain general-purpose feature extractors, transferrable to many other domains.\\nDomain transfer in CNNs is usually achieved by using\\nas features the output of a deep, fully-connected layer of\\nthe network. From the perspective of textures, however,\\nthis choice has three drawbacks. The ﬁrst one (I) is that,\\nwhile the convolutional layers are akin to non-linear ﬁlter\\nbanks, the fully connected layers capture their spatial layout. While this may be useful for representing the shape of\\nan object, it may not be as useful for representing texture. A\\nsecond drawback (II) is that the input to the CNN has to be\\nof ﬁxed size to be compatible with the fully connected layers, which requires an expensive resizing of the input image,\\nparticularly when features are computed for many different\\nregions [14, 15]. A third and more subtle drawback (III) is\\nthat deeper layers may be more domain-speciﬁc and therefore potentially less transferrable than shallower layers.',\n",
       " 'that deeper layers may be more domain-speciﬁc and therefore potentially less transferrable than shallower layers.\\nThe second contribution of this paper is FV-CNN\\n(Sect. 3), a pooling method that overcomes these drawbacks . The idea is to regard the convolutional layers of a\\nCNN as a ﬁlter bank and build an orderless representation\\nusing FV as a pooling mechanism, as is commonly done\\nin the bag-of-words approaches. Although the suggested\\nchange is simple, the approach is remarkably ﬂexible and\\neffective. First, pooling is orderless and multi-scale, hence\\nsuitable for textures. Second, any image size can be processed by convolutional layers, avoiding costly resizing operations. Third, convolutional ﬁlters, pooled by FV-CNN,\\nare shown to transfer more easily than fully-connected ones\\neven without ﬁne-tuning. While other authors [15, 18] have\\nrecently proposed alternative pooling strategies for CNNs,\\nwe show that our method is more natural, faster and often\\nsigniﬁcantly more accurate.\\nThethird contribution of the paper is a thorough evaluation of these descriptors on a variety of benchmarks, from\\ntextures to objects (Sect. 4). In textures, we evaluate material and describable attributes recognition and segmentation',\n",
       " 'textures to objects (Sect. 4). In textures, we evaluate material and describable attributes recognition and segmentation\\non new datasets derived from OpenSurfaces (Sect. 2). When\\nused with linear SVMs, FV-CNN improves the state of the\\nart on texture recognition by a signiﬁcant margin. Like textures, scenes are also weakly structured and a bag-of-words\\nrepresentation is effective. FV-CNN obtains 81.1% accuracy on the MIT indoor scenes dataset [34], signiﬁcantly\\noutperforming the current state-of-the-art of 70.8% [47].\\nWhat is remarkable is that, where [47] ﬁnds that CNNs\\ntrained on scene recognition data perform better than CNNs\\ntrained on an object domain (ImageNet), when used in FV-CNN not only is there an overall performance improvement, but the domain-speciﬁc advantage is entirely removed\\n(Tab. 3). This indicates that FV-CNN are in fact better at domain transfer. Our method also matches the previous best in\\nPASCAL VOC 2007 classiﬁcation dataset providing measurable boost over CNNs and closely approaches competitor methods on CUB 2010-2011 datasets when ground-truth\\nobject bounding boxes are given.\\nFV-CNN can be used for describing regions by simply',\n",
       " 'object bounding boxes are given.\\nFV-CNN can be used for describing regions by simply\\npooling across pixels within the region. Combined with\\na low-level segmentation algorithm this sufﬁces to localize textures within images. This approach is similar to a\\nrecently proposed method called “R-CNN” for localizing\\nobjects [14]. However, in contrast to it we do not need repeated evaluations of the CNN since the convolutional features can be computed just once for the entire image and\\npooled differently. This makes FV-CNN not only faster,\\nbut also as experiments suggest, much more accurate at\\ntexture localization. We achieve state of the art results on\\nthe MSRC segmentation dataset using a simple scheme of\\nclassifying “superpixels” obtaining an accuracy of 87.0%\\n(previous best 86.5%). The corresponding R-CNN obtains\\n57.7% accuracy. Segmentation results are promising in the\\nOpenSurfaces dataset [4] as well.\\nFinally, we analyze the utility of different network layers\\nand architectures as ﬁlter banks, concluding that: SIFT is\\ncompetitive only with the ﬁrst few layers of a CNN (Fig. 4)',\n",
       " 'and architectures as ﬁlter banks, concluding that: SIFT is\\ncompetitive only with the ﬁrst few layers of a CNN (Fig. 4)\\nand that signiﬁcant improvement to the underlying CNN architecture, such as the ones achieved by the very deep models of Simonyan and Zisserman [39], directly translate into\\nmuch better ﬁlter banks for texture recognition.\\n2. Texture recognition in clutter\\nA contribution of this work is the analysis of materials\\nand texture attributes in realistic imaging conditions. Earlier datasets such as KTH-TIPS were collected in controlled\\nconditions, which makes their applicability to natural images unclear. More recent datasets such as FMD and DTD\\nremove this limitation by building on images downloaded\\nfrom the Internet, dubbed images “in the wild”. However,\\nin these datasets texture always ﬁll the ﬁeld of view of the\\ncamera. In this paper we remove this limitation by experimenting for the ﬁrst time with a large dataset of textures\\ncollected in the wild and in cluttered conditions.\\nIn particular, we build on the Open Surfaces (OS) dataset\\nthat was recently introduced by Bell et al. [4] in computer\\ngraphics. OS comprises 25,357 images, each containing a\\nnumber of high-quality texture/material segments. Many of',\n",
       " 'graphics. OS comprises 25,357 images, each containing a\\nnumber of high-quality texture/material segments. Many of\\nthese segments are annotated with additional attributes such\\nas the material name, the viewpoint, the BRDF, and the object class. Not all segments have a complete set of annotations; the experiments in this paper focus on the 58,928 that\\ncontain material names. Since material classes are highly\\nunbalanced, only the materials that contain at least 400 examples are considered. This result in 53,915 annotated\\nmaterial segments in 10,422 images spanning 22 different\\nclasses.1Images are split evenly into training, validation,\\nand test subsets with 3,474 images each. Segment sizes are\\nhighly variable, with half of them being relatively small,\\nwith an area smaller than 64\\x0264pixels. While the lack\\nof exhaustive annotations makes it impossible to deﬁne a\\ncomplete background class, several less common materials\\n(including for example segments that annotators could not\\nassign to a material) are merged in an “other” class that acts\\nas pseudo-background.\\nIn order to study perceptual properties as well as materials, we augment the OS dataset with some of the 47\\nattributes from the DTD [7]. Since the OS segments do\\nnot trigger with sufﬁcient frequency all the 47 attributes,',\n",
       " 'attributes from the DTD [7]. Since the OS segments do\\nnot trigger with sufﬁcient frequency all the 47 attributes,\\nthe evaluation is restricted to eleven of them for which it\\nwas possible to identify at least 100 matching segments.2\\nThe attributes were manually labelled in the 53,915 segments retained for materials. We refer to this data as\\nOSA. The complete list of images, segments, labels, and\\nsplits are available at http://www.robots.ox.ac.\\nuk/˜vgg/data/wildtex/ .\\n3. Method\\nThis section describes the methodological contributions\\nof this paper: region description and segmentation.\\n3.1. Region description\\nThis section introduces a number of visual descriptors\\nsuitable to model the appearance of image regions. Texture\\nis traditionally described by orderless pooling of ﬁlter bank\\nresponses as, unlike in objects, the overall shape information is usually unimportant. However, small under-sampled\\ntextures may beneﬁt if recognized in the context of an object. Thus, the primacy of orderless pooling may not always\\nhold in the recognition of textures in natural conditions.\\nIn order to explore the interplay between shape and orderless pooling, we evaluate two corresponding region descriptors: FC-CNN for shape and FV-CNN for texture.',\n",
       " 'In order to explore the interplay between shape and orderless pooling, we evaluate two corresponding region descriptors: FC-CNN for shape and FV-CNN for texture.\\nBoth descriptors are based on the same CNN features [23]\\nobtained from an off-the-shelf CNN pre-trained on the ImageNet ILSVRC 2012 data as suggested in [6, 21, 35]. Since\\nthe underlying CNN is the same, it is meaningful to compare FC- and FV-CNN directly.\\n1The classes and corresponding number of example segments are:\\nbrick (610), cardboard (423), carpet/rug (1,975), ceramic (1,643), concrete (567), fabric/cloth (7,484), food (1,461), glass (4,571), granite/marble (1,596), hair (443), other (2,035), laminate (510), leather (957),\\nmetal (4,941), painted (7,870), paper/tissue (1,226), plastic/clear (586),\\nplastic/opaque (1,800), stone (417), tile (3,085), wallpaper (483), wood\\n(9,232).\\n2These are: banded, blotchy, chequered, ﬂecked, gauzy, grid, marbled,',\n",
       " '(9,232).\\n2These are: banded, blotchy, chequered, ﬂecked, gauzy, grid, marbled,\\npaisley, pleated, stratiﬁed, wrinkled.Object descriptor: FC-CNN. The FC-CNN descriptor is\\nobtained by extracting as features the output of the penultimate Fully-Connected (FC) layer of a CNN, including\\nthe non-linear gating function, applied to the input image.\\nThis can be considered an object descriptor because the\\nfully connected layers allow FC-CNN to capture the overall\\nshape of the object contained in the region. FC-CNN is applied to an image region R(which may be the whole image)\\nby warping the bounding box enclosing R(plus a 10% border) to a square of a ﬁxed size matching the default CNN\\ninput geometry, obtaining the same R-CNN descriptor introduced by Girshick et al. [14] as a state-of-the-art object\\ndetector in the PASCAL VOC [10] data.\\nTexture descriptor: FV-CNN. The FV-CNN descriptor\\nis inspired by the state-of-the-art texture descriptors of [7]\\nbased on the Fisher Vector (FV). Differently from FC-CNN,\\nFV pools local features densely within the described regions',\n",
       " 'based on the Fisher Vector (FV). Differently from FC-CNN,\\nFV pools local features densely within the described regions\\nremoving global spatial information , and is therefore more\\napt at describing textures than objects. Here FV is computed\\non the output of a single (last) convolutional layer of the\\nCNN, but we compared features from other layers as well\\n(Sect 4.4). By avoiding the computation of the fully connected layers, the input image does not need to be rescaled\\nto a speciﬁc size; in fact, the dense convolutional features\\nare extracted at multiple scales and pooled into a single FV\\njust like for SIFT. The pooled convolutional features are extracted immediately after the last linear ﬁltering operator\\nand are not otherwise normalised.\\nThe FV-CNN descriptor is related to the one proposed\\nby Gong et al. [15]. There VLAD pooling, which is similar to FV , is applied to FC-CNN-like descriptors extracted\\nfrom densely sampled image windows. A key difference of\\nFV-CNN is that dense features are extracted from the convolutional rather than fully-connected layers. This is more\\nnatural, signiﬁcantly more efﬁcient (as it does not require',\n",
       " 'natural, signiﬁcantly more efﬁcient (as it does not require\\nrecomputing the network for each extracted local descriptor) and, as shown in Sect. 4, more accurate.\\n3.2. Region segmentation\\nThis section discusses our method to automatically partition an image into a number of recognisable regions. Inspired by Cimpoi et al. [7] that successfully ported object\\ndescription methods to texture descriptors, here we propose\\na segmentation technique inspired by object detection. An\\nincreasingly popular method for object detection, followed\\nfor example by R-CNN [14], is to ﬁrst propose a number of\\ncandidate object regions using low-level image cues, and\\nthen verifying a shortlist of such regions using a powerful classiﬁer. Applied to textures, this requires a low-level\\nmechanism to generate textured region proposals, followed\\nby a region classiﬁer. A key advantage of this approach\\nis that it allows applying object- (FC-CNN) and texturelike (FV-CNN) descriptors alike. After proposal classiﬁcation, each pixel can be assigned more than one label; this is\\nsolved with simple voting schemes, also inspired by object\\ndetections methods.\\nThe paper explores two such region generation methods:',\n",
       " 'solved with simple voting schemes, also inspired by object\\ndetections methods.\\nThe paper explores two such region generation methods:\\nthe crisp regions of [19] and the multi-scale combinatorial\\ngrouping of [3]. In both cases, region proposals are generated using low-level image cues, such as colour or texture\\nconsistency, as speciﬁed by the original methods. It would\\nof course be possible to incorporate FC-CNN and FV-CNN\\namong these energy terms to potentially strengthen the region generation mechanism itself. However, this contradicts partially the logic of the scheme, which breaks down\\nthe problem into cheaply generating tentative segmentations and then verifying them using a more powerful (and\\nlikely expensive) model. Furthermore, and more importantly, these cues focus on separating texture instances , as\\npresented in each particular image, whereas FC-CNN and\\nFV-CNN are meant to identify a texture class. It is reasonable to expect instance-speciﬁc cues (say the colour of a\\npainted wall) to be better for segmentation.\\n4. Results\\nThis section evaluates the proposed region recognition\\nmethods for classifying and segmenting materials, describable texture properties, and higher-level object categories.\\nSect. 4.1 evaluates the classiﬁcation task by assessing how\\nwell regions can be classiﬁed given that their true extent is',\n",
       " 'Sect. 4.1 evaluates the classiﬁcation task by assessing how\\nwell regions can be classiﬁed given that their true extent is\\nknown and Sect. 4.3 evaluates both classiﬁcation and segmentation . The rest of the section introduces the evaluation\\nbenchmarks and technical details of the representations.\\nDatasets. The evaluation considers three texture recognition benchmarks other than OS (Sect. 2). The ﬁrst one is the\\nFlickr Material Dataset (FMD) [37], a recent benchmark\\ncontaining 10 material classes. The second one is the Describable Texture Datasets (DTD) [7], which contains texture images jointly annotated with 47 describable attributes\\ndrawn from the psychological literature. Both FMD and\\nDTD contain images “in the wild”, i.e. collected in uncontrolled conditions. However, differently from OS, these images are uncluttered. The third texture dataset is KTH-TIPS2b[5, 17], containing a number of example images for each\\nof four samples of 11 material categories. For each material, images of one sample are used for training and the\\nremaining for testing.\\nObject categorisation is evaluated in the PASCAL VOC\\n2007 [10] dataset, containing 20 object categories, any\\ncombination of which may be associated to any of the',\n",
       " 'Object categorisation is evaluated in the PASCAL VOC\\n2007 [10] dataset, containing 20 object categories, any\\ncombination of which may be associated to any of the\\nbenchmark images. Scene categorisation uses the MIT Indoor [34] dataset, containing 67 indoor scene classes. Finegrained categorisation uses the Caltech/UCSD Bird dataset\\n(CUB) [45], containing images of 200 bird species.\\nNote that some of these datasets come with ground truth\\nregion/object localisation. The +R sufﬁx will be appendedto a dataset to indicate that this information is used both at\\ntraining and testing time. For example, OS means that segmentation is performed automatically at test time, whereas\\nOS+R means that ground-truth segmentations are used.\\nEvaluation measures. For each dataset the corresponding standard evaluator protocols and accuracy measures are\\nused. In particular, for FMD, DTD, MIT Indoor, CUB,\\nand OS+R, evaluation uses average classiﬁcation accuracy,\\nper-segment/image and normalized for each class. When\\nevaluating the quality of a segmentation algorithm, however, one must account for the fact that in most datasets,\\nand in OS and MSRC in particular, not all pixels are labelled. In this case, accuracy is measured per-pixel rather',\n",
       " 'and in OS and MSRC in particular, not all pixels are labelled. In this case, accuracy is measured per-pixel rather\\nthan per-segment, ignoring all pixels that are unlabelled in\\nthe ground truth. For MSRC, furthermore, accuracy is normalised across all pixels regardless of their category. For\\nOSA, since some segments may have more than one label,\\nwe are reporting mAP, following the standard procedure for\\nmulti-label datasets. Finally, PASCAL VOC 2007 classiﬁcation uses mean average precision (mAP), computed using\\nthe TRECVID 11-point interpolation [10].3\\nDescriptor details. FC-CNN and FV-CNN build on the\\npre-trained VGG-M [6] model as this performs better than\\nother popular models such as [21] while having a similar computational cost. This network results in 4096dimensional FC-CNN features and 512-dimensional local\\nfeatures for FV-CNN computation. The latter are pooled\\ninto a FV representation with 64 Gaussian components,\\nresulting in 65K-dimensional descriptors. While the FVCNN dimensionality is much higher than the 4K dimensions of FC-CNN, the FV is known to be highly redundant and can be typically compressed by one order of magnitude without appreciable reduction in the classiﬁcation',\n",
       " 'performance [31], so the effective dimensionality of FCand FV-CNN is likely comparable. We veriﬁed that by\\nPCA-reducing FV to 4096 dimensions and observing only\\na marginal reduction in classiﬁcation performance in the\\nPASCAL VOC object recognition task described below. In\\naddition to VGG-M, the recent state-of-the art VGG-VD\\n(very deep with 19 layers) model of Simonyan and Zisserman [39] is also evaluated.\\nDue to the similarity between FV-CNN and the dense\\nSIFT FV descriptors used for texture recognition in [7],\\nthe latter is evaluated as well. Since SIFT descriptors\\nare smaller (128-dimensional) than the convolutional ones\\n(512-dimensional), a larger number of Gaussian components (256) are used to obtain FV descriptors with a comparable dimensionality. The SIFT descriptors support is\\n32\\x0232pixels at the base scale.\\nIn order to make results comparable to [7], we use\\nthe same settings whenever possible. FV-CNN and DSIFT compute features after rescaling the image by factors\\n3The deﬁnition of AP was changed in later versions of the benchmark.',\n",
       " '3The deﬁnition of AP was changed in later versions of the benchmark.\\ndataset meas.IFVVGG-M VGG-VD FV-SIFTSoA(%) FC FV FC+FV FC FV FC+FV FC+FV-VD\\n(a)KTH-T2b acc 70.8\\x062.771\\x062.373.3\\x064.773.9\\x064.975.4\\x061.581.8\\x062.581.1\\x062.4 81.5\\x062.0 76.0\\x062.9[40]\\nFMD acc 59.8\\x061.670.3\\x061.873.5\\x062.076.6\\x061.977.4\\x061.879.8\\x061.882.4\\x061.5 82.2\\x061.4 57.7\\x061.7[33, 37]\\nOS+R acc 39.8 41.3 52.5 54.9 43.4 59.5 60.9 58.7 –\\n(b)DTD acc 58.6\\x061.258.8\\x060.866.8\\x061.669.8\\x061.162.9\\x060.872.3\\x061.074.7\\x061.0 75.5\\x060.8 –',\n",
       " 'OSA+R mAP 56.5 54.3 65.2 67.9 49.7 67.2 67.9 68.2 –\\n(d)MSRC+R acc 85.7 85.0 95.4 96.9 79.4 97.7 98.8 99.1 –\\nMSRC+R msrc-acc 92.0 84.0 97.6 98.1 82.0 99.2 99.6 99.5 –\\nVOC07 mAP11 59.9 76.8 76.4 79.5 81.7 84.9 85.1 84.5 85.2 [44]\\nMIT Indoor acc 54.9 62.5 74.2 74.4 67.6 81.0 80.3 80.0 70.8 [47]\\nCUB acc 17.5 46.1 49.9 54.9 54.6 66.7 67.3 65.4 73.9 (62.8\\x03) [46]\\nCUB+R acc 27.7 56.5 65.5 68.1 62.8 73.0 74.9 73.6 76.37 [46]\\nTable 1. Evaluation of texture descriptors. The table compares FC-CNN, FV-CNN on two networks trained on ImageNet – VGG-M and',\n",
       " 'Table 1. Evaluation of texture descriptors. The table compares FC-CNN, FV-CNN on two networks trained on ImageNet – VGG-M and\\nVGG-VD, and IFV on dense SIFT. We evaluated these descriptors on (a) material datasets (FMD, KTH-T2b, OS+R), (b) texture attributes\\n(DTD, OSA+R) and (c) general categorisation datasets (MSRC+R,VOC07,MIT Indoor) and ﬁne grained categorisation (CUB, CUB+R).\\nFor this experiment the region support is assumed to be known (and equal to the entire image for all the datasets except OS+R and MSRC+R\\nand for CUB+R, where it is set to the bounding box of a bird). (\\x03) using a model without parts. Best results are marked in bold.\\nVGG-M VGG-VD\\ndataset measure (%) FC-CNN FV-CNN FV+FC-CNN FC-CNN FV-CNN FC+FV-CNN SoA\\nOS pp-acc 36.3 48.7 (46.9) 50.5 38.8 55.4 (55.7) 55.2 –',\n",
       " 'OS pp-acc 36.3 48.7 (46.9) 50.5 38.8 55.4 (55.7) 55.2 –\\nMSRC msrc-acc 56.1 82.3 75.5 57.7 87.0 80.4 86.5 [24]\\nTable 2. Segmentation and recognition using crisp region proposals of materials (OS) and things & stuff (MSRC). Per-pixel accuracies are\\nreported, using the MSRC variant (see text) for the MSRC dataset. Results using MCG proposals [3] are reported in brackets for FV-CNN.\\n2s; s=\\x003;\\x002:5; : : :1:5(but, for efﬁciency, discarding\\nscales that would make the image larger than 10242pixels). Before pooling descriptors with a FV , these are usually\\nde-correlated by using PCA. Here PCA is applied to SIFT,\\nadditionally reducing its dimension to 80, as this was empirically shown to improve the overall recognition performance. However, PCA is not applied to the convolutional\\nfeatures in FV-CNN as in this case results were worse.\\nLearning details. The region descriptors (FC-CNN, FVCNN, and D-SIFT) are classiﬁed using 1-vs-rest Support',\n",
       " 'Learning details. The region descriptors (FC-CNN, FVCNN, and D-SIFT) are classiﬁed using 1-vs-rest Support\\nVector Machine (SVM) classiﬁers. Prior to learning, descriptors are L2normalised and the learning constant set\\ntoC= 1. This is motivated by the fact that, after data\\nnormalisation, the exact choice of Chas a negligible effect\\non performance. Furthermore, the accuracy of the 1-vs-rest\\nclassiﬁcation scheme is improved by recalibrating the SVM\\nscores after training, by scaling the SVM weight vector and\\nbias such that the median scores of the negative and positive\\ntraining samples for each class are mapped respectively to\\nthe values\\x001and1.\\n4.1. Region recognition: textures\\nThis and the following section evaluate region recognition assuming that the ground-truth region Ris known (Table 1), for example because it ﬁlls the entire image. This\\nsection focuses on textures (materials and perceptual attributes), while the next on objects and scenes.\\nTexture recognition without clutter. This experimentevaluates the performance of FC-CNN, FV-CNN, D-SIFT,\\nand their combinations in standard texture recognition',\n",
       " 'Texture recognition without clutter. This experimentevaluates the performance of FC-CNN, FV-CNN, D-SIFT,\\nand their combinations in standard texture recognition\\nbenchmarks such as FMD, KTH-TIPS-2, and DTD. FCCNN is roughly equivalent to the DeCAF method used\\nin [7] for this data as regions ﬁll images; however, while the\\nperformance of our FC-CNN is similar in KTH ( \\x1870%),\\nit is substantially better in FMD ( 60:7%!70:4%accuracy) and DTD ( 54:8%!58:7%). This likely is caused by\\nthe improved underlying CNN, an advantage which is more\\nobvious in FMD and DTD that are closer to object recognition than KTH. FV-CNN performs within \\x062%in FMD and\\nKTH but substantially better for DTD ( 58:7%!66:6%).\\nD-SIFT is comparable in performance to FC-CNN in DTD\\nand KTH, but substantially worse ( 70:4%!59:2%) in\\nFMD. Our conclusion is that, even when textures ﬁll the\\ninput image as in these benchmarks, orderless pooling in\\nFV-CNN and D-SIFT can be either the same or substantially better than the pooling operated in the fully-connected',\n",
       " 'input image as in these benchmarks, orderless pooling in\\nFV-CNN and D-SIFT can be either the same or substantially better than the pooling operated in the fully-connected\\nlayers by FC-CNN.\\nCombining FC- and FV-CNN improves performance in\\nall datasets by 1\\x003%. While this combination is already signiﬁcantly above the state-of-the-art in DTD and\\nFMD ( +2:6%=11:2%), the method of [7] still outperforms\\nthese descriptors in KTH. However, replacing VGG-M with\\nVGG-VD signiﬁcantly improves the performance in all\\ncases – a testament to the power of deep features. In particular, the best method FC+FV-CNN-VD, improves the state\\nof the art by at least 6%in all datasets. Interestingly, this is\\nobtained by using a single low-level feature type as FC- and\\nFV-CNN build on the same convolutional features. Adding\\nD-SIFT results in at most \\x181%improvement, and in some\\ncases it slightly degrades performance.\\nTexture recognition in clutter. The advantage of FV-CNN\\nover FC-CNN is much larger when textures do not ﬁll the',\n",
       " 'cases it slightly degrades performance.\\nTexture recognition in clutter. The advantage of FV-CNN\\nover FC-CNN is much larger when textures do not ﬁll the\\nimage but are extracted from clutter. In OS+R (Sect. 2),\\nmaterial recognition accuracy starts at about 46% for both\\nFC-CNN and D-SIFT; however, FV-CNN improves this by\\nmore than 11% (46:5%!58:1%). The combination of FCand FV-CNN improves results further by \\x182%, but adding\\nSIFT deteriorates performance. With the very deep CNN\\nconclusions are similar; however, switching to VGG-VD\\nbarely affects the FC-CNN performance ( 46:5!48:0%),\\nbut strongly affects the one of FV-CNN ( 58:1%!65:1%).\\nThis conﬁrms that FC-CNN, while excellent in object detection, is not a very good descriptor for classifying textured\\nregions. Results in OSA+R for texture attribute recognition\\n(Sect. 2) and in MSRC+R for semantic segmentation are\\nanalogous; it is worth noting that, when ground-truth segments are used in this experiment, the best model achieves\\na nearly perfect 99:7%classiﬁcation rate in MSRC.',\n",
       " 'a nearly perfect 99:7%classiﬁcation rate in MSRC.\\n4.2. Region recognition: objects and scenes\\nThis section shows that the FV-CNN descriptor, despite\\nits orderless nature that make it an excellent texture descriptor, excels at object and scene recognition as well. In the remainder of the section, and unless otherwise noted, region\\ndescriptors are applied to images as a whole by considering\\nthese single regions.\\nFV-CNN vs FC-CNN. As seen in Table 1, in PASCAL\\nVOC and MIT Indoor the FC-CNN descriptor performs\\nvery well but in line with previous results for this class of\\nmethods [6]. FV-CNN performs similarly to FC-CNN in\\nPASCAL VOC, but substantially better ( +5% for VGG-M\\nand +13% for VGG-VD) in MIT Indoor. As further discussed below, this is an example of the ability of FV-CNN\\nto transfer between domains better than FC-CNN. The gap\\nbetween FC-CNN and FV-CNN is the highest for the very\\ndeep VGG-VD models (68.1% !81.1%), a trend also exhibited by other datasets as seen in Tab. 1. In the CUB\\ndataset, FV-CNN signiﬁcantly outperforms FC-CNN both',\n",
       " 'dataset, FV-CNN signiﬁcantly outperforms FC-CNN both\\nwhether the descriptor is computed from the whole image\\n(CUB) or from the bird bounding box (CUB+R). In the latter case, the difference is very large ( +10\\x0014%).\\nComparison with alternative pooling methods. FV-CNN\\nis related to the method of [15], which uses a similar\\nunderlying CNN and VLAD instead of FV for pooling.\\nNotably, however, FV-CNN results on MIT Indoor are\\nmarkedly better than theirs for both VGG-M and VGGVD ( 68:8%!73:5%=81:1%resp.) and marginally better (68:8%!69:1%) when the same CAFFE CNN is used\\n(Tab. 3). The key difference is that FV-CNN pools convolu-Accuracy (%)\\nCNN FC-CNN FV-CNN FC+FV-CNN\\nPLACES 65.0 67.6 73.1\\nCAFFE 58.6 69.7 71.6\\nVGG-M 62.5 74.2 74.4\\nVGG-VD 67.6 81.0 80.3\\nTable 3. Accuracy of various CNNs on the MIT indoor dataset.',\n",
       " 'VGG-VD 67.6 81.0 80.3\\nTable 3. Accuracy of various CNNs on the MIT indoor dataset.\\ntional features, whereas [15] pools fully-connected descriptors extracted from square image patches. Thus, even without spatial information as used by [15], FV-CNN is not only\\nsubstantially faster, but at least as accurate. Using the same\\nsettings, that is, the same net and the same three scales, our\\napproach results in an 8.5 \\x02speedup.\\nComparison with the state-of-the-art. The best result obtained in PASCAL VOC is comparable to the current stateof-the-art set by the deep learning method of [44] ( 85:2%!\\n85:0%), but using a much more straightforward pipeline. In\\nMIT Places our best performance is also substantially superior ( +10% ) to the current state-of-the-art using deep convolutional networks learned on the MIT Place dataset [47]\\n(see also below). In the CUB dataset, our best performance\\nis a little short (\\x183%) of the state-of-the-art results of [46].\\nHowever, [46] uses a category-speciﬁc part detector and\\ncorresponding part descriptor as well as a CNN ﬁne-tuned',\n",
       " 'However, [46] uses a category-speciﬁc part detector and\\ncorresponding part descriptor as well as a CNN ﬁne-tuned\\non the CUB data; by contrast, FV-CNN and FC-CNN are\\nused here as global image descriptors which, furthermore,\\nare the same for all the datasets considered . Compared to\\nthe results of [46] without part-based descriptors (but still\\nusing a part-based object detector), our global image descriptors perform substantially better ( 62:1%!69:1%).\\nWe conclude that FV-CNN is a very strong descriptor.\\nResults are usually as good, and often signiﬁcantly better,\\nthan FC-CNN. In most applications, furthermore, FV-CNN\\nis many times faster as it does not require evaluating the\\nCNN for each target image region. Finally, FC- and FVCNN can be combined outperforming the state-of-the-art in\\nmany benchmarks.\\nDomain transfer. So far, the same underlying CNN features, trained on ImageNet’s ILSVCR, were used for all\\ndatasets. Here we investigate the effect of using domainspeciﬁc features. To do so, we consider the PLACES [47],',\n",
       " 'datasets. Here we investigate the effect of using domainspeciﬁc features. To do so, we consider the PLACES [47],\\ntrained to recognize places on a dataset of about 2.5 million labeled images. [47] showed that, applied to the task of\\nscene recognition in MIT Indoor, these features outperform\\nsimilar ones trained on ILSVCR (denoted CAFFE [21] below) – a fact explained by the similarity of domains. Below,\\nwe repeat this experiment using FC- and FV-CNN descriptors on top of VGG-M, VGG-VD, PLACES, and CAFFE.\\nResults are shown in Table 3. The FC-CNN results are\\nin line with those reported in [47] – in scene recognition\\nwith FC-CNN the same CNN architecture performs better\\nif trained on the Places dataset instead of the ImageNet data\\n(58:6%!65:0%accuracy4). Nevertheless, stronger CNN\\narchitectures such as VGG-M and VGG-VD can approach\\nand outperform PLACES even if trained on ImageNet data\\n(65:0%!63:0%=68:1%).\\nHowever, when it comes to using the ﬁlter banks with\\nFV-CNN, conclusions are very different. First, FV-CNN',\n",
       " 'However, when it comes to using the ﬁlter banks with\\nFV-CNN, conclusions are very different. First, FV-CNN\\noutperforms FC-CNN in all cases, with substantial gains\\nup to 20% in correspondence of a domain transfer from\\nImageNet to MIT Indoor. Second, the advantage of using domain-speciﬁc CNNs disappears . In fact, the same\\nCAFFE model that is 6.4% worse than PLACES with FCCNN, is actually 1.5% better when used in FV-CNN. The\\nconclusion is that FV-CNN appears to be immune, or at\\nleast substantially less sensitive, to domain shifts.\\nOur tentative explanation of this surprising phenomenon\\nis that the convolutional layers are less committed to a speciﬁc dataset than the fully ones. Hence, by using those,\\nFV-CNN tends to be a more general than FC-CNN.\\n4.3. Texture segmentation\\nThe previous section considered the problem of region\\nrecognition when the region support is known at test time.\\nThis section studies the problem of recognising regions\\nwhen their extent Risnotknown and also be estimated.\\nThe ﬁrst experiment (Tab. 2) investigates the simplest possible scheme: combining the region descriptors\\nof Sect. 4.1 with a general-purpose image segmentation',\n",
       " 'The ﬁrst experiment (Tab. 2) investigates the simplest possible scheme: combining the region descriptors\\nof Sect. 4.1 with a general-purpose image segmentation\\nmethod, namely the crisp regions of [19]. Two datasets\\nare evaluated: OS for material recognition and MSRC for\\nthings & stuff. Compared to OS+R, classifying crisp regions results in a drop of about 5%points for all descriptors. As this dataset is fairly challenging with best achievable performance is 55:4%, this is a satisfactory result. But\\nit also illustrates that there is ample space for future improvements. In MSRC, the best accuracy is 87:0%, just a\\nhair above the best published result 86:5%[25]. Remarkably, these algorithms not use any dataset-speciﬁc training, nor CRF-regularised semantic inference: they simply\\ngreedily classify regions as obtained from a general-purpose\\nsegmentation algorithms. Qualitative segmentation results\\n(sampled at random) are given in Fig. 2 and 3.\\nUnlike crisp regions, the proposals of [3] are overlapping and a typical image contains thousands of them. We\\npropose a simple scheme to combine prediction from multiple proposals. For each proposal we set its label to the\\nhighest scoring class, and score to the highest score. We\\nthen sort the proposals in the increasing order of their score',\n",
       " 'highest scoring class, and score to the highest score. We\\nthen sort the proposals in the increasing order of their score\\ndivided by their area and paste them one by one. This has\\nthe effect of considering larger regions before smaller ones\\nand more conﬁdent regions after less ones for regions of\\nthe same area. Results using FV-CNN shown in Tab. 2 in\\n4[47] report 68:3%for PLACES applied to MIT Indoor, a small difference explained by implementation details such as the fact that, for all the\\nmethods, we do not perform data augmentation by jittering.\\n40444852566064687276Performance (%)\\nconv1\\n[7 x  7]\\n 96x256\\nconv2\\n[27 x  27]\\n256x128\\nconv3\\n[75 x  75]\\n512x 64\\nconv4\\n[107 x  107]\\n512x 64\\nconv5\\n[139 x  139]\\n512x 64(VGG−M) Filterbank Analysis\\n  \\nVOC07 (mAP11)\\nMIT Indoor (acc)\\nFMD (acc)\\nDTD (acc)\\nSIFTFigure 4. CNN ﬁlterbank analysis for VGG-M. Performance of\\nﬁlter banks extracted from various layers network are shown on',\n",
       " 'SIFTFigure 4. CNN ﬁlterbank analysis for VGG-M. Performance of\\nﬁlter banks extracted from various layers network are shown on\\nvarious datasets. For each layer convf1; : : : ; 5gwe show, the size\\nof the receptive ﬁeld [ N\\x02N], and FB\\x02D, where FBis the size\\nof ﬁlter bank and Dis the dictionary size in the FV representation.\\nThe performance using SIFT is shown in black plus (+) marks.\\nbrackets (FC-CNN was too slow for our experiments). The\\nresults are comparable to those using crisp regions, and we\\nobtain 55:7%accuracy on the OS dataset. Our initial attempts at schemes such as non-maximum suppression of\\noverlapping regions that are quite successful for object segmentation [16] performed rather poorly. We believe this is\\nbecause unlike objects, material information is fairly localized and highly irregularly shaped in an image. However,\\nthere is room for improvement by combining evidence from\\nmultiple segmentations.\\n4.4. Convolutional layer analysis\\nWe study the performance of ﬁlter banks extracted from\\ndifferent layers of a CNN in the FV-CNN framework. We',\n",
       " '4.4. Convolutional layer analysis\\nWe study the performance of ﬁlter banks extracted from\\ndifferent layers of a CNN in the FV-CNN framework. We\\nuse the VGG-M network which has ﬁve convolutional layers. Results on various datasets, obtained as in Sect. 4.1\\nand 4.2, are shown in Fig. 4. In addition we also show the\\nperformance using FVs constructed from dense SIFT using\\na number of words such that the resulting FV is roughly the\\nsame size of FV-CNN. The CNN ﬁlter banks from layer\\n3 and beyond signiﬁcantly outperform SIFT. The performance monotonically improves from layer one to ﬁve.\\n5. Conclusions\\nWe have conducted a range of experiments on material\\nand texture attribute recognition in a large dataset of textures in clutter. This benchmark was derived from OpenSurfaces, an earlier contribution of the computer graphics community, highlights the potential for collaboration between\\ncomputer graphics and vision communities. We have also\\nevaluated a number of state-of-the-art texture descriptors on\\nthese and many other benchmarks. Our main ﬁnding is that\\norderless pooling of convolutional neural network features\\nis a remarkably good texture descriptor, versatile enough to\\ndubbed as a scene and object descriptor, resulting in new',\n",
       " 'orderless pooling of convolutional neural network features\\nis a remarkably good texture descriptor, versatile enough to\\ndubbed as a scene and object descriptor, resulting in new\\nstate-of-the-art performance in several benchmarks.\\nbrick cardboard carpet_rug ceramic concrete fabric_cloth food glass\\ngranite_marble hair other laminate leather metal painted paper_tissue\\nplastic_clear plastic_opaque stone tile wallpaper wood(a) (b) (c) (d) (e) (f)\\nFigure 2. OS material recognition results. Example test image with material recognition and segmentation on the OS dataset. (a) original\\nimage. (b) ground truth segmentations from the OpenSurfaces repository (note that not all pixels are annotated). (c) FC-CNN and crispregion proposals segmentation results. (d) incorrectly predicted pixels (restricted to the ones annotated). (e-f) the same, but for FV-CNN.\\nbuilding grass tree cow sheep sky aeroplane water\\nface car bicycle flower sign bird book chair\\nroad cat dog body boat\\n(a) (b) (c) (d) (e) (f)\\nFigure 3. MSRC object segmentation results. (a) image, (b) ground-truth, (c-d) FC-CNN, (d-e) FV-CNN segmentation and errors.',\n",
       " 'Acknowledgements M. Cimpoi was supported by ERC\\ngrant VisRec no. 228180 and the XRCE UAC grant. S. Maji\\nacknowledges a faculty startup grant from UMass, Amherst.References\\n[1] E. H. Adelson. On seeing stuff: The perception of materials by humans and machines. SPIE , 4299, 2001. 1\\n[2] M. Amadasun and R. King. Textural features corresponding to textural properties. Systems, Man, and Cybernetics , 19(5), 1989. 1\\n[3] P. Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik.\\nMultiscale combinatorial grouping. CVPR, 2014. 4, 5, 7\\n[4] S. Bell, P. Upchurch, N. Snavely, and K. Bala. Opensurfaces: A\\nrichly annotated catalog of surface appearance. In Proc. SIGGRAPH ,\\n2013. 2\\n[5] B. Caputo, E. Hayman, and P. Mallikarjuna. Class-speciﬁc material\\ncategorisation. In ICCV , 2005. 1, 4',\n",
       " 'categorisation. In ICCV , 2005. 1, 4\\n[6] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return\\nof the devil in the details: Delving deep into convolutional nets. In\\nProc. BMVC , 2014. 2, 3, 4, 6\\n[7] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi.\\nDescribing textures in the wild. In Proc. CVPR , 2014. 1, 3, 4, 5\\n[8] K. J. Dana, B. van Ginneken, S. K. Nayar, and J. J. Koenderink.\\nReﬂectance and texture of real world surfaces. ACM Transactions on\\nGraphics , 18(1):1–34, 1999. 1\\n[9] A. Efros and T. Leung. Texture synthesis by non-parametric sampling. In CVPR , volume 2, 1999. 1\\n[10] M. Everingham, A. Zisserman, C. Williams, and L. V . Gool. The\\nPASCAL visual obiect classes challenge 2007 (VOC2007) results.\\nTechnical report, Pascal Challenge, 2007. 3, 4',\n",
       " 'PASCAL visual obiect classes challenge 2007 (VOC2007) results.\\nTechnical report, Pascal Challenge, 2007. 3, 4\\n[11] V . Ferrari and A. Zisserman. Learning visual attributes. In Proc.\\nNIPS , 2007. 1\\n[12] D. Forsyth. Shape from texture and integrability. In ICCV , volume 2,\\npages 447–452. IEEE, 2001. 1\\n[13] J. G ˚arding. Shape from texture for smooth curved surfaces in perspective projection. Journal of Mathematical Imaging and Vision ,\\n2(4):327–350, 1992. 1\\n[14] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature\\nhierarchies for accurate object detection and semantic segmentation.\\nInProc. CVPR , 2014. 2, 3\\n[15] Y . Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale orderless\\npooling of deep convolutional activation features. In Proc. ECCV ,\\n2014. 2, 3, 6\\n[16] B. Hariharan, P. Arbel ´aez, R. Girshick, and J. Malik. Simultaneous\\ndetection and segmentation. In Computer Vision–ECCV 2014 , pages',\n",
       " 'detection and segmentation. In Computer Vision–ECCV 2014 , pages\\n297–312. Springer, 2014. 2, 7\\n[17] E. Hayman, B. Caputo, M. Fritz, and J.-O. Eklundh. On the significance of real-world conditions for material classiﬁcation. ECCV ,\\n2004. 4\\n[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition. In Proc. ECCV , 2014.\\n2\\n[19] P. Isola, D. Zoran, D. Krishnan, and E. H. Adelson. Crisp boundary\\ndetection using pointwise mutual information. In Proc. ECCV , 2014.\\n4, 7\\n[20] A. Jain and F. Farrokhnia. Unsupervised texture segmentation using\\ngabor ﬁlters. Pattern recognition , 24(12):1167–1186, 1991. 1\\n[21] Y . Jia. Caffe: An open source convolutional architecture for fast\\nfeature embedding. http://caffe.berkeleyvision.org/ ,\\n2013. 3, 4, 6\\n[22] B. Julesz and J. R. Bergen. Textons, the fundamental elements in',\n",
       " '2013. 3, 4, 6\\n[22] B. Julesz and J. R. Bergen. Textons, the fundamental elements in\\npreattentive vision and perception of textures. Bell System Technical\\nJournal , 62(6, Pt 3):1619–1645, Jul-Aug 1983. 1\\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Proc. NIPS , 2012.\\n2, 3\\n[24] L. Ladicky, C. Russell, P. Kohli, and P. Torr. Graph cut based inference with co-occurrence statistics. In Proc. ECCV , pages 239–253,\\n2010. 5\\n[25] L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. Torr. What,\\nwhere and how many? combining object detectors and crfs. In Proc.\\nECCV , 2010. 7\\n[26] T. Leung and J. Malik. Representing and recognizing the visual appearance of materials using three-dimensional textons. International\\nJournal of Computer Vision , 43(1):29–44, 2001. 1',\n",
       " 'Journal of Computer Vision , 43(1):29–44, 2001. 1\\n[27] L. Liu, L. Wang, and X. Liu. In defense of soft-assignment coding.\\nInComputer Vision (ICCV), 2011 IEEE International Conference on ,\\npages 2486–2493. IEEE, 2011. 1[28] D. G. Lowe. Object recognition from local scale-invariant features.\\nInProc. ICCV , 1999. 1\\n[29] B. Manjunath and R. Chellappa. Unsupervised texture segmentation\\nusing markov random ﬁeld models. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence , 13(5):478–482, 1991. 1\\n[30] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and Transferring Mid-Level Image Representations using Convolutional Neural\\nNetworks. In Proc. CVPR , 2014. 2\\n[31] O. M. Parkhi, K. Simonyan, A. Vedaldi, and A. Zisserman. A compact and discriminative face track descriptor. In Proc. CVPR , 2014.\\n4',\n",
       " '4\\n[32] F. Perronnin, J. S ´anchez, and T. Mensink. Improving the ﬁsher kernel for large-scale image classiﬁcation. In Computer Vision–ECCV\\n2010 , pages 143–156. Springer, 2010. 1\\n[33] X. Qi, R. Xiao, C. G. Li, Y . Qiao, J. Guo, and X. Tang. Pairwise rotation invariant co-occurrence local binary pattern. PAMI ,\\n36(11):2199–2213, Nov 2014. 5\\n[34] A. Quattoni and A. Torralba. Recognizing indoor scenes. In Proc.\\nCVPR , 2009. 2, 4\\n[35] A. S. Razavin, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: An astounding baseline for recognition. In DeepVision workshop , 2014. 3\\n[36] G. Schwartz and K. Nishino. Visual material traits: Recognizing\\nper-pixel material context. In Proc. CVCP , 2013. 1\\n[37] L. Sharan, C. Liu, R. Rosenholtz, and E. H. Adelson. Recognizing\\nmaterials using perceptually inspired features. International Journal',\n",
       " '[37] L. Sharan, C. Liu, R. Rosenholtz, and E. H. Adelson. Recognizing\\nmaterials using perceptually inspired features. International Journal\\nof Computer Vision , 103(3):348–371, 2013. 1, 4, 5\\n[38] L. Sharan, R. Rosenholtz, and E. H. Adelson. Material perceprion:\\nWhat can you see in a brief glance? Journal of Vision , 9:784(8),\\n2009. 1\\n[39] K. Simonyan and A. Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. CoRR , abs/1409.1556, 2014. 2, 4\\n[40] M. Sulc and J. Matas. Fast features invariant to rotation and scale of\\ntexture. Technical report, 2014. 5\\n[41] M. Varma and A. Zisserman. Texture classiﬁcation: Are ﬁlter banks\\nnecessary? In CVPR , volume 2, pages II–691. IEEE, 2003. 1\\n[42] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y . Gong. Localityconstrained linear coding for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on ,',\n",
       " 'pages 3360–3367. IEEE, 2010. 1\\n[43] L. Wei and M. Levoy. Fast texture synthesis using treestructured vector quantization. In SIGGRAPH , pages 479–488. ACM\\nPress/Addison-Wesley Publishing Co., 2000. 1\\n[44] Y . Wei, W. Xia, J. Huang, B. Ni, J. Dong, Y . Zhao, and S. Yan. Cnn:\\nSingle-label to multi-label. 2014. 5, 6\\n[45] P. Welinder, S. Branson, T. Mita, C. Wah, and F. Schroff. Caltechucsd birds 200. Technical report, Caltech-UCSD, 2010. 4\\n[46] N. Zhang, J. Donahue, R. Girshickr, and T. Darrell. Part-based RCNNs for ﬁne-grained category detection. In Proc. ECCV , 2014. 5,\\n6\\n[47] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning\\ndeep features for scene recognition using places database. In Proc.\\nNIPS , 2014. 2, 5, 6, 7',\n",
       " 'deep features for scene recognition using places database. In Proc.\\nNIPS , 2014. 2, 5, 6, 7\\n[48] X. Zhou, K. Yu, T. Zhang, and T. S. Huang. Image classiﬁcation\\nusing super-vector coding of local image descriptors. In Computer\\nVision–ECCV 2010 , pages 141–154. Springer, 2010. 1',\n",
       " 'Material Recognition in the Wild with the Materials in Context Database\\nSean Bell\\x03Paul Upchurch\\x03Noah Snavely Kavita Bala\\nDepartment of Computer Science, Cornell University\\nfsbell,paulu,snavely,kb g@cs.cornell.edu\\nAbstract\\nRecognizing materials in real-world images is a challenging task. Real-world materials have rich surface texture,\\ngeometry, lighting conditions, and clutter, which combine\\nto make the problem particularly difﬁcult. In this paper, we\\nintroduce a new, large-scale, open dataset of materials in\\nthe wild, the Materials in Context Database (MINC ), and\\ncombine this dataset with deep learning to achieve material\\nrecognition and segmentation of images in the wild.\\nMINC is an order of magnitude larger than previous material databases, while being more diverse and well-sampled\\nacross its 23 categories. Using MINC , we train convolutional neural networks (CNNs) for two tasks: classifying\\nmaterials from patches, and simultaneous material recognition and segmentation in full images. For patch-based classiﬁcation on MINC we found that the best performing CNN\\narchitectures can achieve 85.2% mean class accuracy. We\\nconvert these trained CNN classiﬁers into an efﬁcient fully',\n",
       " 'architectures can achieve 85.2% mean class accuracy. We\\nconvert these trained CNN classiﬁers into an efﬁcient fully\\nconvolutional framework combined with a fully connected\\nconditional random ﬁeld (CRF) to predict the material at\\nevery pixel in an image, achieving 73.1% mean class accuracy. Our experiments demonstrate that having a large,\\nwell-sampled dataset such as MINC is crucial for real-world\\nmaterial recognition and segmentation.\\n1. Introduction\\nMaterial recognition plays a critical role in our understanding of and interactions with the world. To tell whether\\na surface is easy to walk on, or what kind of grip to use\\nto pick up an object, we must recognize the materials that\\nmake up our surroundings. Automatic material recognition\\ncan be useful in a variety of applications, including robotics,\\nproduct search, and image editing for interior design. But recognizing materials in real-world images is very challenging.\\nMany categories of materials, such as fabric or wood, are\\nvisually very rich and span a diverse range of appearances.\\nMaterials can further vary in appearance due to lighting and\\nshape. Some categories, such as plastic and ceramic, are of\\x03Authors contributed equally\\nCNN\\nSliding\\nCNN“wood”\\nTransfer weights(trained)\\n(discretely',\n",
       " 'CNN\\nSliding\\nCNN“wood”\\nTransfer weights(trained)\\n(discretely \\noptimized)(fixed)AMT AMT AMT MINC\\n3 million\\npatches OpenSurfacesFlickr\\nHouzz(a) Constructing MINC\\nMaterial\\nLabels(b) Patch material classification\\n(c) Full scene material classificationP(material)\\nDense\\nCRF\\nPatchFigure 1. Overview. (a) We construct a new dataset by combining\\nOpenSurfaces [ 1] with a novel three-stage Amazon Mechanical\\nTurk (AMT) pipeline. (b)We train various CNNs on patches from\\nMINC to predict material labels. (c)We transfer the weights to a\\nfully convolutional CNN to efﬁciently generate a probability map\\nacross the image; we then use a fully connected CRF to predict the\\nmaterial at every pixel.\\nten smooth and featureless, requiring reasoning about subtle\\ncues or context to differentiate between them.\\nLarge-scale datasets (e.g., ImageNet [ 21], SUN [ 31,19]\\nand Places [ 34]) combined with convolutional neural networks (CNNs) have been key to recent breakthroughs in\\nobject recognition and scene classiﬁcation. Material recognition is similarly poised for advancement through large-scale\\ndata and learning. To date, progress in material recognition',\n",
       " 'object recognition and scene classiﬁcation. Material recognition is similarly poised for advancement through large-scale\\ndata and learning. To date, progress in material recognition\\nhas been facilitated by moderate-sized datasets like the Flickr\\nMaterial Database (FMD) [ 26]. FMD contains ten material\\ncategories, each with 100 samples drawn from Flickr photos.\\nThese images were carefully selected to illustrate a wide\\nrange of appearances for these categories. FMD has been\\nused in research on new features and learning methods for\\nmaterial perception and recognition [ 17,10,20,25]. While\\nFMD was an important step towards material recognition, it\\nis not sufﬁcient for classifying materials in real-world im1arXiv:1412.0623v2  [cs.CV]  14 Apr 2015\\nagery. This is due to the relatively small set of categories,\\nthe relatively small number of images per category, and also\\nbecause the dataset has been designed around hand-picked\\niconic images of materials. The OpenSurfaces dataset [ 1]\\naddresses some of these problems by introducing 105,000\\nmaterial segmentations from real-world images, and is signiﬁcantly larger than FMD. However, in OpenSurfaces many\\nmaterial categories are under-sampled, with only tens of\\nimages.',\n",
       " 'material categories are under-sampled, with only tens of\\nimages.\\nA major contribution of our paper is a new, well-sampled\\nmaterial dataset, called the Materials in Context Database\\n(MINC ), with 3 million material samples. MINC is more\\ndiverse, has more examples in less common categories, and\\nis much larger than existing datasets. MINC draws data from\\nboth Flickr images, which include many “regular” scenes,\\nas well as Houzz images from professional photographers of\\nstaged interiors. These sources of images each have different\\ncharacteristics that together increase the range of materials\\nthat can be recognized. See Figure 2 for examples of our\\ndata. We make our full dataset available online at http:\\n//minc.cs.cornell.edu/.\\nWe use this data for material recognition by training different CNN architectures on this new dataset. We perform\\nexperiments that illustrate the effect of network architecture, image context, and training data size on subregions\\n(i.e., patches) of a full scene image. Further, we build on\\nour patch classiﬁcation results and demonstrate simultaneous material recognition and segmentation of an image by\\nperforming dense classiﬁcation over the image with a fully\\nconnected conditional random ﬁeld (CRF) model [ 12]. By',\n",
       " 'performing dense classiﬁcation over the image with a fully\\nconnected conditional random ﬁeld (CRF) model [ 12]. By\\nreplacing the fully connected layers of the CNN with convolutional layers [ 24], the computational burden is signiﬁcantly\\nlower than a naive sliding window approach.\\nIn summary, we make two new contributions:\\n\\x0fWe introduce a new material dataset, MINC , and 3stage crowdsourcing pipeline for efﬁciently collecting\\nmillions of click labels (Section 3.2).\\n\\x0fOur new semantic segmentation method combines a\\nfully-connected CRF with unary predictions based on\\nCNN learned features (Section 4.2) for simultaneous\\nmaterial recognition and segmentation.\\n2. Prior Work\\nMaterial Databases. Much of the early work on material\\nrecognition focused on classifying speciﬁc instances of textures or material samples. For instance, the CUReT [ 4]\\ndatabase contains 61 material samples, each captured under\\n205 different lighting and viewing conditions. This led to\\nresearch on the task of instance-level texture or material classiﬁcation [ 15,30], and an appreciation of the challenges of\\nbuilding features that are invariant to pose and illumination.',\n",
       " 'building features that are invariant to pose and illumination.\\nLater, databases with more diverse examples from each ma-terial category began to appear, such as KTH-TIPS [ 9,2],\\nand led explorations of how to generalize from one example\\nof a material to another—from one sample of wood to a completely different sample, for instance. Real-world texture\\nattributes have also recently been explored [3].\\nIn the domain of categorical material databases, Sharan et\\nal. released FMD [ 26] (described above). Subsequently,\\nBell et al. released OpenSurfaces [ 1] which contains over\\n20,000 real-world scenes labeled with both materials and objects, using a multi-stage crowdsourcing pipeline. Because\\nOpenSurfaces images are drawn from consumer photos on\\nFlickr, material samples have real-world context, in contrast\\nto prior databases (CUReT, KTH-TIPS, FMD) which feature cropped stand-alone samples. While OpenSurfaces is a\\ngood starting point for a material database, we substantially\\nexpand it with millions of new labels.\\nMaterial recognition. Much prior work on material recognition has focused on the classiﬁcation problem (categorizing\\nan image patch into a set of material categories), often using',\n",
       " 'Material recognition. Much prior work on material recognition has focused on the classiﬁcation problem (categorizing\\nan image patch into a set of material categories), often using\\nhand-designed image features. For FMD, Liu et al. [17] introduced reﬂectance-based edge features in conjunction with\\nother general image features. Hu et al. [10] proposed features based on variances of oriented gradients. Qi et al. [20]\\nintroduced a pairwise local binary pattern (LBP) feature.\\nLiet al. [16] synthesized a dataset based on KTH-TIPS2\\nand built a classiﬁer from LBP and dense SIFT. Timofte et\\nal. [29] proposed a classiﬁcation framework with minimal\\nparameter optimization. Schwartz and Nishino [ 23] introduced material traits that incorporate learned convolutional\\nauto-encoder features. Recently, Cimpoi et al. [3] developed a CNN and improved Fisher vector (IFV) classiﬁer that\\nachieves state-of-the-art results on FMD and KTH-TIPS2.\\nFinally, it has been shown that jointly predicting objects and\\nmaterials can improve performance [10, 33].\\nConvolutional neural networks. While CNNs have been',\n",
       " 'Finally, it has been shown that jointly predicting objects and\\nmaterials can improve performance [10, 33].\\nConvolutional neural networks. While CNNs have been\\naround for a few decades, with early successes such as\\nLeNet [ 14], they have only recently led to state-of-theart results in object classiﬁcation and detection, leading\\nto enormous progress. Driven by the ILSVRC challenge [ 21], we have seen many successful CNN architectures [ 32,24,28,27], led by the work of Krizhevsky et al.\\non their SuperVision (a.k.a. AlexNet ) network [ 13], with\\nmore recent architectures including GoogLeNet [ 28]. In addition to image classiﬁcation, CNNs are the state-of-the-art\\nfor detection and localization of objects, with recent work\\nincluding R-CNNs [ 7], Overfeat [ 24], and VGG [ 27]. Finally, relevant to our goal of per-pixel material segmentation,\\nFarabet et al. [6] use a multi-scale CNN to predict the class\\nat every pixel in a segmentation. Oquab et al. [18] employ a\\nsliding window approach to localize patch classiﬁcation of\\nobjects. We build on this body of work in deep learning to',\n",
       " 'sliding window approach to localize patch classiﬁcation of\\nobjects. We build on this body of work in deep learning to\\nsolve our problem of material recognition and segmentation.\\n2\\nBrick Carpet Ceramic Fabric Foliage Food Glass Hair\\nLeather Metal Mirror Other Painted Paper Plastic Pol. stone\\nSkin Sky Stone Tile Wallpaper Water Wood\\nFigure 2. Example patches from all 23 categories of the Materials in Context Database (MINC ). Note that we sample patches so that the\\npatch center is the material in question (and not necessarily the entire patch). See Table 1 for the size of each category.\\n3. The Materials in Context Database (MINC)\\nWe now describe the methodology that went into building\\nour new material database. Why a new database? We needed\\na dataset with the following properties:\\n\\x0fSize: It should be sufﬁciently large that learning methods can generalize beyond the training set.\\n\\x0fWell-sampled : Rare categories should be represented\\nwith a large number of examples.\\n\\x0fDiversity : Images should span a wide range of appearances of each material in real-world settings.\\n\\x0fNumber of categories : It should contain many different materials found in the real world.\\n3.1. Sources of data\\nWe decided to start with the public, crowdsourced\\nOpenSurfaces dataset [ 1] as the seed for MINC since it is',\n",
       " '3.1. Sources of data\\nWe decided to start with the public, crowdsourced\\nOpenSurfaces dataset [ 1] as the seed for MINC since it is\\ndrawn from Flickr imagery of everyday, real-world scenes\\nwith reasonable diversity. Furthermore, it has a large number\\nof categories and the most samples of all prior databases.\\nWhile OpenSurfaces data is a good start, it has a few limitations. Many categories in OpenSurfaces are not well sampled. While the largest category, wood , has nearly 20K samples, smaller categories, such as water , have only tens of examples. This imbalance is due to the way the OpenSurfaces\\ndataset was annotated; workers on Amazon Mechanical Turk\\n(AMT) were free to choose any material subregion to segment. Workers often gravitated towards certain common\\ntypes of materials or salient objects, rather than being encouraged to label a diverse set of materials. Further, the\\nimages come from a single source (Flickr).\\nWe decided to augment OpenSurfaces with substantially\\nmore data, especially for underrepresented material cate-gories, with the initial goal of gathering at least 10K samples\\nper material category. We decided to gather this data from\\nanother source of imagery, professional photos on the interior design website Houzz (houzz.com). Our motivation for\\nusing this different source of data was that, despite Houzz',\n",
       " 'another source of imagery, professional photos on the interior design website Houzz (houzz.com). Our motivation for\\nusing this different source of data was that, despite Houzz\\nphotos being more “staged” (relative to Flickr photos), they\\nactually represent a larger variety of materials. For instance,\\nHouzz photos contain a wide range of types of polished\\nstone. With these sources of image data, we now describe\\nhow we gather material annotations.\\n3.2. Segments, Clicks, and Patches\\nWhat speciﬁc kinds of material annotations make for a\\ngood database? How should we collect these annotations?\\nThe type of annotations to collect is guided in large part by\\nthe tasks we wish to generate training data for. For some\\ntasks such as scene recognition, whole-image labels can\\nsufﬁce [ 31,34]. For object detection, labeled bounding\\nboxes as in PASCAL are often used [ 5]. For segmentation or\\nscene parsing tasks, per-pixel segmentations are required [ 22,\\n8]. Each style of annotation comes with a cost proportional\\nto its complexity. For materials, we decided to focus on two\\nproblems, guided by prior work:\\n\\x0fPatch material classiﬁcation. Given an image patch,\\nwhat kind of material is it at the center?',\n",
       " 'problems, guided by prior work:\\n\\x0fPatch material classiﬁcation. Given an image patch,\\nwhat kind of material is it at the center?\\n\\x0fFull scene material classiﬁcation. Given a full image, produce a full per-pixel segmentation and labeling.\\nAlso known as semantic segmentation orscene parsing\\n(but in our case, focused on materials). Note that classiﬁcation can be a component of segmentation, e.g., with\\nsliding window approaches.\\n3\\n(a) Which images \\ncontain wood ?(b) Click on 3 \\npoints of wood(c) What is this \\nmaterial?Figure 3. AMT pipeline schematic for collecting clicks. (a)\\nWorkers ﬁlter by images that contain a certain material, (b)workers click on materials, and (c)workers validate click locations by\\nre-labeling each point. Example responses are shown in orange.\\nSegments. OpenSurfaces contains material segmentations—\\ncarefully drawn polygons that enclose same-material regions.\\nTo form the basis of MINC, we selected OpenSurfaces segments with high conﬁdence (inter-worker agreement) and\\nmanually curated segments with low conﬁdence, giving a\\ntotal of 72K shapes. To better balance the categories, we',\n",
       " 'manually curated segments with low conﬁdence, giving a\\ntotal of 72K shapes. To better balance the categories, we\\nmanually segmented a few hundred extra samples for sky,\\nfoliage andwater .\\nSince some of the OpenSurfaces categories are difﬁcult\\nfor humans, we consolidated these categories. We found\\nthat many AMT workers could not disambiguate stone from\\nconcrete ,clear plastic from opaque plastic , and granite\\nfrom marble . Therefore, we merged these into stone ,plastic ,\\nandpolished stone respectively. Without this merging, many\\nground truth examples in these categories would be incorrect.\\nThe ﬁnal list of 23 categories is shown in Table 1. The\\ncategory other is different in that it was created by combining\\nvarious smaller categories.\\nClicks. Since we want to expand our dataset to millions\\nof samples, we decided to augment OpenSurfaces segments\\nby collecting clicks : single points in an image along with a\\nmaterial label, which are much cheaper and faster to collect.\\nFigure 3 shows our pipeline for collecting clicks.\\nInitially, we tried asking workers to click on examples\\nof a given material in a photo. However, we found that\\nworkers would get frustrated if the material was absent in\\ntoo many of the photos. Thus, we added an initial ﬁrst',\n",
       " 'workers would get frustrated if the material was absent in\\ntoo many of the photos. Thus, we added an initial ﬁrst\\nstage where workers ﬁlter out such photos. To increase\\nthe accuracy of our labels, we verify the click labels by\\nasking different workers to specify the material for each\\nclick without providing them with the label from the previous\\nstage.\\nTo ensure that we obtain high quality annotations and\\navoid collecting labels from workers who are not making an\\neffort, we include secret known answers (sentinels) in the\\nﬁrst and third stages, and block workers with an accuracy\\nbelow 50% and 85% respectively. We do not use sentinels\\nin the second stage since it would require per-pixel ground\\ntruth labels, and it turned out not to be necessary. Workers\\ngenerally performed all three tasks so we could identify bad\\nworkers in the ﬁrst or third task.Patches Category Patches Category Patches Category\\n564,891 Wood 114,085 Polished stone 35,246 Skin\\n465,076 Painted 98,891 Carpet 29,616 Stone\\n397,982 Fabric 83,644 Leather 28,108 Ceramic\\n216,368 Glass 75,084 Mirror 26,103 Hair\\n188,491 Metal 64,454 Brick 25,498 Food\\n147,346 Tile 55,364 Water 23,779 Paper',\n",
       " '216,368 Glass 75,084 Mirror 26,103 Hair\\n188,491 Metal 64,454 Brick 25,498 Food\\n147,346 Tile 55,364 Water 23,779 Paper\\n142,150 Sky 39,612 Other 14,954 Wallpaper\\n120,957 Foliage 38,975 Plastic\\nTable 1. MINC patch counts by category. Patches were created\\nfrom both OpenSurfaces segments and our newly collected clicks .\\nSee Section 3.2 for details.\\nMaterial clicks were collected for both OpenSurfaces\\nimages and the new Houzz images. This allowed us to use\\nlabels from OpenSurfaces to generate the sentinel data; we\\nincluded 4 sentinels per task. With this streamlined pipeline\\nwe collected 2,341,473 annotations at an average cost of\\n$0.00306 per annotation (stage 1: $0.02 / 40 images, stage\\n2: $0.10 / 50 images, 2, stage 3: $0.10 / 50 points).\\nPatches. Labeled segments and clicks form the core of\\nMINC . For training CNNs and other types of classiﬁers,\\nit is useful to have data in the form of ﬁxed-sized patches.\\nWe convert both forms of data into a uniﬁed dataset format:\\nsquare image patches. We use a patch center andpatch scale',\n",
       " 'We convert both forms of data into a uniﬁed dataset format:\\nsquare image patches. We use a patch center andpatch scale\\n(a multiplier of the smaller image dimension) to deﬁne the\\nimage subregion that makes a patch. For our patch classiﬁcation experiments, we use 23.3% of the smaller image\\ndimension. Increasing the patch scale provides more context\\nbut reduces the spatial resolution. Later in Section 5 we\\njustify our choice with experiments that vary the patch scale\\nfor AlexNet.\\nWe place a patch centered around each click label. For\\neach segment , if we were to place a patch at every interior\\npixel then we would have a very large and redundant dataset.\\nTherefore, we Poisson-disk subsample each segment , separating patch centers by at least 9.1% of the smaller image\\ndimension. These segments generated 655,201 patches (an\\naverage of 9.05 patches per segment ). In total, we generated 2,996,674 labeled patches from 436,749 images. Patch\\ncounts are shown in Table 1, and example patches from\\nvarious categories are illustrated in Figure 2.\\n4. Material recognition in real-world images\\nOur goal is to train a system that recognizes the material\\nat every pixel in an image. We split our training procedure\\ninto multiple stages and analyze the performance of the',\n",
       " 'Our goal is to train a system that recognizes the material\\nat every pixel in an image. We split our training procedure\\ninto multiple stages and analyze the performance of the\\nnetwork at each stage. First, we train a CNN that produces a\\nsingle prediction for a given input patch. Then, we convert\\nthe CNN into a sliding window and predict materials on a\\ndense grid across the image. We do this at multiple scales\\nand average to obtain a unary term. Finally, a dense CRF\\n[12] combines the unary term with fully connected pairwise\\nreasoning to output per-pixel material predictions. The entire\\nsystem is depicted in Figure 1, and described more below.\\n4\\nSky\\nWater\\nFoliageTile\\n(a) Multiscale input (b) Probability map (12 of 23 shown) (c) Predicted labelsSliding\\nCNNDense\\nCRF\\nWood\\nPlastic Sliding\\nCNNSliding\\nCNN\\nAvg.\\nGlass\\nHair\\nFabric\\nPolished\\nstone\\nStone\\n PaperWaterSky\\nFoliage\\nTilePaperWood\\nStoneWood\\nTileFigure 4. Pipeline for full scene material classiﬁcation. An image (a)is resized to multiple scales [1=p\\n2;1;p\\n2]. The same sliding CNN',\n",
       " '2;1;p\\n2]. The same sliding CNN\\npredicts a probability map (b)across the image for each scale; the results are upsampled and averaged. A fully connected CRF predicts a\\nﬁnal label for each pixel (c). This example shows predictions from a single GoogLeNet converted into a sliding CNN (no average pooling).\\n4.1. Training procedure\\nMINC contains 3 million patches that we split into training, validation and test sets. Randomly splitting would result\\nin nearly identical patches (e.g., from the same OpenSurfaces segment) being put in training and test, thus inﬂating\\nthe test score. To prevent correlation, we group photos into\\nclusters of near-duplicates, then assign each cluster to one\\nof train, validate or test. We make sure that there are at least\\n75segments of each category in the test set to ensure there\\nare enough segments to evaluate segmentation accuracy. To\\ndetect near-duplicates, we compare AlexNet CNN features\\ncomputed from each photo (see the supplemental for details).\\nFor exact duplicates, we discard all but one of the copies.\\nWe train all of our CNNs by ﬁne-tuning the network\\nstarting from the weights obtained by training on 1.2 million images from ImageNet (ILSVRC2012). When training',\n",
       " 'starting from the weights obtained by training on 1.2 million images from ImageNet (ILSVRC2012). When training\\nAlexNet , we use stochastic gradient descent with batchsize\\n128, dropout rate 0.5, momentum 0.9, and a base learning\\nrate of 10\\x003that decreases by a factor of 0.25 every 50,000\\niterations. For GoogLeNet , we use batchsize 69, dropout 0.4,\\nand learning rate \\x0bt= 10\\x004p\\n1\\x00t=250000 for iteration t.\\nOur training set has a different number of examples per\\nclass, so we cycle through the classes and randomly sample\\nan example from each class. Failing to properly balance the\\nexamples results in a 5.7% drop in mean class accuracy (on\\nthe validation set). Further, since it has been shown to reduce\\noverﬁtting, we randomly augment samples by taking crops\\n(227\\x02227out of 256\\x02256), horizontal mirror ﬂips, spatial\\nscales in the range [1=p\\n2;p\\n2], aspect ratios from 3:4 to 4:3,\\nand amplitude shifts in [0:95;1:05]. Since we are looking at\\nlocal regions, we subtract a per-channel mean (R: 124, G:',\n",
       " 'and amplitude shifts in [0:95;1:05]. Since we are looking at\\nlocal regions, we subtract a per-channel mean (R: 124, G:\\n117, B: 104) rather than a mean image [13].\\n4.2. Full scene material classiﬁcation\\nFigure 4 shows an overview of our method for simultaneously segmenting and recognizing materials. Given a\\nCNN that can classify individual points in the image, we\\nconvert it to a sliding window detector and densely classify\\na grid across the image. Speciﬁcally, we replace the last\\nfully connected layers with convolutional layers, so that the\\nnetwork is fully convolutional and can classify images ofany shape. After conversion, the weights are ﬁxed and not\\nﬁne-tuned. With our converted network, the strides of each\\nlayer cause the network to output a prediction every 32 pixels. We obtain predictions every 16 pixels by shifting the\\ninput image by half-strides (16 pixels). While this appears to\\nrequire 4x the computation, Sermanet et al. [24] showed that\\nthe convolutions can be reused and only the pool5 through\\nfc8 layers need to be recomputed for the half-stride shifts.\\nAdding half-strides resulted in a minor 0.2% improvement\\nin mean class accuracy across segments (after applying the',\n",
       " 'Adding half-strides resulted in a minor 0.2% improvement\\nin mean class accuracy across segments (after applying the\\ndense CRF, described below), and about the same mean class\\naccuracy at click locations.\\nThe input image is resized so that a patch maps to a\\n256x256 square. Thus, for a network trained at patch scale\\ns, the resized input has smaller dimension d= 256=s. Note\\nthatdis inversely proportional to scale, so increased context\\nleads to lower spatial resolution. We then add padding so\\nthat the output probability map is aligned with the input\\nwhen upsampled. We repeat this at 3 different scales (smaller\\ndimensiond=p\\n2,d,dp\\n2), upsample each output probability\\nmap with bilinear interpolation, and average the predictions.\\nTo make the next step more efﬁcient, we upsample the output\\nto a ﬁxed smaller dimension of 550.\\nWe then use the dense CRF of Kr¨ahenb ¨uhlet al. [12] to\\npredict a label at every pixel, using the following energy:\\nE(xjI) =X\\ni i(xi) +X\\ni<j ij(xi;xj) (1)\\n i(xi) =\\x00logpi(xi) (2)',\n",
       " 'i i(xi) +X\\ni<j ij(xi;xj) (1)\\n i(xi) =\\x00logpi(xi) (2)\\n ij(xi;xj) =wp\\x0e(xi6=xj)k(fi\\x00fj) (3)\\nwhere iis the unary energy (negative log of the aggregated softmax probabilities) and  ijis the pairwise term\\nthat connects every pair of pixels in the image. We use a\\nsingle pairwise term with a Potts label compatibility term\\n\\x0eweighted by wpand unit Gaussian kernel k. For the featuresfi, we convert the RGB image to L*a*b*and use color\\n(IL\\ni;Ia\\ni;Ib\\ni)and position (px;py)as pairwise features for\\neach pixel: fi=h\\npx\\ni\\n\\x12pd;py\\ni\\n\\x12pd;IL\\ni\\n\\x12L;Ia\\ni\\n\\x12ab;Ib\\ni\\n\\x12abi\\n, wheredis the\\nsmaller image dimension. Figure 4 shows an example unary\\ntermpiand the resulting segmentation x.\\n5\\nFigure 5. Varying patch scale . We train/test patches of different\\nscales (the patch locations do not vary). The optimum is a trade-off',\n",
       " '5\\nFigure 5. Varying patch scale . We train/test patches of different\\nscales (the patch locations do not vary). The optimum is a trade-off\\nbetween context and spatial resolution. CNN: AlexNet.\\nArchitecture Validation Test\\nAlexNet [13] 82.2% 81.4%\\nGoogLeNet [28] 85.9% 85.2%\\nVGG-16 [27] 85.6% 84.8%\\nTable 2. Patch material classiﬁcation results. Mean class accuracy for different CNNs trained on MINC. See Section 5.1.\\nSky 97.3% Food 90.4% Wallpaper 83.4% Glass 78.5%\\nHair 95.3% Leather 88.2% Tile 82.7% Fabric 77.8%\\nFoliage 95.1% Other 87.9% Ceramic 82.7% Metal 77.7%\\nSkin 93.9% Pol. stone 85.8% Stone 82.7% Mirror 72.0%\\nWater 93.6% Brick 85.1% Paper 81.8% Plastic 70.9%\\nCarpet 91.6% Painted 84.2% Wood 81.3%\\nTable 3. Patch test accuracy by category. CNN: GoogLeNet . See\\nthe supplemental material for a full confusion matrix.\\n5. Experiments and Results',\n",
       " 'Table 3. Patch test accuracy by category. CNN: GoogLeNet . See\\nthe supplemental material for a full confusion matrix.\\n5. Experiments and Results\\n5.1. Patch material classiﬁcation\\nIn this section, we evaluate the effect of many different\\ndesign decisions for training methods for material classiﬁcation and segmentation, including various CNN architectures,\\npatch sizes, and amounts of data.\\nCNN Architectures. Our ultimate goal is full material segmentation, but we are also interested in exploring which\\nCNN architectures give the best results for classifying single patches. Among the networks and parameter variations we tried we found the best performing networks were\\nAlexNet [13],VGG-16 [27] and GoogLeNet [28].AlexNet\\nandGoogLeNet are re-implementations by BVLC [ 11], and\\nVGG-16 is conﬁguration D (a 16 layer network) of [ 27].\\nAll models were obtained from the Caffe Model Zoo [ 11].\\nOur experiments use AlexNet for evaluating material classiﬁcation design decisions and combinations of AlexNet and\\nGoogLeNet for evaluating material segmentation. Tables 2\\nand 3 summarize patch material classiﬁcation results on our\\ndataset. Figure 10 shows correct and incorrect predictions\\nmade with high conﬁdence.',\n",
       " 'and 3 summarize patch material classiﬁcation results on our\\ndataset. Figure 10 shows correct and incorrect predictions\\nmade with high conﬁdence.\\nInput patch scale. To classify a point in an image we must\\ndecide how much context to include around it. The context,\\nexpressed as a fraction of image size, is the patch scale. A\\npriori, it is not clear which scale is best since small patches\\nhave better spatial resolution, but large patches have more\\n0.5e6 1e6 1.5e6 2e6 2.5e6\\nNumber of training patches0.600.650.700.750.800.85Mean class accuracy (test set)66.1%74.8%77.9%79.3%80.2%80.9% 81.2% 81.4%Equal size\\n79.6%Figure 6. Varying database size. Patch accuracy when trained on\\nrandom subsets of MINC .Equal size is using equal samples per\\ncategory (size determined by smallest category). CNN: AlexNet.\\nPeak accuracy\\nper category\\nFigure 7. Accuracy vs patch scale by category . Dots: peak accuracy for each category; colored lines: sky,wallpaper ,mirror ; gray\\nlines: other categories. CNN: AlexNet . While most materials are',\n",
       " 'lines: other categories. CNN: AlexNet . While most materials are\\noptimally recognized at 23.3% or 32% patch scale, recognition of\\nsky,wallpaper andmirror improve with increasing context.\\ncontextual information. Holding patch centers ﬁxed we varied scale and evaluated classiﬁcation accuracy with AlexNet .\\nResults and a visualization of patch scales are shown in Figure 5. Scale 32% performs the best. Individual categories\\nhad peaks at middle scale with some exceptions; we ﬁnd\\nthatmirror ,wallpaper andskyimprove with increasing context (Figure 7). We used 23.3% (which has nearly the same\\naccuracy but higher spatial resolution) for our experiments.\\nDataset size. To measure the effect of size on patch classiﬁcation accuracy we trained AlexNet with patches from\\nrandomly sampled subsets of all 369,104 training images\\nand tested on our full test set (Figure 6). As expected, using\\nmore data improved performance. In addition, we still have\\nnot saturated performance with 2.5 million training patches;\\neven higher accuracies may be possible with more training\\ndata (though with diminishing returns).\\nDataset balance. Although we’ve shown that more data is\\nbetter we also ﬁnd that a balanced dataset is more effective.\\nWe trained AlexNet with all patches of our smallest category',\n",
       " 'better we also ﬁnd that a balanced dataset is more effective.\\nWe trained AlexNet with all patches of our smallest category\\n(wallpaper ) and randomly sampled the larger categories\\n(the largest, wood , being 40x larger) to be equal size. We\\nthen measured mean class accuracy on the same full test\\nset. As shown in Figure 6, “Equal size” is more accurate\\nthan a dataset of the same size and just 1.7% lower than\\nthe full training set (which is 9x larger). This result further\\ndemonstrates the value of building up datasets in a balanced\\nmanner, focusing on expanding the smallest, least common\\ncategories.\\n6\\nBrick\\nWoodCarpet\\nWaterWallpaperTileStoneSkySkinPolished stonePlasticPaperPainted\\nOtherMirrorMetalLeatherHairGlassFoodFoliageFabricCeramicFigure 8. Full-scene material classiﬁcation examples: high-accuracy test set predictions by our method. CNN: GoogLeNet (with the\\naverage pooling layer removed). Right: legend for material colors. See Table 4 for quantitative evaluation.\\nInput image (a)Labels from CRF (b)Labels from CRF\\n(test set) trained on segments trained on clicks',\n",
       " 'Input image (a)Labels from CRF (b)Labels from CRF\\n(test set) trained on segments trained on clicks\\nFigure 9. Optimizing for click accuracy leads to sloppy boundaries. In(a), we optimize for mean class accuracy across segments,\\nresulting in high quality boundaries. In (b), we optimize for mean\\nclass accuracy at click locations. Since the clicks are not necessarily close to object boundaries, there is no penalty for sloppy\\nboundaries. CNN: GoogLeNet (without average pooling).\\n5.2. Full scene material segmentation\\nThe full test set for our patch dataset contains 41,801\\nphotos, but most of them contain only a few labels. Since\\nwe want to evaluate the per-pixel classiﬁcation performance,\\nwe select a subset of 5,000 test photos such that each photo\\ncontains a large number of segments and clicks, and small\\ncategories are well sampled. We greedily solve for the best\\nsuch set of photos. We similarly select 2,500 of 25,844\\nvalidation photos. Our splits for all experiments are included\\nonline with the dataset. To train the CRF for our model, we\\ntry various parameter settings ( \\x12p,\\x12ab,\\x12L,wp) and select the\\nmodel that performs best on the validation set. In total, we',\n",
       " 'try various parameter settings ( \\x12p,\\x12ab,\\x12L,wp) and select the\\nmodel that performs best on the validation set. In total, we\\nevaluate 1799 combinations of CNNs and CRF parameters.\\nSee the supplemental material for a detailed breakdown.\\nWe evaluate multiple versions of GoogLeNet : both the\\noriginal architecture and a version with the average pooling\\nlayer (at the end) changed to 5x5, 3x3, and 1x1 (i.e. no\\naverage pooling). We evaluate AlexNet trained at multiple\\npatch scales (Figure 5). When using an AlexNet trained\\nat a different scale, we keep the same scale for testing. We\\nalso experiment with ensembles of GoogLeNet andAlexNet ,Architecture(a)Segments only (b)Clicks only\\nClass Total Class Total\\nAlexNet Scale: 11.6% 64.3% 72.6% 79.9% 77.2%\\nAlexNet Scale: 23.3% 69.6% 76.6% 83.3% 81.1%\\nAlexNet Scale: 32.0% 70.1% 77.1% 83.2% 80.7%\\nAlexNet Scale: 46.5% 69.6% 75.4% 80.8% 77.7%',\n",
       " 'AlexNet Scale: 46.5% 69.6% 75.4% 80.8% 77.7%\\nAlexNet Scale: 66.2% 67.7% 72.0% 77.2% 72.6%\\nGoogLeNet 7x7 avg. pool 64.4% 71.6% 63.6% 63.4%\\nGoogLeNet 5x5 avg. pool 67.6% 74.6% 70.9% 69.8%\\nGoogLeNet 3x3 avg. pool 70.4% 77.7% 76.1% 74.7%\\nGoogLeNet No avg. pool 70.4% 78.8% 79.1% 77.4%\\nEnsemble 2 CNNs 73.1% 79.8% 84.5% 83.1%\\nEnsemble 3 CNNs 73.1% 79.3% 85.9% 83.5%\\nEnsemble 4 CNNs 72.1% 78.4% 85.8% 83.2%\\nEnsemble 5 CNNs 71.7% 78.3% 85.5% 83.2%\\nTable 4. Full scene material classiﬁcation results. Mean class\\nand total accuracy on the test set. When training, we optimize the\\nCRF parameters for mean class accuracy, but report both mean class',\n",
       " 'and total accuracy on the test set. When training, we optimize the\\nCRF parameters for mean class accuracy, but report both mean class\\nand total accuracy (mean accuracy across all examples). In one\\nexperiment (a), we train and test only on segments; in a separate\\nexperiment (b), we train and test only on clicks. Accuracies for\\nsegments are averaged across all pixels that fall in that segment.\\ncombined with either arithmetic or geometric mean.\\nSince we have two types of data, clicks andsegments , we\\nrun two sets of experiments: (a) we train and test only on\\nsegments, and in a separate experiment (b) we train and test\\nonly on clicks. These two training objectives result in very\\ndifferent behavior, as illustrated in Figure 9. In experiment\\n(a), the accuracy across segments are optimized, producing\\nclean boundaries. In experiment (b), the CRF maximizes\\naccuracy only at click locations, thus resulting in sloppy\\nboundaries. As shown in Table 4, the numerical scores for\\nthe two experiments are also very different: segments are\\nmore challenging than clicks. While clicks are sufﬁcient to\\ntrain a CNN, they are not sufﬁcient to train a CRF.\\nFocusing on segmentation accuracy, we see from Table 4(a) that our best single model is GoogLeNet without',\n",
       " 'Focusing on segmentation accuracy, we see from Table 4(a) that our best single model is GoogLeNet without\\naverage pooling (6% better than with pooling). The best\\nensemble is 2 CNNs: GoogLeNet (no average pooling) and\\nAlexNet (patch scale: 46.5%), combined with arithmetic\\nmean. Larger ensembles perform worse since we are aver7\\nCorrect\\nFabric (99%) Foliage (99%) Food (99%) Leather (99%)Correct\\nMetal (99%) Mirror (99%) Painted (99%) Plastic (99%)Incorrect\\nT: Wood T: Polished stone T: Water T: Fabric\\nP: Stone (90%) P: Water (35%) P: Carpet (27%) P: Foliage (67%)\\nFigure 10. High conﬁdence predictions. Top two rows: correct\\npredictions. Bottom row: incorrect predictions ( T: true, P: predicted). Percentages indicate conﬁdence (the predictions shown are\\nat least this conﬁdent). CNN: GoogLeNet.\\naging worse CNNs. In Figure 8, we show example labeling\\nresults on test images.\\n5.3. Comparing MINC to FMD\\nCompared to FMD, the size and diversity of MINC is',\n",
       " 'results on test images.\\n5.3. Comparing MINC to FMD\\nCompared to FMD, the size and diversity of MINC is\\nvaluable for classifying real-world imagery. Table 5 shows\\nthe effect of training on all of FMD and testing on MINC\\n(and vice versa). The results suggests that training on FMD\\nalone is not sufﬁcient for real-world classiﬁcation. Though it\\nmay seem that our dataset is “easy,” since the best classiﬁcations scores are lower for FMD than for MINC , we ﬁnd that\\ndifﬁculty is in fact closely tied to dataset size (Section 5.1).\\nTaking 100 random samples per category, AlexNet achieves\\n54.2\\x060.7% on MINC (64.6 \\x061.3% when considering only\\nthe 10 FMD categories) and 66.5% on FMD.\\n5.4. Comparing CNNs with prior methods\\nCimpoi [ 3] is the best prior material classiﬁcation method\\non FMD. We ﬁnd that by replacing DeCAF with oversampled AlexNet features we can improve on their FMD results.\\nWe then show that on MINC , a ﬁnetuned CNN is even better.',\n",
       " 'We then show that on MINC , a ﬁnetuned CNN is even better.\\nTo improve on [ 3], we take their SIFT IFV, combine it\\nwith AlexNet fc7 features, and add oversampling [ 13] (see\\nsupplemental for details). With a linear SVM we achieve\\n69.6\\x060.3% on FMD. Previous results are listed in Table 6.\\nHaving found that SIFT IFV+fc7 is the new best on\\nFMD, we compare it to a ﬁnetuned CNN on a subset of MINC (2500 patches per category, one patch per\\nphoto). Fine-tuning AlexNet achieves 76.0\\x060.2% whereasTest\\nFMD MINC\\nTrainFMD 66.5% 26.1%\\nMINC 41.7% 85.0%(10 categories\\nin common)\\nTable 5. Cross-dataset experiments. We train on one dataset and\\ntest on another dataset. Since MINC contains 23 categories, we\\nlimit MINC to the 10 categories in common. CNN: AlexNet.\\nMethod Accuracy Trials\\nSharan et al. [25] 57.1\\x060.6% 14 splits\\nCimpoi et al. [3] 67.1\\x060.4% 14 splits\\nFine-tuned AlexNet 66.5\\x061.5% 5 folds',\n",
       " 'Cimpoi et al. [3] 67.1\\x060.4% 14 splits\\nFine-tuned AlexNet 66.5\\x061.5% 5 folds\\nSIFT IFV+fc7 69.6\\x060.3% 10 splits\\nTable 6. FMD experiments. By replacing DeCAF features with\\noversampled AlexNet features we improve on the best FMD result.\\nSIFT IFV+fc7 achieves 67.4 \\x060.5% with a linear SVM\\n(oversampling, 5 splits). This experiment shows that\\na ﬁnetuned CNN is a better method for MINC than\\nSIFT IFV+fc7.\\n6. Conclusion\\nMaterial recognition is a long-standing, challenging problem. We introduce a new large, open, material database,\\nMINC , that includes a diverse range of materials of everyday scenes and staged designed interiors, and is at least an\\norder of magnitude larger than prior databases. Using this\\nlarge database we conduct an evaluation of recent deep learning algorithms for simultaneous material classiﬁcation and\\nsegmentation, and achieve results that surpass prior attempts\\nat material recognition.\\nSome lessons we have learned are:\\n\\x0fTraining on a dataset which includes the surrounding\\ncontext is crucial for real-world material classiﬁcation.',\n",
       " 'at material recognition.\\nSome lessons we have learned are:\\n\\x0fTraining on a dataset which includes the surrounding\\ncontext is crucial for real-world material classiﬁcation.\\n\\x0fLabeled clicks are cheap and sufﬁcient to train a CNN\\nalone. However, to obtain high quality segmentation\\nresults, training a CRF on polygons results in much\\nbetter boundaries than training on clicks.\\nMany future avenues of work remain. Expanding the\\ndataset to a broader range of categories will require new\\nways to mine images that have more variety, and new annotation tasks that are cost-effective. Inspired by attributes\\nfor textures [ 3], in the future we would like to identify material attributes and expand our database to include them. We\\nalso believe that further exploration of joint material and object classiﬁcation and segmentation will be fruitful [ 10] and\\nlead to improvements in both tasks. Our database, trained\\nmodels, and all experimental results are available online at\\nhttp://minc.cs.cornell.edu/.\\nAcknowledgements. This work was supported in part by\\nGoogle, Amazon AWS for Education, a NSERC PGS-D\\nscholarship, the National Science Foundation (grants IIS1149393, IIS-1011919, IIS-1161645), and the Intel Science\\nand Technology Center for Visual Computing.\\n8\\nReferences',\n",
       " 'and Technology Center for Visual Computing.\\n8\\nReferences\\n[1]S. Bell, P. Upchurch, N. Snavely, and K. Bala. OpenSurfaces:\\nA richly annotated catalog of surface appearance. ACM Trans.\\non Graphics (SIGGRAPH) , 32(4), 2013. 1, 2, 3\\n[2]B. Caputo, E. Hayman, and P. Mallikarjuna. Class-speciﬁc\\nmaterial categorisation. In ICCV , pages 1597–1604, 2005. 2\\n[3]M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and\\nA. Vedaldi. Describing textures in the wild. In CVPR , pages\\n3606–3613. IEEE, 2014. 2, 8\\n[4]K. J. Dana, B. Van Ginneken, S. K. Nayar, and J. J. Koenderink. Reﬂectance and texture of real-world surfaces. ACM\\nTransactions on Graphics (TOG) , 18(1):1–34, 1999. 2\\n[5]M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman. The Pascal Visual Object Classes (VOC)',\n",
       " 'and A. Zisserman. The Pascal Visual Object Classes (VOC)\\nChallenge. IJCV , 88(2):303–338, June 2010. 3\\n[6]C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\\nhierarchical features for scene labeling. PAMI , 35(8):1915–\\n1929, 2013. 2\\n[7]R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\\nfeature hierarchies for accurate object detection and semantic\\nsegmentation. In CVPR , 2014. 2\\n[8]S. Gould, R. Fulton, and D. Koller. Decomposing a scene\\ninto geometric and semantically consistent regions. In ICCV ,\\n2009. 3\\n[9]E. Hayman, B. Caputo, M. Fritz, and J. olof Eklundh. On the\\nsigniﬁcance of real-world conditions for material classiﬁcation. In ECCV , 2004. 2\\n[10] D. Hu, L. Bo, and X. Ren. Toward robust material recognition\\nfor everyday objects. In BMVC , pages 1–11. Citeseer, 2011.\\n1, 2, 8',\n",
       " 'for everyday objects. In BMVC , pages 1–11. Citeseer, 2011.\\n1, 2, 8\\n[11] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\\narchitecture for fast feature embedding. In Proceedings of\\nthe ACM International Conference on Multimedia , pages 675–\\n678. ACM, 2014. 6\\n[12] P. Kr ¨ahenb ¨uhl and V . Koltun. Parameter learning and convergent inference for dense random ﬁelds. In ICML , pages\\n513–521, 2013. 2, 4, 5\\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nAdvances in neural information processing systems , pages\\n1097–1105, 2012. 2, 5, 6, 8\\n[14] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation',\n",
       " 'Howard, W. Hubbard, and L. D. Jackel. Backpropagation\\napplied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989. 2\\n[15] T. Leung and J. Malik. Representing and recognizing the visual appearance of materials using three-dimensional textons.\\nIJCV , 43(1):29–44, June 2001. 2\\n[16] W. Li and M. Fritz. Recognizing materials from virtual examples. In ECCV , pages 345–358. Springer, 2012. 2\\n[17] C. Liu, L. Sharan, E. H. Adelson, and R. Rosenholtz. Exploring features in a bayesian framework for material recognition.\\nInCVPR , pages 239–246. IEEE, 2010. 1, 2\\n[18] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and\\ntransferring mid-level image representations using convolutional neural networks. In CVPR , 2014. 2[19] G. Patterson, C. Xu, H. Su, and J. Hays. The SUN Attribute\\nDatabase: Beyond Categories for Deeper Scene Understanding. IJCV , 108(1-2):59–81, 2014. 1',\n",
       " 'Database: Beyond Categories for Deeper Scene Understanding. IJCV , 108(1-2):59–81, 2014. 1\\n[20] X. Qi, R. Xiao, J. Guo, and L. Zhang. Pairwise rotation\\ninvariant co-occurrence local binary pattern. In ECCV , pages\\n158–171. Springer, 2012. 1, 2\\n[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg,\\nand L. Fei-Fei. ImageNet Large Scale Visual Recognition\\nChallenge, 2014. 1, 2\\n[22] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. LabelMe: A database and web-based tool for image\\nannotation. IJCV , 77(1-3):157–173, May 2008. 3\\n[23] G. Schwartz and K. Nishino. Visual material traits: Recognizing per-pixel material context. In Proceedings of the\\nInternational Conference on Computer Vision Workshops (ICCVW) , pages 883–890. IEEE, 2013. 2',\n",
       " 'International Conference on Computer Vision Workshops (ICCVW) , pages 883–890. IEEE, 2013. 2\\n[24] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localization\\nand detection using convolutional networks. In International\\nConference on Learning Representations (ICLR2014) . CBLS,\\nApril 2014. 2, 5\\n[25] L. Sharan, C. Liu, R. Rosenholtz, and E. Adelson. Recognizing materials using perceptually inspired features. IJCV ,\\n2013. 1, 8\\n[26] L. Sharan, R. Rosenholtz, and E. Adelson. Material perception: What can you see in a brief glance? Journal of Vision ,\\n9(8):784–784, 2009. 1, 2\\n[27] K. Simonyan and A. Zisserman. Very deep convolutional\\nnetworks for large-scale image recognition. arXiv preprint\\narXiv:1409.1556 , 2014. 2, 6\\n[28] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,',\n",
       " '[28] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich. Going deeper\\nwith convolutions. CVPR , 2015. 2, 6\\n[29] R. Timofte and L. J. Van Gool. A training-free classiﬁcation\\nframework for textures, writers, and materials. In BMVC ,\\npages 1–12, 2012. 2\\n[30] M. Varma and A. Zisserman. A statistical approach to texture\\nclassiﬁcation from single images. IJCV , 62(1-2):61–81, Apr.\\n2005. 2\\n[31] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva.\\nSUN Database: Exploring a large collection of scene categories. IJCV , 2014. 1, 3\\n[32] M. D. Zeiler and R. Fergus. Visualizing and understanding\\nconvolutional networks. In ECCV , pages 818–833. Springer,\\n2014. 2\\n[33] S. Zheng, M.-M. Cheng, J. Warrell, P. Sturgess, V . Vineet,',\n",
       " '2014. 2\\n[33] S. Zheng, M.-M. Cheng, J. Warrell, P. Sturgess, V . Vineet,\\nC. Rother, and P. H. Torr. Dense semantic image segmentation\\nwith objects and attributes. In CVPR , pages 3214–3221. IEEE,\\n2014. 2\\n[34] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.\\nLearning deep features for scene recognition using Places\\ndatabase. NIPS , 2014. 1, 3\\n9',\n",
       " 'DeepEdge: A Multi-Scale Bifurcated Deep Network\\nfor Top-Down Contour Detection\\nGedas Bertasius\\nUniversity of Pennsylvania\\ngberta@seas.upenn.eduJianbo Shi\\nUniversity of Pennsylvania\\njshi@seas.upenn.eduLorenzo Torresani\\nDartmouth College\\nlt@dartmouth.edu\\nAbstract\\nContour detection has been a fundamental component\\nin many image segmentation and object detection systems.\\nMost previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues\\nfor a higher-level task such as object detection. However,\\nwe claim that recognizing objects and predicting contours\\nare two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level\\ncues for a higher-level recognition task, we exploit objectrelated features as high-level cues for contour detection.\\nWe achieve this goal by means of a multi-scale deep network that consists of ﬁve convolutional layers and a bifurcated fully-connected sub-network. The section from the input layer to the ﬁfth convolutional layer is ﬁxed and directly',\n",
       " 'lifted from a pre-trained network optimized over a largescale object classiﬁcation task. This section of the network\\nis applied to four different scales of the image input. These\\nfour parallel and identical streams are then attached to\\na bifurcated sub-network consisting of two independentlytrained branches. One branch learns to predict the contour likelihood (with a classiﬁcation objective) whereas the\\nother branch is trained to learn the fraction of human labelers agreeing about the contour presence at a given point\\n(with a regression criterion).\\nWe show that without any feature engineering our multiscale deep learning approach achieves state-of-the-art results in contour detection.\\n1. Introduction\\nContour detection is typically considered a low-level\\nproblem, and used to aid higher-level tasks such as object\\ndetection [1, 3, 31, 24]. However, it can be argued that\\nthe tasks of detecting objects and predicting contours are\\nclosely related. For instance, given the contours we can\\neasily infer which objects are present in the image. Con0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91\\nRecallPrecision\\n  \\n[F=.80] Human',\n",
       " 'RecallPrecision\\n  \\n[F=.80] Human\\n[F=.753] DeepEdge\\n[F=.753] N4−fields [10]\\n[F=.747] MCG [2]\\n[F=.746] SE [8]\\n[F=.739] SCG [27]\\n[F=.737] PMI [14]\\n[F=.727] Sketch Tokens [19]\\n[F=.726] gPb−owt−ucm [1]Figure 1: Contour detection accuracy on the BSDS500\\ndataset. Our method attains higher average precision compared to prior methods and state-of-the-art F-score. At low\\nrecall, DeepEdge achieves nearly 100% precision..\\nversely, if we are given exact locations of the objects we\\ncould predict contours just as easily. A commonly established pipeline in computer vision starts with with low-level\\ncontour prediction and then moves up to higher-level object\\ndetection. However, since we claim that these two tasks are\\nmutually related, we propose to invert this process. Instead\\nof using contours as low-level cues for object detection, we\\nwant to use object-speciﬁc information as high-level cues\\nfor contour detection. Thus, in a sense our scheme can be',\n",
       " 'want to use object-speciﬁc information as high-level cues\\nfor contour detection. Thus, in a sense our scheme can be\\nviewed as a top-down approach where object-level cues inform the low-level contour detection process.\\nIn this work, we present a uniﬁed multi-scale deep learning approach that uses higher-level object information to\\n1arXiv:1412.1123v3  [cs.CV]  23 Apr 2015\\npredict contours. Speciﬁcally, we present a front-to-end\\nconvolutional architecture where contours are learned directly from raw pixels. Our proposed deep learning architecture reuses features computed by the ﬁrst ﬁve convolutional layers of the network of Krizhevsky et al. [17]. We refer to this network as the KNet . Because the KNet has been\\ntrained for object classiﬁcation, reusing its features enable\\nour method to incorporate object-level information for contour prediction. In the experimental section, we will show\\nthat this high-level object information greatly improves contour detection results.\\nFurthermore, our deﬁned architecture operates on multiple scales simultaneously and combines local and global\\ninformation from the image, which leads to signiﬁcantly\\nimproved contour detection accuracy rates.',\n",
       " 'information from the image, which leads to signiﬁcantly\\nimproved contour detection accuracy rates.\\nWe connect the features computed by the convolutional\\nlayers of the KNet at four different scales of the input with\\nalearned subnetwork that bifurcates into two branches (the\\narchitecture of our model is illustrated in Fig. 2).\\nWhat should the learning objective be? When a human\\nobserver decides if a pixel is a boundary edge, a number\\nof supporting evidence is used with object level reasoning.\\nWhile it is impossible to record such information, we do\\nhave the fraction of observers in agreement for each pixel.\\nWe argue that a learning objective that predicts the fraction\\nof human labelers in agreement can mimic human reasoning\\nbetter.\\nThus, in the bifurcated sub-network we optimize the two\\nbranches with different learning objectives. The weights in\\none branch are optimized with an edge classiﬁcation objective, while the other branch is trained to predict the fraction of human labelers in agreement, i.e., using a regression\\ncriterion. We show that predictions from the classiﬁcation\\nbranch yield high edge recall, while the outputs of the regression branch have high precision. Thus, fusing these two\\noutputs allows us to obtain excellent results with respect to',\n",
       " 'branch yield high edge recall, while the outputs of the regression branch have high precision. Thus, fusing these two\\noutputs allows us to obtain excellent results with respect to\\nboth metrics and produce state-of-the-art F-score and average precision.\\nIn summary, the use of higher-level object features, independent optimization of edge classiﬁcation and regression\\nobjectives, as well as a uniﬁed multi-scale architecture are\\nthe key characteristics that allow our method to achieve the\\nstate-of-the-art in contour detection (see Fig. 1).\\n2. Related Work\\nDeep Learning. In the recent years, deep convolutional\\nnetworks have achieved remarkable results in a wide array\\nof computer vision tasks [32, 25, 33, 17]. However, thus\\nfar, applications of convolutional networks focused on highlevel vision tasks such as face recognition, image classiﬁcation, pose estimation or scene labeling [32, 25, 33, 17]. Excellent results in these tasks beg the question whether convolutional networks could perform equally well in lowerFigure 2: Visualization of multi-scale DeepEdge network\\narchitecture. To extract candidate contour points, we run the\\nCanny edge detector. Then, around each candidate point,\\nwe extract patches at four different scales and simultaneously run them through the ﬁve convolutional layers of the',\n",
       " 'Canny edge detector. Then, around each candidate point,\\nwe extract patches at four different scales and simultaneously run them through the ﬁve convolutional layers of the\\nKNet [17]. We connect these convolutional layers to two\\nseparately-trained network branches. The ﬁrst branch is\\ntrained for classiﬁcation, while the second branch is trained\\nas a regressor. At testing time, the scalar outputs from these\\ntwo sub-networks are averaged to produce the ﬁnal score.\\nlevel vision tasks such as contour detection. In this paper,\\nwe present a convolutional architecture that achieves stateof-the-art results in a contour detection task, thus demonstrating that convolutional networks can be applied successfully for lower-level vision tasks as well.\\nEdge Detection. Most of the contour detection methods\\ncan be divided into two branches: local and global methods. Local methods perform contour detection by reasoning about small patches inside the image. Some recent\\nlocal methods include sketch tokens [18] and structured\\nedges [7], Both of these methods are trained in a supervised fashion using a random forest classiﬁer. Sketch tokens [18] pose contour detection as a multi-class classiﬁcation task and predicts a label for each of the pixels individually. Structured edges [7], on the other hand, attempt to',\n",
       " 'predict the labels of multiple pixels simultaneously.\\nGlobal methods predict contours based on the information from the full image. Some of the most successful approaches in this genre are the MCG detector [2], gPb detector [1] and sparse code gradients [26]. While sparse\\ncode gradients use supervised SVM learning [4], both gPb\\nand MCG rely on some form of spectral methods. Other\\nspectral-based methods include Normalized Cuts [30] and\\nPMI [13].\\nRecently, there have also been attempts to apply deep\\nlearning methods to the task of contour detection. While\\nSCT [20] is a sparse coding approach, both N4ﬁelds [9]\\nand DeepNet [16] use Convolutional Neural Networks\\n(CNNs) to predict contours. N4ﬁelds rely on dictionary\\na) Input Image\\n c)2ndlayer\\n d)3rdlayer\\n e)4thlayer\\n e)5thlayer\\nFigure 3: Visualization of the activation values at the selected convolutional ﬁlters of the KNet (ﬁlters are resized to the\\noriginal image dimensions). The ﬁlters in the second layer ﬁre on oriented edges inside the image. The third and fourth',\n",
       " 'original image dimensions). The ﬁlters in the second layer ﬁre on oriented edges inside the image. The third and fourth\\nconvolutional layers produce an outline of the shape of the object. The ﬁfth layer ﬁres on the speciﬁc parts of the object.\\nlearning and the use of the Nearest Neighbor algorithm\\nwithin a CNN framework while DeepNet uses a traditional\\nCNN architecture to predict contours.\\nIn comparison to these prior approaches, our work offers\\nseveral contributions. First, we deﬁne a novel multi-scale\\nbifurcated CNN architecture that enables our network to\\nachieve state-of-the-art contour detection results. Second,\\nwe avoid manual feature engineering by learning contours\\ndirectly from raw data. Finally, we believe that we are the\\nﬁrst to propose the use of high-level object features for contour detection, thus inverting the traditional pipeline relying on low-level cues for mid-level feature extraction. Our\\nexperiments show that this top-down approach for contour\\ndetection yields state-of-the-art results.\\n3. The DeepEdge Network\\nIn this section, we describe our proposed deep learning approach for contour detection. For simplicity, we ﬁrst',\n",
       " '3. The DeepEdge Network\\nIn this section, we describe our proposed deep learning approach for contour detection. For simplicity, we ﬁrst\\npresent our architecture in the single-scale scenario (subsection 3.1) and then discuss how to take advantage of multiple\\nscales (subsection 3.2).\\n3.1. Single-Scale Architecture\\nSelection of Candidate Edge Points. To extract a set\\nof candidate contours with high recall we apply the Canny\\nedge detector [5] to the input image. For each of these\\npoints we then extract a patch of ﬁxed size such that our\\ncandidate point is the center of the patch. Patches that do\\nnot ﬁt into the image boundaries are padded with the mirror\\nreﬂections of itself.\\nExtraction of High-Level Features. We then resize the\\npatch of ﬁxed size to match the input dimensions of the\\nKNet [17] and use this network to extract object-level features. The KNet is an appropriate model for our setting as it\\nhas been trained over a large number of object classes (the\\n1000 categories of the ImageNet dataset [28]) and thus captures features that are generic and useful for many object\\ncategories. While such features have been optimized for the\\ntask of object class recognition, they have been shown to',\n",
       " 'categories. While such features have been optimized for the\\ntask of object class recognition, they have been shown to\\nbe highly effective for other image-analysis tasks, includ-ing object detection [10], attribute prediction [34], and image style recognition [15]. The network was trained on 1:2\\nmillion images and it includes more than 60million parameters. Its architecture consists of 5convolutional layers and\\n3fully connected layers. As we intend to use the KNet as\\na feature extractor for boundary detection, we utilize only\\nthe ﬁrst 5convolutional layers, which preserve explicit location information before the “spatial scrambling” of the\\nfully connection layers (note that a spatially variant representation is crucially necessary to predict the presence of\\ncontours at individual pixels). The ﬁrst two KNet convolutional layers learn low-level information. As we move\\ninto the deeper layers, however, we observe that the network\\nlearns higher-level object information. The second convolutional layer seems to encode coherent edge structures. The\\nthird convolutional layer ﬁres at locations corresponding to\\nprototypical object shapes. The fourth layer appears to generate high responses for full shapes of the object, whereas\\nthe ﬁfth layer ﬁres on the speciﬁc object parts (See Fig. 3).',\n",
       " 'the ﬁfth layer ﬁres on the speciﬁc object parts (See Fig. 3).\\nIn order to obtain a representation that captures this hierarchical information, we perform feature extraction at each\\nof the ﬁve convolutional layers, as shown in Fig. 5. Specifically, we consider a small sub-volume of the feature map\\nstack produced at each layer. The sub-volume is centered\\nat the center of the patch in order to assess the presence of\\na contour in a small area around the candidate point. We\\nthen perform max,average , and center pooling on this subvolume. This yields a feature descriptor of size 3\\x02Fwhere\\nFis the number of feature maps computed by the convolutional layer. While max and average pooling are well established operations in deep learning, we deﬁne center pooling\\nas selecting the center-value from each of the feature maps.\\nThe motivation for center pooling is that for each candidate\\npoint we want to predict the contour presence at that particular point. Because the candidate point is located at the center of the input patch, center pooling extracts the activation\\nvalue from the location that corresponds to our candidate\\npoint location.\\nA Bifurcated Sub-Network. We connect the feature\\nmaps computed via pooling from the ﬁve convolutional',\n",
       " 'point location.\\nA Bifurcated Sub-Network. We connect the feature\\nmaps computed via pooling from the ﬁve convolutional\\nlayers to two separately-trained network branches. Each\\nFigure 4: A few samples of ground truth data illustrating the difference between the classiﬁcation (ﬁrst row) and the regression\\n(second row) objectives. The classiﬁcation branch is trained to detect contours that are marked by at least one of the human\\nannotators. Conversely, the regression branch is optimized to the contour values that depict the fraction of human annotators\\nagreeing on the contour.\\nbranch consists of two fully-connected layers. The ﬁrst\\nbranch is trained using binary labels, i.e., to perform contour classiﬁcation. This branch is making less selective predictions by classifying whether a given point is a contour or\\nnot. In a sense, this classiﬁcation branch abstracts details\\nrelated to the edge structure (orientation, strength, etc) and\\nsimply tries to predict the presence/absence of an edge at\\na given point. Due to such abstractions, the classiﬁcation\\nbranch produces contour predictions with high recall.\\nThe second branch is optimized as a regressor to predict',\n",
       " 'a given point. Due to such abstractions, the classiﬁcation\\nbranch produces contour predictions with high recall.\\nThe second branch is optimized as a regressor to predict\\nthe fraction of human labelers agreeing about the contour\\npresence at a particular point. Due to a regression objective,\\nthis branch is much more selective than the ﬁrst branch. Intuitively, the second branch is trained to learn the structural\\ndifferences between the contours that are marked by a different fraction of human labelers. For instance, the area that\\nwas labeled as a contour by 80% of human labelers must\\nbe signiﬁcantly different than the area that was labeled as\\na contour by 20% human labelers. The regression branch\\nis trying to learn such differences by predicting the fraction of human labelers who would mark a particular point\\nas a contour. Thus, in a sense, we are training the regression branch to implicitly mimic how human labelers reason about the contour presence at a given point. In the experimental section, we demonstrate that due to its selectivity, the regression branch produces contour predictions with\\nvery high precision. In Fig. 4, we present several samples\\nof ground truth data that illustrate the different properties of\\nour two end-objectives.\\nThe number of hidden layers in the ﬁrst and second fully',\n",
       " 'of ground truth data that illustrate the different properties of\\nour two end-objectives.\\nThe number of hidden layers in the ﬁrst and second fully\\nconnected layers of both branches are 1024 and512, respectively. Both branches optimize the sum of squared difference loss over the (binary or continuous) labels. At test-ing time, the scalar outputs computed from these two subnetworks are averaged to produce a ﬁnal score indicative of\\nthe probability that the candidate point is a contour. Visualization of this architecture is presented in Fig. 5.\\nIn order to train our sub-network, we generate patch examples and labels using training images with ground truth\\nannotations from multiple human labelers. To generate the\\nbinary labels, we ﬁrst sample 40;000positive examples that\\nwere marked as contours by at least one of the labelers. To\\ngenerate negative examples we consider the points that were\\nselected as candidate contour points by the Canny edge detector but that have not been marked as contours by any of\\nthe human labelers. These are essentially false positives.\\nFor training, we use a random subset of 40;000 of such\\npoints in order to have equal proportion of negative and\\npositive examples. These 80;000examples are then used\\nto train our classiﬁcation sub-network.',\n",
       " 'points in order to have equal proportion of negative and\\npositive examples. These 80;000examples are then used\\nto train our classiﬁcation sub-network.\\nIn addition to the binary labels, we also generate continuous labels that are used to train the regression network.\\nFor this purpose, we deﬁne the regression label of a point to\\nbe the fraction of human labelers that marked the point as a\\ncontour. These 80;000examples with continuous labels are\\nthen also used to train our regression sub-network.\\n3.2. Multi-Scale Architecture\\nIn the previous section we presented a convolutional architecture for contour prediction utilizing a single scale.\\nHowever, in practice, we found that a multi-scale approach\\nworks much better. In this section, we show how to modify\\nthe single-scale architecture so that it can exploit multiple\\nscales simultaneously.\\nRather than extracting a patch at a single scale as we did\\nin the previous section, in a multi-scale setting we extract\\n|{z}\\n|{z}Fixed WeightsLearned WeightsClassiﬁcation Branch\\nRegression Branch\\n}\\n}Figure 5: Detailed illustration of our proposed architecture in a single-scale setting. First, an input patch, centered around the',\n",
       " 'Regression Branch\\n}\\n}Figure 5: Detailed illustration of our proposed architecture in a single-scale setting. First, an input patch, centered around the\\ncandidate point, goes through ﬁve convolutional layers of the KNet . To extract high-level features, at each convolutional layer\\nwe extract a small sub-volume of the feature map around the center point, and perform max,average , and center pooling on\\nthis sub-volume. The pooled values feed a bifurcated sub-network. At testing time, the scalar outputs computed from the\\nbranches of a bifurcated sub-networks are averaged to produce a ﬁnal contour prediction.\\npatches around the candidate point for different patch sizes\\nso that they cover different spatial extents of the image. We\\nthen resize the patches to ﬁt the KNet input and pump them\\nin parallel through the ﬁve convolutional layers. Our highlevel features are then built by performing max, average and\\ncenter pooling in a small sub-volume of the feature map at\\neach convolutional layer and at each scale. This effectively\\nincreases the dimensionality of the feature vector by a factor\\nequal to the number of scales compared to the single-scale\\nsetting. These pooled features are then connected as before',\n",
       " 'increases the dimensionality of the feature vector by a factor\\nequal to the number of scales compared to the single-scale\\nsetting. These pooled features are then connected as before\\nto the two separately-trained network branches. A visualization of our multi-scale architecture is shown in Fig. 2.\\n3.3. Implementation Details\\nIn this section, we describe additional implementation\\ndetails of our model. Our deep network is implemented using the software library Caffe [14].\\nWe use four different scales for our patches. The sizes\\nof these patches are 64\\x0264;128\\x02128;196\\x02196and a\\nfull-sized image. All of the patches are then resized to the\\nKNet input dimensions of 227\\x02227.\\nWhen extracting high-level features from the convolutional layers of KNet , we use sub-volumes of convolutionalfeature maps having spatial sizes 7\\x027;5\\x025;3\\x023;3\\x023,\\nand3\\x023for the convolutional layers 1;2;3;4;5, respectively. Note that we shrink the size of the subvolume as we\\ngo deeper in the network since the feature maps get smaller\\ndue to pooling. Our choice of subvolume sizes is made to\\nensure we are roughly considering the same spatial extent\\nof the original image at each layer.\\nAs illustrated in Fig. 5, during the training the weights',\n",
       " 'ensure we are roughly considering the same spatial extent\\nof the original image at each layer.\\nAs illustrated in Fig. 5, during the training the weights\\nin the convolutional layers are ﬁxed and only the weights in\\nthe fully connected layers of the two branches are learned.\\nTo train our model we use the learning rate of 0:1, the\\ndropout fraction of 0:5,50number of epochs, and the size\\nof the batch equal to 100.\\nAs described earlier, to train classiﬁcation and regression branches we sample 80;000examples with binary labels. We also generate continuous labels for these 80;000\\nexamples. In addition, we sample a hold-out dataset of size\\n40;000. This hold-out dataset is used for the hard-positive\\nmining step [22].\\nFor the ﬁrst 25epochs we train classiﬁcation and regression branches independently on the original 80;000sample training dataset. After the ﬁrst 25epochs, we test both\\nbranches on the hold-out dataset and detect false negative\\npredictions made by each branch. We then use these false\\nnegative examples along with the same number of randomly\\nselected true negative examples to augment our original\\n80;000training dataset. For the remaining 25epochs, we\\ntrain both branches on this augmented dataset.',\n",
       " 'selected true negative examples to augment our original\\n80;000training dataset. For the remaining 25epochs, we\\ntrain both branches on this augmented dataset.\\nThe motivation for the hard-positive mining step is to reduce the number of false negative predictions produced by\\nboth branches. By augmenting the original 80;000sized\\ntraining data with false negative examples, we are forcing\\nboth branches to focus on hard positive examples, and thus,\\neffectively reducing the number of false negative predictions.\\n4. Experiments\\nIn this section, we present our results on the BSDS500\\ndataset [23], which is arguably the most established benchmark for contour detection. This dataset contains 200training images, 100 validation images, and 200 test images.\\nContour detection accuracy is evaluated using three standard measures: ﬁxed contour threshold (ODS), per-image\\nbest threshold (OIS), and average precision (AP).\\nIn section 4.1 we quantitatively compare our approach\\nto the state-of-the-art. In sections 4.2-4.5 we study how the\\nperformance of our system changes as we modify some of\\nthe architecture choices (number of scales, feature maps,\\npooling scheme, training objective). This will cast additional insight into the factors that critically contribute to the\\nhigh accuracy of our system.\\n4.1. Comparison with Other Methods',\n",
       " 'pooling scheme, training objective). This will cast additional insight into the factors that critically contribute to the\\nhigh accuracy of our system.\\n4.1. Comparison with Other Methods\\nWe compare the results produced by our approach and\\npreviously proposed contour detection methods. Table 1\\nsummarizes the results. We note that our algorithm achieves\\ncontour detection accuracy that is higher or equal to stateof-the-art results according to two of the three metrics.\\nFig. 1 shows the precision and recall curve for the methods considered in our comparison. It also lists the F-score\\nfor each method (in the legend). We observe that there is the\\naccuracy margin separating our approach from prior techniques. In particular, for low-recall our method achieves almost perfect precision rate. It also produces state-of-the-art\\nF-score.\\n4.2. Single Scale versus Multiple Scales\\nIn this section we study the beneﬁts of a multi-scale architecture. Results in Table 2 report accuracy for different numbers and choices of scales. The ﬁrst four rows in\\nthe table illustrate the results achieved using a single-scale\\napproach. Speciﬁcally, these four cases show performance\\nobtained when training and testing our system with an input',\n",
       " 'the table illustrate the results achieved using a single-scale\\napproach. Speciﬁcally, these four cases show performance\\nobtained when training and testing our system with an input\\npatch of size 64\\x0264;128\\x02128;196\\x02196or a full-sized image, respectively. Note that adding information from multiple scales leads to signiﬁcantly higher F-scores and higherMethod ODS OIS AP\\nFelz., Hutt. [8] 0.610 0.640 0.560\\nMean Shift [6] 0.640 0.680 0.560\\nNcuts [30] 0.640 0.680 0.450\\nSCT [20] 0.710 0.720 0.740\\ngPb-owt-ucm [1] 0.726 0.757 0.696\\nSketch Tokens [18] 0.727 0.746 0.780\\nPMI [13] 0.737 0.771 0.783\\nDeepNet [16] 0.738 0.759 0.758\\nSCG [26] 0.739 0.758 0.773\\nSE [7] 0.746 0.767 0.803\\nMCG [2] 0.747 0.779 0.759\\nN4-ﬁelds [9] 0.753 0.769 0.784',\n",
       " 'MCG [2] 0.747 0.779 0.759\\nN4-ﬁelds [9] 0.753 0.769 0.784\\nDeepEdge 0.753 0.772 0.807\\nTable 1: Edge detection results on the BSDS500 benchmark. Our DeepEdge method achieves state-of-the-art contour detections results according to both F-score and AP\\nmetrics.\\nScale ODS OIS AP\\n64 0.71 0.73 0.76\\n128 0.72 0.74 0.78\\n196 0.71 0.73 0.76\\nFull Image 0.67 0.69 0.57\\n64, 128 0.72 0.75 0.78\\n64, 128,196 0.72 0.75 0.78\\n64,128,196,Full Image 0.75 0.77 0.81\\nTable 2: Results illustrating the effect of using a multi-scale\\narchitecture. Considering multiple scales for contour detection yields signiﬁcantly higher accuracy relative to a single\\nscale approach.\\naverage precisions. Thus, these results suggest that a multiscale approach is highly advantageous in comparison to a\\nsingle scale setting.\\n4.3. Advantages of Higher-Level Features\\nIn this section, we examine the validity of our earlier',\n",
       " 'single scale setting.\\n4.3. Advantages of Higher-Level Features\\nIn this section, we examine the validity of our earlier\\nclaim that higher-level object-features enhance contour detection accuracy. In Table 3, we present individual contour\\ndetection results using features from the different convolutional layers of KNet . Note that the 4thconvolutional layer\\nproduces the most effective features when considering one\\nlayer at a time. From our earlier discussion we know that\\nthe4thconvolutional layer encodes higher-level object information related to shape and speciﬁc object parts. This\\nindicates that object speciﬁc cues are particularly beneﬁcial\\nfor contour detection accuracy.\\nWe also observe that by incorporating features from all\\nInput Image\\n Canny Edges\\n Raw DeepEdges\\n Thresholded DeepEdges\\n Ground Truth Edges\\nFigure 6: Qualitative results produced by our method. Notice how our method learns to distinguish between strong and\\nweak contours. For instance, in the last row of predictions, contours corresponding to zebra stripes are assigned much lower\\nprobabilities than contours that correspond to the actual object boundaries separating the zebras from the background.\\nConv. Layers ODS OIS AP\\n1st0.66 0.68 0.69\\n2nd0.71 0.74 0.76',\n",
       " 'Conv. Layers ODS OIS AP\\n1st0.66 0.68 0.69\\n2nd0.71 0.74 0.76\\n3rd0.74 0.75 0.79\\n4th0.74 0.76 0.79\\n5th0.73 0.74 0.77\\nAll 0.75 0.77 0.81\\nTable 3: This table shows the advantage of using higherlevel features from the KNet convolutional layers. Individually, the 4thconvolutional layer produces the best contour\\nprediction results, which implies that higher-level object information is indeed beneﬁcial for contour detection. Combining the features from all convolutional layers leads to\\nstate-of-the-art results.\\nthe convolutional layers, our method achieves state-of-theart contour detection results. This suggests that the features\\ncomputed by different layers are complementary and that\\nconsidering information from the entire hierarchy is advantageous.4.4. Pooling Scheme\\nWhen presenting the architecture of our model, we discussed three different types of pooling: max,average , and\\ncenter pooling. These three techniques were used to pool\\nthe values from the sub-volumes extracted around the center point in each convolutional ﬁlter as illustrated in Fig. 5.\\nWe now show how each type of pooling affects contour',\n",
       " 'We now show how each type of pooling affects contour\\ndetection results. Table 4 illustrates that, individually, center pooling yields the best contour detection results. This is\\nexpected because the candidate point for which we are trying to predict a contour probability is located at the center\\nof the input patch.\\nHowever, we note that combining all three types of pooling, achieves better contour detection results than any single\\npooling technique alone.\\n4.5. Bifurcation and Training Objective\\nNext, we want to show that that the two independentlytrained classiﬁcation and regression branches in the bifurcated sub-network provide complementary information that\\nyields improved contour detection accuracy. In Table 5, we\\npresent contour detection results achieved by using predictions from the individual branches of the bifurcated subnetwork.\\nPooling Type ODS OIS AP\\nAverage 0.73 0.75 0.78\\nMax 0.69 0.72 0.73\\nCenter 0.74 0.76 0.8\\nAvg+Max+Cen 0.75 0.77 0.81\\nTable 4: Effect of different pooling schemes on contour detection results. Center pooling produces better results than\\nmax or average pooling. Combining all three types of pooling further improves the results.\\nBranch ODS OIS AP',\n",
       " 'max or average pooling. Combining all three types of pooling further improves the results.\\nBranch ODS OIS AP\\nClassiﬁcation 0.75 0.76 0.78\\nRegression 0.74 0.76 0.80\\nClassiﬁcation+Regression 0.75 0.77 0.81\\nTable 5: Contour detection accuracy of the two branches\\nin our bifurcated sub-network. The classiﬁcation branch\\nyields solid F-score results whereas the regression branch\\nachieves high average precision. Averaging the outputs\\nfrom these two branches further improve the results.\\nFrom these results, we observe that using predictions\\njust from the classiﬁcation branch produces high F-score\\nwhereas using predictions only from the regression branch\\nyields high average precision. Combining the predictions\\nfrom both branches improves the results according to both\\nmetrics thus, supporting our claim that separately optimizing edge classiﬁcation and regression objectives is beneﬁcial to contour detection.\\n4.6. Qualitative Results\\nFinally, we present qualitative results produced by our\\nmethod. In Figure 6 we show for each input image example, the set of candidate points produced by the Canny edge\\ndetector, the un-thresholded predictions of our method, the',\n",
       " 'method. In Figure 6 we show for each input image example, the set of candidate points produced by the Canny edge\\ndetector, the un-thresholded predictions of our method, the\\nthresholded predictions, and the ground truth contour map\\ncomputed as an average of the multiple manual annotations.\\nTo generate the thresholded predictions, we use a probability threshold of 0:5.\\nNote that our method successfully distinguishes between\\nstrong and weak contours. Speciﬁcally, observe that in the\\nlast row of Figure 6, our method assigns lower probability\\nto contours corresponding to zebra stripes compared to the\\ncontours of the actual object boundary separating the zebras\\nfrom the background. Thus, in the thresholded version of\\nthe prediction, the weak contours inside the zebra bodies\\nare removed and we obtain contour predictions that look\\nvery similar to the ground truth.\\nDue to locality of our method, it may be beneﬁcial\\nto apply spectral methods [21, 30] or conditional randomﬁelds [27] on top of our method to further improve its performance.\\n4.7. Computational Cost\\nIn its current form, our method requires about 60KKNet\\nevaluations ( 15Kper scale) to extract the features. Based\\non the runtimes reported in [14], if executed on a GPU our',\n",
       " 'evaluations ( 15Kper scale) to extract the features. Based\\non the runtimes reported in [14], if executed on a GPU our\\nmethod would take about 5 minutes and could be made\\nfaster using the approach described in [12].\\nAn alternative way to dramatically reduce the runtime of\\nDeepEdge is to interpolate the entries of the feature maps\\nproduced by applying the KNet to the full image rather than\\nto individual patches. Such an approach would reduce the\\nnumber of CNN evaluations needed from 60Kto4(one\\nfor each scale), which would allow our method to run in\\nreal time even on CPUs. We note that interpolation of features in deep layers has been used successfully in several\\nrecent vision papers [29, 11, 19]. Thus, we believe that such\\nan approach could yield nearly equivalent contour detection\\naccuracy, up to a small possible degradation caused by interpolation.\\nSince in this work we were primarily interested in studying the effective advantage enabled by object-level features\\nin contour detection, we have not invested any effort in optimizing the implementation of our method. This will be one\\nof our immediate goals in the future.\\n5. Conclusions\\nIn this work, we presented a multi-scale bifurcated deep\\nnetwork for top-down contour detection. In the past, contour detection has been approached as a bottom-up task',\n",
       " 'network for top-down contour detection. In the past, contour detection has been approached as a bottom-up task\\nwhere low-level features are engineered ﬁrst, then contour\\ndetection is performed, and ﬁnally contours may be used\\nas cues for object detection. However, due to a close relationship between object and contour detection tasks, we\\nproposed to invert this pipeline and perform contour detection in a top-down fashion. We demonstrated how to use\\nhigher-level object cues to predict contours and showed that\\nconsidering higher-level object-features leads to a substantial gain in contour detection accuracy.\\nAdditionally, we demonstrated that our multi-scale architecture is beneﬁcial to contour prediction as well. By\\nconsidering multiple scales, our method incorporates local\\nand global information around the candidate contour points,\\nwhich leads to signiﬁcantly better contour detection results.\\nFurthermore, we showed that independent optimization of\\ncontour classiﬁcation and regression objectives improves\\ncontour prediction accuracy as well. As our experiments indicate, DeepEdge achieves higher average precision results\\ncompared to any prior or concurrent work.\\nIn conclusion, our results suggest that pure CNN systems can be applied successfully to contour detection and\\npossibly to many other low-level vision tasks.\\n6. Acknowledgements',\n",
       " 'In conclusion, our results suggest that pure CNN systems can be applied successfully to contour detection and\\npossibly to many other low-level vision tasks.\\n6. Acknowledgements\\nWe thank Piotr Teterwak, Du Tran and Mohammad Haris\\nBaig for helpful discussions. This research was funded in\\npart by NSF award CNS-1205521.\\nReferences\\n[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour\\ndetection and hierarchical image segmentation. IEEE Trans.\\nPattern Anal. Mach. Intell. , 33(5):898–916, May 2011. 1, 2,\\n6\\n[2] P. Arbelaez, J. Pont-Tuset, J. Barron, F. Marqu ´es, and J. Malik. Multiscale combinatorial grouping. In Computer Vision\\nand Pattern Recognition (CVPR) , 2014. 2, 6\\n[3] E. Borenstein. Combining top-down and bottom-up segmentation. In In Proceedings IEEE workshop on Perceptual Organization in Computer Vision, CVPR , page 46, 2004. 1\\n[4] C. J. C. Burges. A tutorial on support vector machines for\\npattern recognition. Data Mining and Knowledge Discovery ,\\n2:121–167, 1998. 2',\n",
       " '[4] C. J. C. Burges. A tutorial on support vector machines for\\npattern recognition. Data Mining and Knowledge Discovery ,\\n2:121–167, 1998. 2\\n[5] J. Canny. A computational approach to edge detection. IEEE\\nTrans. Pattern Anal. Mach. Intell. , 8(6):679–698, June 1986.\\n3\\n[6] D. Comaniciu, P. Meer, and S. Member. Mean shift: A robust\\napproach toward feature space analysis. IEEE Transactions\\non Pattern Analysis and Machine Intelligence , 24:603–619,\\n2002. 6\\n[7] P. Doll ´ar and C. L. Zitnick. Fast edge detection using structured forests. PAMI , 2015. 2, 6\\n[8] P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient\\ngraph-based image segmentation. Int. J. Comput. Vision ,\\n59(2):167–181, Sept. 2004. 6\\n[9] Y . Ganin and V . S. Lempitsky. N4-ﬁelds: Neural network\\nnearest neighbor ﬁelds for image transforms. ACCV , 2014.\\n2, 6',\n",
       " 'nearest neighbor ﬁelds for image transforms. ACCV , 2014.\\n2, 6\\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic\\nsegmentation. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2014. 3\\n[11] B. Hariharan, P. A. Arbel ´aez, R. B. Girshick, and J. Malik.\\nHypercolumns for object segmentation and ﬁne-grained localization. CoRR , abs/1411.5752, 2014. 8\\n[12] F. N. Iandola, M. W. Moskewicz, S. Karayev, R. B. Girshick,\\nT. Darrell, and K. Keutzer. Densenet: Implementing efﬁcient\\nconvnet descriptor pyramids. CoRR , abs/1404.1869, 2014. 8\\n[13] P. Isola, D. Zoran, D. Krishnan, and E. H. Adelson. Crisp\\nboundary detection using pointwise mutual information. In\\nECCV , 2014. 2, 6',\n",
       " 'boundary detection using pointwise mutual information. In\\nECCV , 2014. 2, 6\\n[14] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093 , 2014. 5, 8\\n[15] S. Karayev, A. Hertzmann, H. Winnemoeller, A. Agarwala, and T. Darrell. Recognizing image style. CoRR ,\\nabs/1311.3715, 2013. 3[16] J. J. Kivinen, C. K. Williams, N. Heess, and D. Technologies. Visual boundary prediction: A deep neural prediction\\nnetwork and quality dissection. AISTATS , 1(2):9, 2014. 2, 6\\n[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nF. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25 ,',\n",
       " 'F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25 ,\\npages 1097–1105. Curran Associates, Inc., 2012. 2, 3\\n[18] J. Lim, C. L. Zitnick, and P. Doll ´ar. Sketch tokens: A learned\\nmid-level representation for contour and object detection. In\\nCVPR , 2013. 2, 6\\n[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\\nnetworks for semantic segmentation. CoRR , abs/1411.4038,\\n2014. 8\\n[20] M. Maire, S. X. Yu, and P. Perona. Reconstructive sparse\\ncode transfer for contour detection and semantic labeling. In\\nAsian Conference on Computer Vision (ACCV) , 2014. 2, 6\\n[21] J. Malik, S. Belongie, T. Leung, and J. Shi. Contour and texture analysis for image segmentation. Int. J. Comput. Vision ,\\n43(1):7–27, June 2001. 8\\n[22] T. Malisiewicz, A. Gupta, and A. A. Efros. Ensemble of\\nexemplar-svms for object detection and beyond. In ICCV ,',\n",
       " 'exemplar-svms for object detection and beyond. In ICCV ,\\n2011. 5\\n[23] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database\\nof human segmented natural images and its application to\\nevaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l Conf. Computer Vision , volume 2, pages 416–423, July 2001. 6\\n[24] A. Opelt and A. Zisserman. A boundary-fragment-model for\\nobject detection. In In ECCV , pages 575–588, 2006. 1\\n[25] P. H. O. Pinheiro and R. Collobert. Recurrent convolutional\\nneural networks for scene parsing. CoRR , abs/1306.2795,\\n2013. 2\\n[26] X. Ren and L. Bo. Discriminatively Trained Sparse Code\\nGradients for Contour Detection. In Advances in Neural Information Processing Systems , December 2012. 2, 6\\n[27] X. Ren, C. C. Fowlkes, and J. Malik. Scale-invariant contour\\ncompletion using condition random ﬁelds. Technical report.\\n8',\n",
       " 'completion using condition random ﬁelds. Technical report.\\n8\\n[28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\\nRecognition Challenge, 2014. 3\\n[29] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR ,\\nabs/1312.6229, 2013. 8\\n[30] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , 22:888–905, 1997. 2, 6, 8\\n[31] J. Shotton. Contour-based learning for object detection. In\\nIn Proc. ICCV , pages 503–510, 2005. 1\\n[32] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:',\n",
       " '[32] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:\\nClosing the gap to human-level performance in face veriﬁcation. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2014. 2\\n[33] A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. CoRR , abs/1312.4659, 2013.\\n2\\n[34] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdev.\\nPanda: Pose aligned networks for deep attribute modeling.\\nCoRR , abs/1311.5591, 2013. 3',\n",
       " 'Published as a conference paper at ICLR 2015\\nSEMANTIC IMAGE SEGMENTATION WITH DEEP CONVOLUTIONAL NETS AND FULLY CONNECTED CRF S\\nLiang-Chieh Chen\\nUniv. of California, Los Angeles\\nlcchen@cs.ucla.edu\\nGeorge Papandreou\\x03\\nGoogle Inc.\\ngpapan@google.com\\nIasonas Kokkinos\\nCentraleSup ´elec and INRIA\\niasonas.kokkinos@ecp.fr\\nKevin Murphy\\nGoogle Inc.\\nkpmurphy@google.com\\nAlan L. Yuille\\nUniv. of California, Los Angeles\\nyuille@stat.ucla.edu\\nABSTRACT\\nDeep Convolutional Neural Networks (DCNNs) have recently shown state of the\\nart performance in high level vision tasks, such as image classiﬁcation and object detection. This work brings together methods from DCNNs and probabilistic\\ngraphical models for addressing the task of pixel-level classiﬁcation (also called\\n”semantic image segmentation”). We show that responses at the ﬁnal layer of\\nDCNNs are not sufﬁciently localized for accurate object segmentation. This is',\n",
       " 'DCNNs are not sufﬁciently localized for accurate object segmentation. This is\\ndue to the very invariance properties that make DCNNs good for high level tasks.\\nWe overcome this poor localization property of deep networks by combining the\\nresponses at the ﬁnal DCNN layer with a fully connected Conditional Random\\nField (CRF). Qualitatively, our “DeepLab” system is able to localize segment\\nboundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic\\nimage segmentation task, reaching 71.6% IOU accuracy in the test set. We show\\nhow these results can be obtained efﬁciently: Careful network re-purposing and a\\nnovel application of the ’hole’ algorithm from the wavelet community allow dense\\ncomputation of neural net responses at 8 frames per second on a modern GPU.\\n1 I NTRODUCTION\\nDeep Convolutional Neural Networks (DCNNs) had been the method of choice for document recognition since LeCun et al. (1998), but have only recently become the mainstream of high-level vision',\n",
       " 'research. Over the past two years DCNNs have pushed the performance of computer vision systems to soaring heights on a broad array of high-level problems, including image classiﬁcation\\n(Krizhevsky et al., 2013; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014;\\n\\x03Work initiated when G.P. was with the Toyota Technological Institute at Chicago. The ﬁrst two authors\\ncontributed equally to this work.\\n1arXiv:1412.7062v4  [cs.CV]  7 Jun 2016\\nPublished as a conference paper at ICLR 2015\\nPapandreou et al., 2014), object detection (Girshick et al., 2014), ﬁne-grained categorization (Zhang\\net al., 2014), among others. A common theme in these works is that DCNNs trained in an end-to-end\\nmanner deliver strikingly better results than systems relying on carefully engineered representations,\\nsuch as SIFT or HOG features. This success can be partially attributed to the built-in invariance of\\nDCNNs to local image transformations, which underpins their ability to learn hierarchical abstractions of data (Zeiler & Fergus, 2014). While this invariance is clearly desirable for high-level vision',\n",
       " 'tasks, it can hamper low-level tasks, such as pose estimation (Chen & Yuille, 2014; Tompson et al.,\\n2014) and semantic segmentation - where we want precise localization, rather than abstraction of\\nspatial details.\\nThere are two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial ‘insensitivity’ (invariance). The ﬁrst problem relates to the reduction of signal\\nresolution incurred by the repeated combination of max-pooling and downsampling (‘striding’) performed at every layer of standard DCNNs (Krizhevsky et al., 2013; Simonyan & Zisserman, 2014;\\nSzegedy et al., 2014). Instead, as in Papandreou et al. (2014), we employ the ‘atrous’ (with holes)\\nalgorithm originally developed for efﬁciently computing the undecimated discrete wavelet transform\\n(Mallat, 1999). This allows efﬁcient dense computation of DCNN responses in a scheme substantially simpler than earlier solutions to this problem (Giusti et al., 2013; Sermanet et al., 2013).',\n",
       " 'The second problem relates to the fact that obtaining object-centric decisions from a classiﬁer requires invariance to spatial transformations, inherently limiting the spatial accuracy of the DCNN\\nmodel. We boost our model’s ability to capture ﬁne details by employing a fully-connected Conditional Random Field (CRF). Conditional Random Fields have been broadly used in semantic segmentation to combine class scores computed by multi-way classiﬁers with the low-level information\\ncaptured by the local interactions of pixels and edges (Rother et al., 2004; Shotton et al., 2009) or\\nsuperpixels (Lucchi et al., 2011). Even though works of increased sophistication have been proposed\\nto model the hierarchical dependency (He et al., 2004; Ladicky et al., 2009; Lempitsky et al., 2011)\\nand/or high-order dependencies of segments (Delong et al., 2012; Gonfaus et al., 2010; Kohli et al.,\\n2009; Chen et al., 2013; Wang et al., 2015), we use the fully connected pairwise CRF proposed by\\nKr¨ahenb ¨uhl & Koltun (2011) for its efﬁcient computation, and ability to capture ﬁne edge details',\n",
       " 'Kr¨ahenb ¨uhl & Koltun (2011) for its efﬁcient computation, and ability to capture ﬁne edge details\\nwhile also catering for long range dependencies. That model was shown in Kr ¨ahenb ¨uhl & Koltun\\n(2011) to largely improve the performance of a boosting-based pixel-level classiﬁer, and in our work\\nwe demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level\\nclassiﬁer.\\nThe three main advantages of our “DeepLab” system are (i) speed: by virtue of the ‘atrous’ algorithm, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF\\nrequires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of Mostajabi et al. (2014) by a margin\\nof 7.2 %and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs.\\n2 R ELATED WORK',\n",
       " 'of 7.2 %and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs.\\n2 R ELATED WORK\\nOur system works directly on the pixel representation, similarly to Long et al. (2014). This is in contrast to the two-stage approaches that are now most common in semantic segmentation with DCNNs:\\nsuch techniques typically use a cascade of bottom-up image segmentation and DCNN-based region\\nclassiﬁcation, which makes the system commit to potential errors of the front-end segmentation system. For instance, the bounding box proposals and masked regions delivered by (Arbel ´aez et al.,\\n2014; Uijlings et al., 2013) are used in Girshick et al. (2014) and (Hariharan et al., 2014b) as inputs\\nto a DCNN to introduce shape information into the classiﬁcation process. Similarly, the authors of\\nMostajabi et al. (2014) rely on a superpixel representation. A celebrated non-DCNN precursor to\\nthese works is the second order pooling method of (Carreira et al., 2012) which also assigns labels\\nto the regions proposals delivered by (Carreira & Sminchisescu, 2012). Understanding the perils',\n",
       " 'to the regions proposals delivered by (Carreira & Sminchisescu, 2012). Understanding the perils\\nof committing to a single segmentation, the authors of Cogswell et al. (2014) build on (Yadollahpour et al., 2013) to explore a diverse set of CRF-based segmentation proposals, computed also by\\n(Carreira & Sminchisescu, 2012). These segmentation proposals are then re-ranked according to a\\nDCNN trained in particular for this reranking task. Even though this approach explicitly tries to\\nhandle the temperamental nature of a front-end segmentation algorithm, there is still no explicit ex2\\nPublished as a conference paper at ICLR 2015\\nploitation of the DCNN scores in the CRF-based segmentation algorithm: the DCNN is only applied\\npost-hoc, while it would make sense to directly try to use its results during segmentation.\\nMoving towards works that lie closer to our approach, several other researchers have considered\\nthe use of convolutionally computed DCNN features for dense image labeling. Among the ﬁrst\\nhave been Farabet et al. (2013) who apply DCNNs at multiple image resolutions and then employ a\\nsegmentation tree to smooth the prediction results; more recently, Hariharan et al. (2014a) propose to',\n",
       " 'segmentation tree to smooth the prediction results; more recently, Hariharan et al. (2014a) propose to\\nconcatenate the computed inter-mediate feature maps within the DCNNs for pixel classiﬁcation, and\\nDai et al. (2014) propose to pool the inter-mediate feature maps by region proposals. Even though\\nthese works still employ segmentation algorithms that are decoupled from the DCNN classiﬁer’s\\nresults, we believe it is advantageous that segmentation is only used at a later stage, avoiding the\\ncommitment to premature decisions.\\nMore recently, the segmentation-free techniques of (Long et al., 2014; Eigen & Fergus, 2014) directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers of a DCNN by convolutional layers. In order to deal with the spatial localization\\nissues outlined in the beginning of the introduction, Long et al. (2014) upsample and concatenate\\nthe scores from inter-mediate feature maps, while Eigen & Fergus (2014) reﬁne the prediction result\\nfrom coarse to ﬁne by propagating the coarse results to another DCNN.\\nThe main difference between our model and other state-of-the-art models is the combination of',\n",
       " 'from coarse to ﬁne by propagating the coarse results to another DCNN.\\nThe main difference between our model and other state-of-the-art models is the combination of\\npixel-level CRFs and DCNN-based ‘unary terms’. Focusing on the closest works in this direction,\\nCogswell et al. (2014) use CRFs as a proposal mechanism for a DCNN-based reranking system,\\nwhile Farabet et al. (2013) treat superpixels as nodes for a local pairwise CRF and use graph-cuts for\\ndiscrete inference; as such their results can be limited by errors in superpixel computations, while ignoring long-range superpixel dependencies. Our approach instead treats every pixel as a CRF node,\\nexploits long-range dependencies, and uses CRF inference to directly optimize a DCNN-driven cost\\nfunction. We note that mean ﬁeld had been extensively studied for traditional image segmentation/edge detection tasks, e.g., (Geiger & Girosi, 1991; Geiger & Yuille, 1991; Kokkinos et al.,\\n2008), but recently Kr ¨ahenb ¨uhl & Koltun (2011) showed that the inference can be very efﬁcient for\\nfully connected CRF and particularly effective in the context of semantic segmentation.',\n",
       " 'fully connected CRF and particularly effective in the context of semantic segmentation.\\nAfter the ﬁrst version of our manuscript was made publicly available, it came to our attention that\\ntwo other groups have independently and concurrently pursued a very similar direction, combining\\nDCNNs and densely connected CRFs (Bell et al., 2014; Zheng et al., 2015). There are several\\ndifferences in technical aspects of the respective models. Bell et al. (2014) focus on the problem\\nof material classiﬁcation, while Zheng et al. (2015) unroll the CRF mean-ﬁeld inference steps to\\nconvert the whole system into an end-to-end trainable feed-forward network.\\nWe have updated our proposed “DeepLab” system with much improved methods and results in our\\nlatest work (Chen et al., 2016). We refer the interested reader to the paper for details.\\n3 C ONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING\\nHerein we describe how we have re-purposed and ﬁnetuned the publicly available Imagenetpretrained state-of-art 16-layer classiﬁcation network of (Simonyan & Zisserman, 2014) (VGG-16)\\ninto an efﬁcient and effective dense feature extractor for our dense semantic image segmentation',\n",
       " 'into an efﬁcient and effective dense feature extractor for our dense semantic image segmentation\\nsystem.\\n3.1 E FFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE\\nALGORITHM\\nDense spatial score evaluation is instrumental in the success of our dense CNN feature extractor. As\\na ﬁrst step to implement this, we convert the fully-connected layers of VGG-16 into convolutional\\nones and run the network in a convolutional fashion on the image at its original resolution. However\\nthis is not enough as it yields very sparsely computed detection scores (with a stride of 32 pixels). To\\ncompute scores more densely at our target stride of 8 pixels, we develop a variation of the method\\npreviously employed by Giusti et al. (2013); Sermanet et al. (2013). We skip subsampling after\\nthe last two max-pooling layers in the network of Simonyan & Zisserman (2014) and modify the\\nconvolutional ﬁlters in the layers that follow them by introducing zeros to increase their length ( 2\\x02in\\n3\\nPublished as a conference paper at ICLR 2015\\nInput stride\\nOutput stride\\nFigure 1: Illustration of the hole algorithm in 1-D, when kernel size = 3 ,input stride = 2 , and',\n",
       " 'Input stride\\nOutput stride\\nFigure 1: Illustration of the hole algorithm in 1-D, when kernel size = 3 ,input stride = 2 , and\\noutput stride = 1 .\\nthe last three convolutional layers and 4\\x02in the ﬁrst fully connected layer). We can implement this\\nmore efﬁciently by keeping the ﬁlters intact and instead sparsely sample the feature maps on which\\nthey are applied on using an input stride of 2 or 4 pixels, respectively. This approach, illustrated\\nin Fig. 1 is known as the ‘hole algorithm’ (‘atrous algorithm’) and has been developed before for\\nefﬁcient computation of the undecimated wavelet transform (Mallat, 1999). We have implemented\\nthis within the Caffe framework (Jia et al., 2014) by adding to the im2col function (it converts multichannel feature maps to vectorized patches) the option to sparsely sample the underlying feature\\nmap. This approach is generally applicable and allows us to efﬁciently compute dense CNN feature\\nmaps at any target subsampling rate without introducing any approximations.\\nWe ﬁnetune the model weights of the Imagenet-pretrained VGG-16 network to adapt it to the image',\n",
       " 'We ﬁnetune the model weights of the Imagenet-pretrained VGG-16 network to adapt it to the image\\nclassiﬁcation task in a straightforward fashion, following the procedure of Long et al. (2014). We\\nreplace the 1000-way Imagenet classiﬁer in the last layer of VGG-16 with a 21-way one. Our\\nloss function is the sum of cross-entropy terms for each spatial position in the CNN output map\\n(subsampled by 8 compared to the original image). All positions and labels are equally weighted in\\nthe overall loss function. Our targets are the ground truth labels (subsampled by 8). We optimize the\\nobjective function with respect to the weights at all network layers by the standard SGD procedure\\nof Krizhevsky et al. (2013).\\nDuring testing, we need class score maps at the original image resolution. As illustrated in Figure 2\\nand further elaborated in Section 4.1, the class score maps (corresponding to log-probabilities) are\\nquite smooth, which allows us to use simple bilinear interpolation to increase their resolution by a\\nfactor of 8 at a negligible computational cost. Note that the method of Long et al. (2014) does not\\nuse the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the CNN',\n",
       " 'use the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the CNN\\noutput. This forced them to use learned upsampling layers, signiﬁcantly increasing the complexity\\nand training time of their system: Fine-tuning our network on PASCAL VOC 2012 takes about 10\\nhours, while they report a training time of several days (both timings on a modern GPU).\\n3.2 C ONTROLLING THE RECEPTIVE FIELD SIZE AND ACCELERATING DENSE\\nCOMPUTATION WITH CONVOLUTIONAL NETS\\nAnother key ingredient in re-purposing our network for dense score computation is explicitly controlling the network’s receptive ﬁeld size. Most recent DCNN-based image recognition methods\\nrely on networks pre-trained on the Imagenet large-scale classiﬁcation task. These networks typically have large receptive ﬁeld size: in the case of the VGG-16 net we consider, its receptive ﬁeld\\nis224\\x02224(with zero-padding) and 404\\x02404pixels if the net is applied convolutionally. After',\n",
       " 'is224\\x02224(with zero-padding) and 404\\x02404pixels if the net is applied convolutionally. After\\nconverting the network to a fully convolutional one, the ﬁrst fully connected layer has 4,096 ﬁlters of large 7\\x027spatial size and becomes the computational bottleneck in our dense score map\\ncomputation.\\nWe have addressed this practical problem by spatially subsampling (by simple decimation) the ﬁrst\\nFC layer to 4\\x024(or3\\x023) spatial size. This has reduced the receptive ﬁeld of the network down to\\n128\\x02128(with zero-padding) or 308\\x02308(in convolutional mode) and has reduced computation time\\nfor the ﬁrst FC layer by 2\\x003times. Using our Caffe-based implementation and a Titan GPU, the\\nresulting VGG-derived network is very efﬁcient: Given a 306\\x02306input image, it produces 39\\x0239\\n4\\nPublished as a conference paper at ICLR 2015\\ndense raw feature scores at the top of the network at a rate of about 8 frames/sec during testing. The\\nspeed during training is 3 frames/sec. We have also successfully experimented with reducing the\\nnumber of channels at the fully connected layers from 4,096 down to 1,024, considerably further',\n",
       " 'speed during training is 3 frames/sec. We have also successfully experimented with reducing the\\nnumber of channels at the fully connected layers from 4,096 down to 1,024, considerably further\\ndecreasing computation time and memory footprint without sacriﬁcing performance, as detailed in\\nSection 5. Using smaller networks such as Krizhevsky et al. (2013) could allow video-rate test-time\\ndense feature computation even on light-weight GPUs.\\n4 D ETAILED BOUNDARY RECOVERY : FULLY -CONNECTED CONDITIONAL\\nRANDOM FIELDS AND MULTI -SCALE PREDICTION\\n4.1 D EEPCONVOLUTIONAL NETWORKS AND THE LOCALIZATION CHALLENGE\\nAs illustrated in Figure 2, DCNN score maps can reliably predict the presence and rough position\\nof objects in an image but are less well suited for pin-pointing their exact outline. There is a natural\\ntrade-off between classiﬁcation accuracy and localization accuracy with convolutional networks:\\nDeeper models with multiple max-pooling layers have proven most successful in classiﬁcation tasks,\\nhowever their increased invariance and large receptive ﬁelds make the problem of inferring position\\nfrom the scores at their top output levels more challenging.',\n",
       " 'however their increased invariance and large receptive ﬁelds make the problem of inferring position\\nfrom the scores at their top output levels more challenging.\\nRecent work has pursued two directions to address this localization challenge. The ﬁrst approach is\\nto harness information from multiple layers in the convolutional network in order to better estimate\\nthe object boundaries (Long et al., 2014; Eigen & Fergus, 2014). The second approach is to employ\\na super-pixel representation, essentially delegating the localization task to a low-level segmentation\\nmethod. This route is followed by the very successful recent method of Mostajabi et al. (2014).\\nIn Section 4.2, we pursue a novel alternative direction based on coupling the recognition capacity\\nof DCNNs and the ﬁne-grained localization accuracy of fully connected CRFs and show that it is\\nremarkably successful in addressing the localization challenge, producing accurate semantic segmentation results and recovering object boundaries at a level of detail that is well beyond the reach\\nof existing methods.\\n4.2 F ULLY -CONNECTED CONDITIONAL RANDOM FIELDS FOR ACCURATE LOCALIZATION\\nImage/G.T. DCNN output CRF Iteration 1 CRF Iteration 2 CRF Iteration 10',\n",
       " 'Image/G.T. DCNN output CRF Iteration 1 CRF Iteration 2 CRF Iteration 10\\nFigure 2: Score map (input before softmax function) and belief map (output of softmax function) for\\nAeroplane. We show the score (1st row) and belief (2nd row) maps after each mean ﬁeld iteration.\\nThe output of last DCNN layer is used as input to the mean ﬁeld inference. Best viewed in color.\\nTraditionally, conditional random ﬁelds (CRFs) have been employed to smooth noisy segmentation\\nmaps (Rother et al., 2004; Kohli et al., 2009). Typically these models contain energy terms that\\ncouple neighboring nodes, favoring same-label assignments to spatially proximal pixels. Qualitatively, the primary function of these short-range CRFs has been to clean up the spurious predictions\\nof weak classiﬁers built on top of local hand-engineered features.\\nCompared to these weaker classiﬁers, modern DCNN architectures such as the one we use in this\\nwork produce score maps and semantic label predictions which are qualitatively different. As illustrated in Figure 2, the score maps are typically quite smooth and produce homogeneous classiﬁcation',\n",
       " 'work produce score maps and semantic label predictions which are qualitatively different. As illustrated in Figure 2, the score maps are typically quite smooth and produce homogeneous classiﬁcation\\nresults. In this regime, using short-range CRFs can be detrimental, as our goal should be to recover\\ndetailed local structure rather than further smooth it. Using contrast-sensitive potentials (Rother\\n5\\nPublished as a conference paper at ICLR 2015\\nDeep \\nConvolutional \\nNeural \\nNetwork\\nInputAeroplane\\nCoarse Score map\\nBi-linear Interpolation Fully Connected CRF Final Output\\nFigure 3: Model Illustration. The coarse score map from Deep Convolutional Neural Network (with\\nfully convolutional layers) is upsampled by bi-linear interpolation. A fully connected CRF is applied\\nto reﬁne the segmentation result. Best viewed in color.\\net al., 2004) in conjunction to local-range CRFs can potentially improve localization but still miss\\nthin-structures and typically requires solving an expensive discrete optimization problem.\\nTo overcome these limitations of short-range CRFs, we integrate into our system the fully connected\\nCRF model of Kr ¨ahenb ¨uhl & Koltun (2011). The model employs the energy function\\nE(x) =X\\ni\\x12i(xi) +X',\n",
       " 'E(x) =X\\ni\\x12i(xi) +X\\nij\\x12ij(xi;xj) (1)\\nwhere xis the label assignment for pixels. We use as unary potential \\x12i(xi) =\\x00logP(xi), where\\nP(xi)is the label assignment probability at pixel ias computed by DCNN. The pairwise potential\\nis\\x12ij(xi;xj) =\\x16(xi;xj)PK\\nm=1wm\\x01km(fi;fj), where\\x16(xi;xj) = 1 ifxi6=xj, and zero\\notherwise ( i.e., Potts Model). There is one pairwise term for each pair of pixels iandjin the image\\nno matter how far from each other they lie, i.e. the model’s factor graph is fully connected. Each km\\nis the Gaussian kernel depends on features (denoted as f) extracted for pixel iandjand is weighted\\nby parameter wm. We adopt bilateral position and color terms, speciﬁcally, the kernels are\\nw1exp\\x10\\n\\x00jjpi\\x00pjjj2\\n2\\x1b2\\x0b\\x00jjIi\\x00Ijjj2\\n2\\x1b2\\n\\x0c\\x11\\n+w2exp\\x10\\n\\x00jjpi\\x00pjjj2\\n2\\x1b2\\r\\x11\\n(2)',\n",
       " '2\\x1b2\\n\\x0c\\x11\\n+w2exp\\x10\\n\\x00jjpi\\x00pjjj2\\n2\\x1b2\\r\\x11\\n(2)\\nwhere the ﬁrst kernel depends on both pixel positions (denoted as p) and pixel color intensities\\n(denoted asI), and the second kernel only depends on pixel positions. The hyper parameters \\x1b\\x0b,\\x1b\\x0c\\nand\\x1b\\rcontrol the “scale” of the Gaussian kernels.\\nCrucially, this model is amenable to efﬁcient approximate probabilistic inference (Kr ¨ahenb ¨uhl &\\nKoltun, 2011). The message passing updates under a fully decomposable mean ﬁeld approximationb(x) =Q\\nibi(xi)can be expressed as convolutions with a Gaussian kernel in feature space.\\nHigh-dimensional ﬁltering algorithms (Adams et al., 2010) signiﬁcantly speed-up this computation\\nresulting in an algorithm that is very fast in practice, less that 0.5 sec on average for Pascal VOC\\nimages using the publicly available implementation of (Kr ¨ahenb ¨uhl & Koltun, 2011).\\n4.3 M ULTI -SCALE PREDICTION',\n",
       " 'images using the publicly available implementation of (Kr ¨ahenb ¨uhl & Koltun, 2011).\\n4.3 M ULTI -SCALE PREDICTION\\nFollowing the promising recent results of (Hariharan et al., 2014a; Long et al., 2014) we have also\\nexplored a multi-scale prediction method to increase the boundary localization accuracy. Specifically, we attach to the input image and the output of each of the ﬁrst four max pooling layers a\\ntwo-layer MLP (ﬁrst layer: 128 3x3 convolutional ﬁlters, second layer: 128 1x1 convolutional ﬁlters) whose feature map is concatenated to the main network’s last layer feature map. The aggregate\\nfeature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels. We only adjust\\nthe newly added weights, keeping the other network parameters to the values learned by the method\\nof Section 3. As discussed in the experimental section, introducing these extra direct connections\\nfrom ﬁne-resolution layers improves localization performance, yet the effect is not as dramatic as\\nthe one obtained with the fully-connected CRF.\\n6\\nPublished as a conference paper at ICLR 2015\\nMethod mean IOU (%)\\nDeepLab 59.80\\nDeepLab-CRF 63.74',\n",
       " '6\\nPublished as a conference paper at ICLR 2015\\nMethod mean IOU (%)\\nDeepLab 59.80\\nDeepLab-CRF 63.74\\nDeepLab-MSc 61.30\\nDeepLab-MSc-CRF 65.21\\nDeepLab-7x7 64.38\\nDeepLab-CRF-7x7 67.64\\nDeepLab-LargeFOV 62.25\\nDeepLab-CRF-LargeFOV 67.64\\nDeepLab-MSc-LargeFOV 64.21\\nDeepLab-MSc-CRF-LargeFOV 68.70Method mean IOU (%)\\nMSRA-CFM 61.8\\nFCN-8s 62.2\\nTTI-Zoomout-16 64.4\\nDeepLab-CRF 66.4\\nDeepLab-MSc-CRF 67.1\\nDeepLab-CRF-7x7 70.3\\nDeepLab-CRF-LargeFOV 70.3\\nDeepLab-MSc-CRF-LargeFOV 71.6\\n(a) (b)\\nTable 1: (a) Performance of our proposed models on the PASCAL VOC 2012 ‘val’ set (with training\\nin the augmented ‘train’ set). The best performance is achieved by exploiting both multi-scale',\n",
       " 'in the augmented ‘train’ set). The best performance is achieved by exploiting both multi-scale\\nfeatures and large ﬁeld-of-view. (b) Performance of our proposed models (with training in the\\naugmented ‘trainval’ set) compared to other state-of-art methods on the PASCAL VOC 2012 ‘test’\\nset.\\n5 E XPERIMENTAL EVALUATION\\nDataset We test our DeepLab model on the PASCAL VOC 2012 segmentation benchmark (Everingham et al., 2014), consisting of 20 foreground object classes and one background class. The\\noriginal dataset contains 1;464,1;449, and 1;456images for training, validation, and testing, respectively. The dataset is augmented by the extra annotations provided by Hariharan et al. (2011),\\nresulting in 10;582training images. The performance is measured in terms of pixel intersectionover-union (IOU) averaged across the 21 classes.\\nTraining We adopt the simplest form of piecewise training, decoupling the DCNN and CRF training stages, assuming the unary terms provided by the DCNN are ﬁxed during CRF training.\\nFor DCNN training we employ the VGG-16 network which has been pre-trained on ImageNet. We',\n",
       " 'For DCNN training we employ the VGG-16 network which has been pre-trained on ImageNet. We\\nﬁne-tuned the VGG-16 network on the VOC 21-way pixel-classiﬁcation task by stochastic gradient\\ndescent on the cross-entropy loss function, as described in Section 3.1. We use a mini-batch of 20\\nimages and initial learning rate of 0:001(0:01for the ﬁnal classiﬁer layer), multiplying the learning\\nrate by 0.1 at every 2000 iterations. We use momentum of 0:9and a weight decay of 0:0005 .\\nAfter the DCNN has been ﬁne-tuned, we cross-validate the parameters of the fully connected CRF\\nmodel in Eq. (2) along the lines of Kr ¨ahenb ¨uhl & Koltun (2011). We use the default values of\\nw2= 3 and\\x1b\\r= 3 and we search for the best values of w1,\\x1b\\x0b, and\\x1b\\x0cby cross-validation on a\\nsmall subset of the validation set (we use 100 images). We employ coarse-to-ﬁne search scheme.',\n",
       " 'small subset of the validation set (we use 100 images). We employ coarse-to-ﬁne search scheme.\\nSpeciﬁcally, the initial search range of the parameters are w12[5;10],\\x1b\\x0b2[50 : 10 : 100] and\\n\\x1b\\x0c2[3 : 1 : 10] (MATLAB notation), and then we reﬁne the search step sizes around the ﬁrst\\nround’s best values. We ﬁx the number of mean ﬁeld iterations to 10 for all reported experiments.\\nEvaluation on Validation set We conduct the majority of our evaluations on the PASCAL ‘val’\\nset, training our model on the augmented PASCAL ‘train’ set. As shown in Tab. 1 (a), incorporating\\nthe fully connected CRF to our model (denoted by DeepLab-CRF) yields a substantial performance\\nboost, about 4% improvement over DeepLab. We note that the work of Kr ¨ahenb ¨uhl & Koltun\\n(2011) improved the 27:6%result of TextonBoost (Shotton et al., 2009) to 29:1%, which makes the\\nimprovement we report here (from 59:8%to63:7%) all the more impressive.',\n",
       " 'improvement we report here (from 59:8%to63:7%) all the more impressive.\\nTurning to qualitative results, we provide visual comparisons between DeepLab and DeepLab-CRF\\nin Fig. 7. Employing a fully connected CRF signiﬁcantly improves the results, allowing the model\\nto accurately capture intricate object boundaries.\\nMulti-Scale features We also exploit the features from the intermediate layers, similar to Hariharan et al. (2014a); Long et al. (2014). As shown in Tab. 1 (a), adding the multi-scale features to our\\n7\\nPublished as a conference paper at ICLR 2015\\nMethod kernel size input stride receptive ﬁeld # parameters mean IOU (%) Training speed (img/sec)\\nDeepLab-CRF-7x7 7\\x027 4 224 134.3M 67.64 1.44\\nDeepLab-CRF 4\\x024 4 128 65.1M 63.74 2.90\\nDeepLab-CRF-4x4 4\\x024 8 224 65.1M 67.14 2.90\\nDeepLab-CRF-LargeFOV 3\\x023 12 224 20.5M 67.64 4.84\\nTable 2: Effect of Field-Of-View. We show the performance (after CRF) and training speed on the',\n",
       " 'Table 2: Effect of Field-Of-View. We show the performance (after CRF) and training speed on the\\nPASCAL VOC 2012 ‘val’ set as the function of (1) the kernel size of ﬁrst fully connected layer, (2)\\nthe input stride value employed in the atrous algorithm.\\nDeepLab model (denoted as DeepLab-MSc) improves about 1:5%performance, and further incorporating the fully connected CRF (denoted as DeepLab-MSc-CRF) yields about 4% improvement.\\nThe qualitative comparisons between DeepLab and DeepLab-MSc are shown in Fig. 4. Leveraging\\nthe multi-scale features can slightly reﬁne the object boundaries.\\nField of View The ‘atrous algorithm’ we employed allows us to arbitrarily control the Field-ofView (FOV) of the models by adjusting the input stride, as illustrated in Fig. 1. In Tab. 2, we\\nexperiment with several kernel sizes and input strides at the ﬁrst fully connected layer. The method,\\nDeepLab-CRF-7x7, is the direct modiﬁcation from VGG-16 net, where the kernel size = 7\\x027and\\ninput stride = 4. This model yields performance of 67:64% on the ‘val’ set, but it is relatively slow',\n",
       " 'input stride = 4. This model yields performance of 67:64% on the ‘val’ set, but it is relatively slow\\n(1:44images per second during training). We have improved model speed to 2:9images per second\\nby reducing the kernel size to 4\\x024. We have experimented with two such network variants with\\ndifferent FOV sizes, DeepLab-CRF and DeepLab-CRF-4x4; the latter has large FOV ( i.e., large\\ninput stride) and attains better performance. Finally, we employ kernel size 3\\x023and input stride =\\n12, and further change the ﬁlter sizes from 4096 to 1024 for the last two layers. Interestingly, the\\nresulting model, DeepLab-CRF-LargeFOV , matches the performance of the expensive DeepLabCRF-7x7. At the same time, it is 3:36times faster to run and has signiﬁcantly fewer parameters\\n(20.5M instead of 134.3M).\\nThe performance of several model variants is summarized in Tab. 1, showing the beneﬁt of exploiting\\nmulti-scale features and large FOV .\\nFigure 4: Incorporating multi-scale features improves the boundary segmentation. We show the',\n",
       " 'multi-scale features and large FOV .\\nFigure 4: Incorporating multi-scale features improves the boundary segmentation. We show the\\nresults obtained by DeepLab and DeepLab-MSc in the ﬁrst and second row, respectively. Best\\nviewed in color.\\nMean Pixel IOU along Object Boundaries To quantify the accuracy of the proposed model near\\nobject boundaries, we evaluate the segmentation accuracy with an experiment similar to Kohli et al.\\n(2009); Kr ¨ahenb ¨uhl & Koltun (2011). Speciﬁcally, we use the ‘void’ label annotated in val set,\\nwhich usually occurs around object boundaries. We compute the mean IOU for those pixels that\\nare located within a narrow band (called trimap) of ‘void’ labels. As shown in Fig. 5, exploiting\\nthe multi-scale features from the intermediate layers and reﬁning the segmentation results by a fully\\nconnected CRF signiﬁcantly improve the results around object boundaries.\\nComparison with State-of-art In Fig. 6, we qualitatively compare our proposed model, DeepLabCRF, with two state-of-art models: FCN-8s (Long et al., 2014) and TTI-Zoomout-16 (Mostajabi',\n",
       " 'et al., 2014) on the ‘val’ set (the results are extracted from their papers). Our model is able to\\ncapture the intricate object boundaries.\\n8\\nPublished as a conference paper at ICLR 2015\\n0 510 15 20 25 30 35 405560657075808590Pixelwise Accuracy (%)\\nTrimap Width (pixels)  \\nDL−MSc−CRF\\nDeepLab−CRF\\nDeepLab−MSc\\nDeepLab\\n0 510 15 20 25 30 35 4035404550556065mean IOU (%)\\nTrimap Width (pixels)  \\nDL−MSc−CRF\\nDeepLab−CRF\\nDeepLab−MSc\\nDeepLab\\n(a) (b) (c)\\nFigure 5: (a) Some trimap examples (top-left: image. top-right: ground-truth. bottom-left: trimap\\nof 2 pixels. bottom-right: trimap of 10 pixels). Quality of segmentation result within a band around\\nthe object boundaries for the proposed methods. (b) Pixelwise accuracy. (c) Pixel mean IOU.\\n(a) FCN-8s vs. DeepLab-CRF (b) TTI-Zoomout-16 vs. DeepLab-CRF',\n",
       " '(a) FCN-8s vs. DeepLab-CRF (b) TTI-Zoomout-16 vs. DeepLab-CRF\\nFigure 6: Comparisons with state-of-the-art models on the val set. First row: images. Second row:\\nground truths. Third row: other recent models (Left: FCN-8s, Right: TTI-Zoomout-16). Fourth\\nrow: our DeepLab-CRF. Best viewed in color.\\nReproducibility We have implemented the proposed methods by extending the excellent Caffe\\nframework (Jia et al., 2014). We share our source code, conﬁguration ﬁles, and trained models that\\nallow reproducing the results in this paper at a companion web site https://bitbucket.org/\\ndeeplab/deeplab-public .\\nTest set results Having set our model choices on the validation set, we evaluate our model variants\\non the PASCAL VOC 2012 ofﬁcial ‘test’ set. As shown in Tab. 3, our DeepLab-CRF and DeepLabMSc-CRF models achieve performance of 66:4%and67:1%mean IOU1, respectively. Our models',\n",
       " 'outperform all the other state-of-the-art models (speciﬁcally, TTI-Zoomout-16 (Mostajabi et al.,\\n2014), FCN-8s (Long et al., 2014), and MSRA-CFM (Dai et al., 2014)). When we increase the FOV\\nof the models, DeepLab-CRF-LargeFOV yields performance of 70:3%, the same as DeepLab-CRF7x7, while its training speed is faster. Furthermore, our best model, DeepLab-MSc-CRF-LargeFOV ,\\nattains the best performance of 71:6%by employing both multi-scale features and large FOV .\\n1http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?\\nchallengeid=11&compid=6\\n9\\nPublished as a conference paper at ICLR 2015\\nFigure 7: Visualization results on VOC 2012-val. For each row, we show the input image, the\\nsegmentation result delivered by the DCNN (DeepLab), and the reﬁned segmentation result of the\\nFully Connected CRF (DeepLab-CRF). We show our failure modes in the last three rows. Best\\nviewed in color.\\n10',\n",
       " 'Fully Connected CRF (DeepLab-CRF). We show our failure modes in the last three rows. Best\\nviewed in color.\\n10\\nPublished as a conference paper at ICLR 2015\\nMethod bkg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean\\nMSRA-CFM -75.7 26.7 69.5 48.8 65.6 81.0 69.2 73.3 30.0 68.7 51.5 69.1 68.1 71.7 67.5 50.4 66.5 44.4 58.9 53.5 61.8\\nFCN-8s -76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2\\nTTI-Zoomout-16 89.8 81.9 35.1 78.2 57.4 56.5 80.5 74.0 79.8 22.4 69.6 53.7 74.0 76.0 76.6 68.8 44.3 70.2 40.2 68.9 55.3 64.4',\n",
       " 'DeepLab-CRF 92.1 78.4 33.1 78.2 55.6 65.3 81.3 75.5 78.6 25.3 69.2 52.7 75.2 69.0 79.1 77.6 54.7 78.3 45.1 73.3 56.2 66.4\\nDeepLab-MSc-CRF 92.6 80.4 36.8 77.4 55.2 66.4 81.5 77.5 78.9 27.1 68.2 52.7 74.3 69.6 79.4 79.0 56.9 78.8 45.2 72.7 59.3 67.1\\nDeepLab-CRF-7x7 92.8 83.9 36.6 77.5 58.4 68.0 84.6 79.7 83.1 29.5 74.6 59.3 78.9 76.0 82.1 80.6 60.3 81.7 49.2 78.0 60.7 70.3\\nDeepLab-CRF-LargeFOV 92.6 83.5 36.6 82.5 62.3 66.5 85.4 78.5 83.7 30.4 72.9 60.4 78.5 75.5 82.1 79.7 58.2 82.0 48.8 73.7 63.3 70.3',\n",
       " 'DeepLab-MSc-CRF-LargeFOV 93.1 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6\\nTable 3: Labeling IOU (%) on the PASCAL VOC 2012 test set, using the trainval set for training.\\n6 D ISCUSSION\\nOur work combines ideas from deep convolutional neural networks and fully-connected conditional\\nrandom ﬁelds, yielding a novel method able to produce semantically accurate predictions and detailed segmentation maps, while being computationally efﬁcient. Our experimental results show that\\nthe proposed method signiﬁcantly advances the state-of-art in the challenging PASCAL VOC 2012\\nsemantic image segmentation task.\\nThere are multiple aspects in our model that we intend to reﬁne, such as fully integrating its two\\nmain components (CNN and CRF) and train the whole system in an end-to-end fashion, similar to\\nKr¨ahenb ¨uhl & Koltun (2013); Chen et al. (2014); Zheng et al. (2015). We also plan to experiment',\n",
       " 'Kr¨ahenb ¨uhl & Koltun (2013); Chen et al. (2014); Zheng et al. (2015). We also plan to experiment\\nwith more datasets and apply our method to other sources of data such as depth maps or videos. Recently, we have pursued model training with weakly supervised annotations, in the form of bounding\\nboxes or image-level labels (Papandreou et al., 2015).\\nAt a higher level, our work lies in the intersection of convolutional neural networks and probabilistic\\ngraphical models. We plan to further investigate the interplay of these two powerful classes of\\nmethods and explore their synergistic potential for solving challenging computer vision tasks.\\nACKNOWLEDGMENTS\\nThis work was partly supported by ARO 62250-CS, NIH Grant 5R01EY022247-03, EU Project\\nRECONFIG FP7-ICT-600825 and EU Project MOBOT FP7-ICT-2011-600796. We also gratefully\\nacknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.\\nWe would like to thank the anonymous reviewers for their detailed comments and constructive feedback.\\nPAPER REVISIONS\\nHere we present the list of major paper revisions for the convenience of the readers.\\nv1 Submission to ICLR 2015. Introduces the model DeepLab-CRF, which attains the performance',\n",
       " 'v1 Submission to ICLR 2015. Introduces the model DeepLab-CRF, which attains the performance\\nof66:4%on PASCAL VOC 2012 test set.\\nv2 Rebuttal for ICLR 2015. Adds the model DeepLab-MSc-CRF, which incorporates multi-scale\\nfeatures from the intermediate layers. DeepLab-MSc-CRF yields the performance of 67:1%on\\nPASCAL VOC 2012 test set.\\nv3 Camera-ready for ICLR 2015. Experiments with large Field-Of-View. On PASCAL VOC 2012\\ntest set, DeepLab-CRF-LargeFOV achieves the performance of 70:3%. When exploiting both mutliscale features and large FOV , DeepLab-MSc-CRF-LargeFOV attains the performance of 71:6%.\\nv4 Reference to our updated “DeepLab” system (Chen et al., 2016) with much improved results.\\nREFERENCES\\nAdams, A., Baek, J., and Davis, M. A. Fast high-dimensional ﬁltering using the permutohedral\\nlattice. In Computer Graphics Forum , 2010.',\n",
       " 'lattice. In Computer Graphics Forum , 2010.\\nArbel ´aez, P., Pont-Tuset, J., Barron, J. T., Marques, F., and Malik, J. Multiscale combinatorial\\ngrouping. In CVPR , 2014.\\n11\\nPublished as a conference paper at ICLR 2015\\nBell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials\\nin context database. arXiv:1412.0623 , 2014.\\nCarreira, J. and Sminchisescu, C. Cpmc: Automatic object segmentation using constrained parametric min-cuts. PAMI , 2012.\\nCarreira, J., Caseiro, R., Batista, J., and Sminchisescu, C. Semantic segmentation with second-order\\npooling. In ECCV , 2012.\\nChen, L.-C., Papandreou, G., and Yuille, A. Learning a dictionary of shape epitomes with applications to image labeling. In ICCV , 2013.\\nChen, L.-C., Schwing, A., Yuille, A., and Urtasun, R. Learning deep structured models.\\narXiv:1407.2538 , 2014.',\n",
       " 'arXiv:1407.2538 , 2014.\\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. Deeplab: Semantic\\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\\narXiv:1606.00915 , 2016.\\nChen, X. and Yuille, A. L. Articulated pose estimation by a graphical model with image dependent\\npairwise relations. In NIPS , 2014.\\nCogswell, M., Lin, X., Purushwalkam, S., and Batra, D. Combining the best of graphical models\\nand convnets for semantic segmentation. arXiv:1412.4313 , 2014.\\nDai, J., He, K., and Sun, J. Convolutional feature masking for joint object and stuff segmentation.\\narXiv:1412.1283 , 2014.\\nDelong, A., Osokin, A., Isack, H. N., and Boykov, Y . Fast approximate energy minimization with\\nlabel costs. IJCV , 2012.\\nEigen, D. and Fergus, R. Predicting depth, surface normals and semantic labels with a common',\n",
       " 'label costs. IJCV , 2012.\\nEigen, D. and Fergus, R. Predicting depth, surface normals and semantic labels with a common\\nmulti-scale convolutional architecture. arXiv:1411.4734 , 2014.\\nEveringham, M., Eslami, S. M. A., Gool, L. V ., Williams, C. K. I., Winn, J., and Zisserma, A. The\\npascal visual object classes challenge a retrospective. IJCV , 2014.\\nFarabet, C., Couprie, C., Najman, L., and LeCun, Y . Learning hierarchical features for scene labeling.PAMI , 2013.\\nGeiger, D. and Girosi, F. Parallel and deterministic algorithms from mrfs: Surface reconstruction.\\nPAMI , 13(5):401–412, 1991.\\nGeiger, D. and Yuille, A. A common framework for image segmentation. IJCV , 6(3):227–243,\\n1991.\\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object\\ndetection and semantic segmentation. In CVPR , 2014.',\n",
       " 'detection and semantic segmentation. In CVPR , 2014.\\nGiusti, A., Ciresan, D., Masci, J., Gambardella, L., and Schmidhuber, J. Fast image scanning with\\ndeep max-pooling convolutional neural networks. In ICIP , 2013.\\nGonfaus, J. M., Boix, X., Van de Weijer, J., Bagdanov, A. D., Serrat, J., and Gonzalez, J. Harmony\\npotentials for joint classiﬁcation and segmentation. In CVPR , 2010.\\nHariharan, B., Arbel ´aez, P., Bourdev, L., Maji, S., and Malik, J. Semantic contours from inverse\\ndetectors. In ICCV , 2011.\\nHariharan, B., Arbel ´aez, P., Girshick, R., and Malik, J. Hypercolumns for object segmentation and\\nﬁne-grained localization. arXiv:1411.5752 , 2014a.\\nHariharan, B., Arbel ´aez, P., Girshick, R., and Malik, J. Simultaneous detection and segmentation.\\nInECCV , 2014b.\\n12\\nPublished as a conference paper at ICLR 2015',\n",
       " 'InECCV , 2014b.\\n12\\nPublished as a conference paper at ICLR 2015\\nHe, X., Zemel, R. S., and Carreira-Perpindn, M. Multiscale conditional random ﬁelds for image\\nlabeling. In CVPR , 2004.\\nJia, Y ., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell,\\nT. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093 , 2014.\\nKohli, P., Ladicky, L., and Torr, P. H. Robust higher order potentials for enforcing label consistency.\\nIJCV , 2009.\\nKokkinos, I., Deriche, R., Faugeras, O., and Maragos, P. Computational analysis and learning for a\\nbiologically motivated model of boundary detection. Neurocomputing , 71(10):1798–1812, 2008.\\nKr¨ahenb ¨uhl, P. and Koltun, V . Efﬁcient inference in fully connected crfs with gaussian edge potentials. In NIPS , 2011.',\n",
       " 'Kr¨ahenb ¨uhl, P. and Koltun, V . Parameter learning and convergent inference for dense random ﬁelds.\\nInICML , 2013.\\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional\\nneural networks. In NIPS , 2013.\\nLadicky, L., Russell, C., Kohli, P., and Torr, P. H. Associative hierarchical crfs for object class image\\nsegmentation. In ICCV , 2009.\\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-based learning applied to document\\nrecognition. In Proc. IEEE , 1998.\\nLempitsky, V ., Vedaldi, A., and Zisserman, A. Pylon model for semantic segmentation. In NIPS ,\\n2011.\\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation.\\narXiv:1411.4038 , 2014.\\nLucchi, A., Li, Y ., Boix, X., Smith, K., and Fua, P. Are spatial and global constraints really necessary',\n",
       " 'Lucchi, A., Li, Y ., Boix, X., Smith, K., and Fua, P. Are spatial and global constraints really necessary\\nfor segmentation? In ICCV , 2011.\\nMallat, S. A Wavelet Tour of Signal Processing . Acad. Press, 2 edition, 1999.\\nMostajabi, M., Yadollahpour, P., and Shakhnarovich, G. Feedforward semantic segmentation with\\nzoom-out features. arXiv:1412.0774 , 2014.\\nPapandreou, G., Kokkinos, I., and Savalle, P.-A. Untangling local and global deformations in deep\\nconvolutional networks for image classiﬁcation and sliding window detection. arXiv:1412.0296 ,\\n2014.\\nPapandreou, G., Chen, L.-C., Murphy, K., and Yuille, A. L. Weakly- and semi-supervised learning\\nof a DCNN for semantic image segmentation. arXiv:1502.02734 , 2015.\\nRother, C., Kolmogorov, V ., and Blake, A. Grabcut: Interactive foreground extraction using iterated\\ngraph cuts. In SIGGRAPH , 2004.',\n",
       " 'Rother, C., Kolmogorov, V ., and Blake, A. Grabcut: Interactive foreground extraction using iterated\\ngraph cuts. In SIGGRAPH , 2004.\\nSermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y . Overfeat: Integrated\\nrecognition, localization and detection using convolutional networks. arXiv:1312.6229 , 2013.\\nShotton, J., Winn, J., Rother, C., and Criminisi, A. Textonboost for image understanding: Multiclass object recognition and segmentation by jointly modeling texture, layout, and context. IJCV ,\\n2009.\\nSimonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556 , 2014.\\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., and\\nRabinovich, A. Going deeper with convolutions. arXiv:1409.4842 , 2014.\\n13\\nPublished as a conference paper at ICLR 2015',\n",
       " 'Rabinovich, A. Going deeper with convolutions. arXiv:1409.4842 , 2014.\\n13\\nPublished as a conference paper at ICLR 2015\\nTompson, J., Jain, A., LeCun, Y ., and Bregler, C. Joint Training of a Convolutional Network and a\\nGraphical Model for Human Pose Estimation. In NIPS , 2014.\\nUijlings, J., van de Sande, K., Gevers, T., and Smeulders, A. Selective search for object recognition.\\nIJCV , 2013.\\nWang, P., Shen, X., Lin, Z., Cohen, S., Price, B., and Yuille, A. Towards uniﬁed depth and semantic\\nprediction from a single image. In CVPR , 2015.\\nYadollahpour, P., Batra, D., and Shakhnarovich, G. Discriminative re-ranking of diverse segmentations. In CVPR , 2013.\\nZeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. In ECCV , 2014.\\nZhang, N., Donahue, J., Girshick, R., and Darrell, T. Part-based r-cnns for ﬁne-grained category\\ndetection. In ECCV , 2014.',\n",
       " 'detection. In ECCV , 2014.\\nZheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V ., Su, Z., Du, D., Huang, C., and Torr, P.\\nConditional random ﬁelds as recurrent neural networks. arXiv:1502.03240 , 2015.\\n14',\n",
       " 'arXiv:1502.03167v3  [cs.LG]  2 Mar 2015BatchNormalization: AcceleratingDeepNetworkTrainingb y\\nReducingInternalCovariateShift\\nSergey Ioffe\\nGoogleInc., sioffe@google.comChristianSzegedy\\nGoogleInc., szegedy@google.com\\nAbstract\\nTrainingDeepNeuralNetworksiscomplicatedbythefact\\nthat the distributionofeach layer’sinputschangesduring\\ntraining, as the parametersof the previouslayers change.\\nThis slows down the training by requiringlower learning\\nratesandcarefulparameterinitialization,andmakesitno toriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate\\nshift, and address the problem by normalizing layer inputs. Ourmethoddrawsitsstrengthfrommakingnormalizationapartofthemodelarchitectureandperformingthe\\nnormalization for each training mini-batch . Batch Normalizationallowsustousemuchhigherlearningratesand\\nbe less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout.\\nApplied to a state-of-the-art image classiﬁcation model,\\nBatch Normalizationachievesthe same accuracy with 14\\ntimes fewer training steps, and beats the original model',\n",
       " 'Batch Normalizationachievesthe same accuracy with 14\\ntimes fewer training steps, and beats the original model\\nby a signiﬁcant margin. Using an ensemble of batchnormalizednetworks,weimproveuponthebestpublished\\nresult on ImageNet classiﬁcation: reaching 4.9% top-5\\nvalidation error (and 4.8% test error), exceeding the accuracyofhumanraters.\\n1 Introduction\\nDeep learning has dramatically advanced the state of the\\nart in vision, speech, and many other areas. Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants\\nsuch as momentum (Sutskeveret al., 2013) and Adagrad\\n(Duchiet al.,2011)havebeenusedtoachievestate ofthe\\nart performance. SGD optimizes the parameters Θof the\\nnetwork,soasto minimizetheloss\\nΘ = argmin\\nΘ1\\nNN/summationdisplay\\ni=1ℓ(xi,Θ)',\n",
       " 'network,soasto minimizetheloss\\nΘ = argmin\\nΘ1\\nNN/summationdisplay\\ni=1ℓ(xi,Θ)\\nwherex1...Nisthetrainingdataset. With SGD,thetrainingproceedsinsteps,andateachstepweconsidera minibatchx1...mofsizem. The mini-batchis usedtoapproximate the gradient of the loss functionwith respect to the\\nparameters,bycomputing\\n1\\nm∂ℓ(xi,Θ)\\n∂Θ.Usingmini-batchesofexamples,asopposedtooneexampleatatime,ishelpfulinseveralways. First,thegradient\\nofthelossoveramini-batchisanestimateofthegradient\\noverthetrainingset, whose qualityimprovesas thebatch\\nsize increases. Second, computation over a batch can be\\nmuch more efﬁcient than mcomputations for individual\\nexamples, due to the parallelism afforded by the modern\\ncomputingplatforms.\\nWhile stochastic gradient is simple and effective, it\\nrequires careful tuning of the model hyper-parameters,\\nspeciﬁcallythelearningrateusedinoptimization,aswell',\n",
       " 'requires careful tuning of the model hyper-parameters,\\nspeciﬁcallythelearningrateusedinoptimization,aswell\\nas the initial values for the model parameters. The trainingiscomplicatedbythefactthattheinputstoeachlayer\\nareaffectedbytheparametersofallprecedinglayers–so\\nthat small changes to the network parameters amplify as\\nthenetworkbecomesdeeper.\\nThe change in the distributions of layers’ inputs\\npresents a problem because the layers need to continuously adapt to the new distribution. When the input distributiontoalearningsystemchanges,itissaidtoexperiencecovariateshift (Shimodaira, 2000). This is typically\\nhandled via domain adaptation (Jiang, 2008). However,\\nthe notion of covariate shift can be extended beyond the\\nlearningsystemasawhole,toapplytoitsparts,suchasa\\nsub-networkora layer. Considera networkcomputing\\nℓ=F2(F1(u,Θ1),Θ2)\\nwhereF1andF2are arbitrary transformations, and the\\nparameters Θ1,Θ2are to be learned so as to minimize\\nthe lossℓ. Learning Θ2can be viewed as if the inputs\\nx =F1(u,Θ1)arefedintothesub-network',\n",
       " 'the lossℓ. Learning Θ2can be viewed as if the inputs\\nx =F1(u,Θ1)arefedintothesub-network\\nℓ=F2(x,Θ2).\\nForexample,agradientdescentstep\\nΘ2←Θ2−α\\nmm/summationdisplay\\ni=1∂F2(xi,Θ2)\\n∂Θ2\\n(forbatchsize mandlearningrate α)isexactlyequivalent\\nto that for a stand-alone network F2with input x. Therefore, the input distribution properties that make training\\nmore efﬁcient – such as having the same distribution between the training and test data – apply to training the\\nsub-network as well. As such it is advantageous for the\\ndistributionof xtoremainﬁxedovertime. Then, Θ2does\\n1\\nnot have to readjust to compensate for the change in the\\ndistributionof x.\\nFixed distribution of inputs to a sub-network would\\nhavepositiveconsequencesforthelayers outsidethesubnetwork,as well. Consider a layer with a sigmoid activation function z =g(Wu+b)whereuis the layer input,\\nthe weight matrix Wand bias vector bare the layer parameters to be learned, and g(x) =1',\n",
       " 'the weight matrix Wand bias vector bare the layer parameters to be learned, and g(x) =1\\n1+exp(−x). As|x|\\nincreases, g′(x)tends to zero. This means that for all dimensionsof x =Wu+bexceptthosewithsmallabsolute\\nvalues,thegradientﬂowingdownto uwillvanishandthe\\nmodel will train slowly. However, since xis affected by\\nW,band the parameters of all the layers below, changes\\ntothoseparametersduringtrainingwilllikelymovemany\\ndimensions of xinto the saturated regime of the nonlinearity and slow down the convergence. This effect is\\nampliﬁed as the network depth increases. In practice,\\nthe saturation problem and the resulting vanishing gradientsareusuallyaddressedbyusingRectiﬁedLinearUnits\\n(Nair&Hinton, 2010) ReLU(x) = max( x,0), careful\\ninitialization (Bengio&Glorot, 2010; Saxeet al., 2013),\\nand small learning rates. If, however, we could ensure\\nthat the distribution of nonlinearity inputs remains more\\nstable as the network trains, then the optimizer would be\\nless likely to get stuck in the saturated regime, and the\\ntrainingwouldaccelerate.\\nWe refer to the change in the distributions of internal',\n",
       " 'less likely to get stuck in the saturated regime, and the\\ntrainingwouldaccelerate.\\nWe refer to the change in the distributions of internal\\nnodes of a deep network, in the course of training, as Internal Covariate Shift . Eliminating it offers a promise of\\nfaster training. We propose a new mechanism, which we\\ncallBatch Normalization , that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that ﬁxes the\\nmeansandvariancesoflayerinputs. BatchNormalization\\nalso has a beneﬁcial effect on the gradient ﬂow through\\nthe network, by reducing the dependence of gradients\\non the scale of the parameters or of their initial values.\\nThis allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for\\nDropout(Srivastavaet al., 2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities\\nby preventingthe network from getting stuck in the saturatedmodes.\\nIn Sec. 4.2, we apply Batch Normalization to the bestperforming ImageNet classiﬁcation network, and show\\nthat we can match its performance using only 7% of the',\n",
       " 'that we can match its performance using only 7% of the\\ntraining steps, and can further exceed its accuracy by a\\nsubstantial margin. Using an ensemble of such networks\\ntrained with Batch Normalization, we achieve the top-5\\nerror rate that improves upon the best known results on\\nImageNetclassiﬁcation.2 Towards Reducing Internal\\nCovariateShift\\nWe deﬁne Internal Covariate Shift as the change in the\\ndistribution of network activations due to the change in\\nnetworkparametersduringtraining. Toimprovethetraining, we seek to reduce the internal covariate shift. By\\nﬁxingthe distributionof the layer inputs xas the training\\nprogresses,weexpecttoimprovethetrainingspeed. Ithas\\nbeen long known (LeCunetal., 1998b; Wiesler &Ney,\\n2011) that the network training convergesfaster if its inputsarewhitened–i.e.,linearlytransformedtohavezero\\nmeansandunitvariances,anddecorrelated. Aseachlayer\\nobservestheinputsproducedbythelayersbelow,itwould\\nbe advantageousto achieve the same whiteningof the inputsof each layer. By whitening the inputsto each layer,',\n",
       " 'be advantageousto achieve the same whiteningof the inputsof each layer. By whitening the inputsto each layer,\\nwe would take a step towards achieving the ﬁxed distributions of inputs that would remove the ill effects of the\\ninternalcovariateshift.\\nWe couldconsiderwhiteningactivationsat everytraining step or at some interval, either by modifying the\\nnetwork directly or by changing the parameters of the\\noptimization algorithm to depend on the network activation values (Wiesleret al., 2014; Raikoetal., 2012;\\nPoveyet al., 2014; Desjardins&Kavukcuoglu). However, if these modiﬁcations are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires\\nthe normalization to be updated, which reduces the effect of the gradient step. For example, consider a layer\\nwith the input uthat addsthe learned bias b, and normalizes the result by subtracting the mean of the activation\\ncomputed over the training data: /hatwidex=x−E[x]where\\nx=u+b,X={x1...N}is the set of values of xover\\nthe training set, and E [x] =1\\nN/summationtextN\\ni=1xi. If a gradient',\n",
       " 'the training set, and E [x] =1\\nN/summationtextN\\ni=1xi. If a gradient\\ndescent step ignores the dependence of E [x]onb, then it\\nwill update b←b+ ∆b, where∆b∝−∂ℓ/∂/hatwidex. Then\\nu+ (b+ ∆b)−E[u+ (b+ ∆b)] =u+b−E[u+b].\\nThus, the combination of the update to band subsequent\\nchange in normalization led to no change in the output\\nof the layer nor, consequently, the loss. As the training\\ncontinues, bwill grow indeﬁnitely while the loss remains\\nﬁxed. Thisproblemcangetworseifthenormalizationnot\\nonly centers but also scales the activations. We have observed this empirically in initial experiments, where the\\nmodel blows up when the normalization parameters are\\ncomputedoutsidethe gradientdescentstep.\\nThe issue with the above approach is that the gradient\\ndescent optimization does not take into account the fact\\nthat the normalization takes place. To address this issue,\\nwe would like to ensure that, for any parameter values,\\nthe network alwaysproducesactivationswith the desired\\ndistribution. Doing so would allow the gradient of the',\n",
       " 'we would like to ensure that, for any parameter values,\\nthe network alwaysproducesactivationswith the desired\\ndistribution. Doing so would allow the gradient of the\\nloss with respect to the model parameters to account for\\nthe normalization, and for its dependence on the model\\nparameters Θ. Let again xbe a layer input, treated as a\\n2\\nvector, andXbe the set of these inputs over the training\\ndataset. Thenormalizationcanthenbewrittenasatransformation\\n/hatwidex =Norm(x,X)\\nwhich depends not only on the given training example x\\nbut on all examples X– each of which depends on Θif\\nxis generatedby anotherlayer. For backpropagation,we\\nwouldneedtocomputetheJacobians\\n∂Norm(x,X)\\n∂xand∂Norm(x,X)\\n∂X;\\nignoring the latter term would lead to the explosion describedabove. Withinthisframework,whiteningthelayer\\ninputs is expensive, as it requires computing the covariance matrix Cov [x] =Ex∈X[xxT]−E[x]E[x]Tand its\\ninverse square root, to produce the whitened activations\\nCov[x]−1/2(x−E[x]), as well as the derivatives of these',\n",
       " 'inverse square root, to produce the whitened activations\\nCov[x]−1/2(x−E[x]), as well as the derivatives of these\\ntransformsforbackpropagation.Thismotivatesustoseek\\nan alternative that performs input normalization in a way\\nthat is differentiable and does not require the analysis of\\ntheentiretrainingset aftereveryparameterupdate.\\nSome of the previous approaches (e.g.\\n(Lyu&Simoncelli, 2008)) use statistics computed\\nover a single training example, or, in the case of image\\nnetworks, over differentfeature maps at a given location.\\nHowever, this changes the representation ability of a\\nnetwork by discarding the absolute scale of activations.\\nWe want to a preservethe informationin the network,by\\nnormalizing the activations in a training example relative\\ntothe statisticsoftheentiretrainingdata.\\n3 Normalization via Mini-Batch\\nStatistics\\nSince the full whitening of each layer’s inputs is costly\\nand not everywhere differentiable, we make two necessary simpliﬁcations. The ﬁrst is that instead of whitening\\nthe features in layer inputs and outputs jointly, we will\\nnormalizeeachscalarfeatureindependently,bymakingit\\nhave the mean of zero and the variance of 1. For a layer',\n",
       " 'normalizeeachscalarfeatureindependently,bymakingit\\nhave the mean of zero and the variance of 1. For a layer\\nwithd-dimensionalinput x = (x(1)...x(d)),wewillnormalizeeachdimension\\n/hatwidex(k)=x(k)−E[x(k)]/radicalbig\\nVar[x(k)]\\nwheretheexpectationandvariancearecomputedoverthe\\ntrainingdataset. Asshownin(LeCunetal.,1998b),such\\nnormalizationspeedsupconvergence,evenwhenthefeaturesarenotdecorrelated.\\nNotethatsimplynormalizingeachinputofalayermay\\nchange what the layer can represent. For instance, normalizing the inputsof a sigmoid wouldconstrain them to\\nthe linear regime of the nonlinearity. To address this, we\\nmakesurethat thetransformationinsertedin thenetwork\\ncan represent the identity transform . To accomplish this,weintroduce,foreachactivation x(k),apairofparameters\\nγ(k),β(k),whichscale andshift thenormalizedvalue:\\ny(k)=γ(k)/hatwidex(k)+β(k).\\nThese parameters are learned along with the original\\nmodel parameters, and restore the representation power',\n",
       " 'y(k)=γ(k)/hatwidex(k)+β(k).\\nThese parameters are learned along with the original\\nmodel parameters, and restore the representation power\\nofthenetwork. Indeed,bysetting γ(k)=/radicalbig\\nVar[x(k)]and\\nβ(k)=E[x(k)], we couldrecoverthe originalactivations,\\nifthatwerethe optimalthingto do.\\nInthebatchsettingwhereeachtrainingstepisbasedon\\ntheentire trainingset, we woulduse the wholeset to normalize activations. However,this is impracticalwhen using stochastic optimization. Therefore, we make the secondsimpliﬁcation: since we use mini-batchesin stochastic gradient training, each mini-batch produces estimates\\nofthemeanandvariance ofeachactivation. Thisway,the\\nstatistics used for normalization can fully participate in\\nthe gradient backpropagation. Note that the use of minibatchesis enabledbycomputationof per-dimensionvariances rather than joint covariances; in the joint case, regularizationwouldbe requiredsince the mini-batchsize is\\nlikely to be smaller than the number of activations being\\nwhitened,resultinginsingularcovariancematrices.',\n",
       " 'likely to be smaller than the number of activations being\\nwhitened,resultinginsingularcovariancematrices.\\nConsider a mini-batch Bof sizem. Since the normalization is applied to each activation independently, let us\\nfocusonaparticularactivation x(k)andomitkforclarity.\\nWe havemvaluesofthisactivationinthemini-batch,\\nB={x1...m}.\\nLetthenormalizedvaluesbe /hatwidex1...m,andtheirlineartransformationsbe y1...m. We referto thetransform\\nBNγ,β:x1...m→y1...m\\nas theBatch Normalizing Transform . We present the BN\\nTransforminAlgorithm1. Inthealgorithm, ǫisaconstant\\naddedtothemini-batchvariancefornumericalstability.\\nInput:Valuesof xovera mini-batch: B={x1...m};\\nParametersto belearned: γ,β\\nOutput:{yi=BNγ,β(xi)}\\nµB←1\\nmm/summationdisplay\\ni=1xi // mini-batchmean\\nσ2\\nB←1\\nmm/summationdisplay\\ni=1(xi−µB)2// mini-batchvariance',\n",
       " 'σ2\\nB←1\\nmm/summationdisplay\\ni=1(xi−µB)2// mini-batchvariance\\n/hatwidexi←xi−µB/radicalbig\\nσ2\\nB+ǫ// normalize\\nyi←γ/hatwidexi+β≡BNγ,β(xi) // scale andshift\\nAlgorithm 1: Batch Normalizing Transform, applied to\\nactivation xoveramini-batch.\\nTheBNtransformcanbeaddedtoanetworktomanipulate any activation. In the notation y=BNγ,β(x), we\\n3\\nindicate that the parameters γandβare to be learned,\\nbut it should be noted that the BN transform does not\\nindependently process the activation in each training example. Rather, BN γ,β(x)depends both on the training\\nexampleand the other examples in the mini-batch . The\\nscaled and shifted values yare passed to other network\\nlayers. The normalized activations /hatwidexare internal to our\\ntransformation, but their presence is crucial. The distributions of values of any /hatwidexhas the expected value of 0\\nand the variance of 1, as long as the elements of each\\nmini-batch are sampled from the same distribution, and',\n",
       " 'and the variance of 1, as long as the elements of each\\nmini-batch are sampled from the same distribution, and\\nif we neglect ǫ. This can be seen by observing that/summationtextm\\ni=1/hatwidexi= 0and1\\nm/summationtextm\\ni=1/hatwidex2\\ni= 1, and taking expectations. Eachnormalizedactivation /hatwidex(k)canbeviewedas\\nan input to a sub-network composed of the linear transformy(k)=γ(k)/hatwidex(k)+β(k), followed by the other processing doneby the originalnetwork. Thesesub-network\\ninputs all have ﬁxed means and variances, and although\\nthe jointdistributionofthese normalized /hatwidex(k)canchange\\nover the course of training, we expect that the introduction of normalized inputs accelerates the training of the\\nsub-networkand,consequently,thenetworkasawhole.\\nDuring training we need to backpropagate the gradient of loss ℓthrough this transformation,as well as compute the gradients with respect to the parameters of the\\nBN transform. We use chainrule,as follows(beforesimpliﬁcation):\\n∂ℓ\\n∂/hatwidexi=∂ℓ\\n∂yi·γ',\n",
       " '∂ℓ\\n∂/hatwidexi=∂ℓ\\n∂yi·γ\\n∂ℓ\\n∂σ2\\nB=/summationtextm\\ni=1∂ℓ\\n∂/hatwidexi·(xi−µB)·−1\\n2(σ2\\nB+ǫ)−3/2\\n∂ℓ\\n∂µB=/parenleftbigg/summationtextm\\ni=1∂ℓ\\n∂/hatwidexi·−1√\\nσ2\\nB+ǫ/parenrightbigg\\n+∂ℓ\\n∂σ2\\nB·/summationtextm\\ni=1−2(xi−µB)\\nm\\n∂ℓ\\n∂xi=∂ℓ\\n∂/hatwidexi·1√\\nσ2\\nB+ǫ+∂ℓ\\n∂σ2\\nB·2(xi−µB)\\nm+∂ℓ\\n∂µB·1\\nm\\n∂ℓ\\n∂γ=/summationtextm\\ni=1∂ℓ\\n∂yi·/hatwidexi\\n∂ℓ',\n",
       " '∂ℓ\\n∂γ=/summationtextm\\ni=1∂ℓ\\n∂yi·/hatwidexi\\n∂ℓ\\n∂β=/summationtextm\\ni=1∂ℓ\\n∂yi\\nThus,BNtransformisadifferentiabletransformationthat\\nintroduces normalized activations into the network. This\\nensures that as the model is training, layers can continue\\nlearningoninputdistributionsthatexhibitlessinternal covariate shift, thus accelerating the training. Furthermor e,\\nthe learned afﬁne transform applied to these normalized\\nactivationsallowstheBNtransformtorepresenttheidentity transformationandpreservesthenetworkcapacity.\\n3.1 Training and Inference with BatchNormalizedNetworks\\nToBatch-Normalize anetwork,wespecifyasubsetofactivations and insert the BN transform for each of them,\\naccording to Alg. 1. Any layer that previously received\\nxas the input, now receives BN (x). A model employing\\nBatch Normalization can be trained using batch gradient\\ndescent,orStochasticGradientDescentwithamini-batch\\nsizem >1, or with any of its variants such as Adagrad(Duchiet al.,2011). Thenormalizationofactivationsthat',\n",
       " 'sizem >1, or with any of its variants such as Adagrad(Duchiet al.,2011). Thenormalizationofactivationsthat\\ndependsonthemini-batchallowsefﬁcienttraining,but is\\nneithernecessarynordesirableduringinference;wewant\\nthe output to depend only on the input, deterministically.\\nFor this, once the network has been trained, we use the\\nnormalization\\n/hatwidex=x−E[x]/radicalbig\\nVar[x]+ǫ\\nusing the population, rather than mini-batch, statistics.\\nNeglecting ǫ, these normalized activations have the same\\nmean0 and variance1 as duringtraining. We use the unbiased variance estimate Var [x] =m\\nm−1·EB[σ2\\nB], where\\ntheexpectationisovertrainingmini-batchesofsize mand\\nσ2\\nBaretheirsamplevariances. Usingmovingaveragesinstead, we can track the accuracy of a model as it trains.\\nSincethemeansandvariancesareﬁxedduringinference,\\nthe normalization is simply a linear transform applied to\\neachactivation. Itmayfurtherbecomposedwiththescaling byγand shift by β, to yield a single linear transform',\n",
       " 'the normalization is simply a linear transform applied to\\neachactivation. Itmayfurtherbecomposedwiththescaling byγand shift by β, to yield a single linear transform\\nthat replacesBN (x). Algorithm 2 summarizesthe procedurefortrainingbatch-normalizednetworks.\\nInput:NetworkNwith trainableparameters Θ;\\nsubsetofactivations {x(k)}K\\nk=1\\nOutput: Batch-normalizednetworkforinference, Ninf\\nBN\\n1:Ntr\\nBN←N// TrainingBN network\\n2:fork= 1...Kdo\\n3:Add transformation y(k)=BNγ(k),β(k)(x(k))to\\nNtr\\nBN(Alg.1)\\n4:Modify each layer in Ntr\\nBNwith input x(k)to take\\ny(k)instead\\n5:end for\\n6:TrainNtr\\nBNto optimize the parameters Θ∪\\n{γ(k),β(k)}K\\nk=1\\n7:Ninf\\nBN←Ntr\\nBN// InferenceBN networkwithfrozen\\n// parameters\\n8:fork= 1...Kdo\\n9:// Forclarity, x≡x(k),γ≡γ(k),µB≡µ(k)\\nB, etc.',\n",
       " '9:// Forclarity, x≡x(k),γ≡γ(k),µB≡µ(k)\\nB, etc.\\n10:Process multiple training mini-batches B, each of\\nsizem,andaverageoverthem:\\nE[x]←EB[µB]\\nVar[x]←m\\nm−1EB[σ2\\nB]\\n11:InNinf\\nBN, replace the transform y=BNγ,β(x)with\\ny=γ√\\nVar[x]+ǫ·x+/parenleftbig\\nβ−γE[x]√\\nVar[x]+ǫ/parenrightbig\\n12:end for\\nAlgorithm2: Traininga Batch-NormalizedNetwork\\n3.2 Batch-Normalized Convolutional Networks\\nBatch Normalization can be applied to any set of activations in the network. Here, we focus on transforms\\n4\\nthat consist of an afﬁne transformation followed by an\\nelement-wisenonlinearity:\\nz =g(Wu+b)\\nwhereWandbare learned parametersof the model, and\\ng(·)isthenonlinearitysuchassigmoidorReLU.Thisformulation covers both fully-connected and convolutional\\nlayers. We add the BN transform immediately before the',\n",
       " 'layers. We add the BN transform immediately before the\\nnonlinearity,bynormalizing x =Wu+b. Wecouldhave\\nalso normalized the layer inputs u, but since uis likely\\nthe output of another nonlinearity, the shape of its distributionislikelytochangeduringtraining,andconstrainin g\\nits ﬁrst and second moments would not eliminate the covariate shift. In contrast, Wu + bis more likely to have\\na symmetric,non-sparsedistribution,that is “moreGaussian”(Hyv¨ arinen&Oja,2000);normalizingitislikelyto\\nproduceactivationswithastable distribution.\\nNotethat,sincewenormalize Wu+b,thebiasbcanbe\\nignoredsinceitseffectwillbecanceledbythesubsequent\\nmeansubtraction(theroleofthebiasissubsumedby βin\\nAlg.1). Thus, z =g(Wu+b)is replacedwith\\nz =g(BN(Wu))\\nwhere the BN transformis applied independentlyto each\\ndimension of x =Wu, with a separate pair of learned\\nparameters γ(k),β(k)perdimension.\\nForconvolutionallayers,we additionallywant the normalization to obey the convolutional property – so that',\n",
       " 'parameters γ(k),β(k)perdimension.\\nForconvolutionallayers,we additionallywant the normalization to obey the convolutional property – so that\\ndifferent elements of the same feature map, at different\\nlocations, are normalized in the same way. To achieve\\nthis, we jointly normalize all the activations in a minibatch, overall locations. In Alg. 1, we let Bbe the set of\\nall values in a feature map across both the elements of a\\nmini-batch and spatial locations – so for a mini-batch of\\nsizemand feature maps of size p×q, we use the effective mini-batch of size m′=|B|=m·pq. We learn a\\npair of parameters γ(k)andβ(k)per feature map, rather\\nthan per activation. Alg. 2 is modiﬁed similarly, so that\\nduringinferencetheBNtransformappliesthesamelinear\\ntransformationtoeachactivationina givenfeaturemap.\\n3.3 Batch Normalization enables higher\\nlearning rates\\nIn traditional deep networks, too-high learning rate may\\nresult in the gradients that explode or vanish, as well as\\ngetting stuck in poor local minima. Batch Normalization helps address these issues. By normalizing activations throughout the network, it prevents small changes',\n",
       " 'getting stuck in poor local minima. Batch Normalization helps address these issues. By normalizing activations throughout the network, it prevents small changes\\nto the parameters from amplifying into larger and suboptimal changes in activations in gradients; for instance, it\\nprevents the training from getting stuck in the saturated\\nregimesofnonlinearities.\\nBatchNormalizationalsomakestrainingmoreresilient\\ntotheparameterscale. Normally,largelearningratesmay\\nincreasethescaleoflayerparameters,whichthenamplifythegradientduringbackpropagationandleadtothemodel\\nexplosion. However, with Batch Normalization, backpropagation through a layer is unaffected by the scale of\\nitsparameters. Indeed,fora scalar a,\\nBN(Wu) =BN((aW)u)\\nandwe canshowthat\\n∂BN((aW)u)\\n∂u=∂BN(Wu)\\n∂u\\n∂BN((aW)u)\\n∂(aW)=1\\na·∂BN(Wu)\\n∂W\\nThe scale does not affect the layer Jacobian nor, consequently, the gradient propagation. Moreover, larger\\nweights lead to smallergradients, and Batch Normalizationwill stabilize theparametergrowth.\\nWe further conjecture that Batch Normalization may',\n",
       " 'weights lead to smallergradients, and Batch Normalizationwill stabilize theparametergrowth.\\nWe further conjecture that Batch Normalization may\\nleadthelayerJacobianstohavesingularvaluescloseto1,\\nwhich is known to be beneﬁcial for training (Saxeet al.,\\n2013). Consider two consecutive layers with normalized\\ninputs, and the transformation between these normalized\\nvectors:/hatwidez =F(/hatwidex). Ifweassumethat /hatwidexand/hatwidezareGaussian\\nanduncorrelated,andthat F(/hatwidex)≈J/hatwidexisalineartransformationforthe givenmodelparameters,thenboth /hatwidexand/hatwidez\\nhave unit covariances, and I=Cov[/hatwidez] =JCov[/hatwidex]JT=\\nJJT. Thus,JJT=I, and so all singular values of J\\nare equal to 1, which preserves the gradient magnitudes\\nduring backpropagation. In reality, the transformation is\\nnotlinear,andthenormalizedvaluesarenotguaranteedto\\nbe Gaussian nor independent, but we nevertheless expect\\nBatch Normalization to help make gradient propagation\\nbetter behaved. The precise effect of Batch Normalization on gradient propagation remains an area of further\\nstudy.',\n",
       " 'Batch Normalization to help make gradient propagation\\nbetter behaved. The precise effect of Batch Normalization on gradient propagation remains an area of further\\nstudy.\\n3.4 Batch Normalization regularizes the\\nmodel\\nWhen training with Batch Normalization, a training example is seen in conjunction with other examples in the\\nmini-batch, and the training network no longer producing deterministic values for a given training example. In\\nour experiments,we foundthis effect to be advantageous\\nto the generalization of the network. Whereas Dropout\\n(Srivastavaet al., 2014) is typically used to reduce overﬁtting,inabatch-normalizednetworkwefoundthatitcan\\nbeeitherremovedorreducedinstrength.\\n4 Experiments\\n4.1 Activationsovertime\\nTo verify the effects of internal covariate shift on training, and the ability of Batch Normalization to combat it,\\nweconsideredtheproblemofpredictingthedigitclasson\\ntheMNISTdataset(LeCunetal.,1998a). Weusedavery\\nsimple network, with a 28x28binary image as input, and\\n5\\n10K20K30K40K50K0.70.80.91\\n  \\nWithout BN\\nWith BN\\n−202\\n−202\\n(a) (b)WithoutBN (c)With BN',\n",
       " 'Without BN\\nWith BN\\n−202\\n−202\\n(a) (b)WithoutBN (c)With BN\\nFigure 1: (a) The test accuracy of the MNIST network\\ntrained with and without Batch Normalization, vs. the\\nnumber of training steps. Batch Normalization helps the\\nnetwork train faster and achieve higher accuracy. (b,\\nc)The evolution of input distributions to a typical sigmoid,overthecourseoftraining,shownas {15,50,85}th\\npercentiles. Batch Normalization makes the distribution\\nmorestableandreducestheinternalcovariateshift.\\n3fully-connectedhiddenlayerswith100activationseach.\\nEachhiddenlayercomputes y =g(Wu+b)withsigmoid\\nnonlinearity, and the weights Winitialized to small random Gaussian values. The last hidden layer is followed\\nby a fully-connected layer with 10 activations (one per\\nclass) and cross-entropyloss. We trained the network for\\n50000steps, with 60 examplespermini-batch. We added\\nBatchNormalizationtoeachhiddenlayerofthenetwork,\\nas in Sec. 3.1. We were interested in the comparison betweenthebaselineandbatch-normalizednetworks,rather\\nthanachievingthestateoftheartperformanceonMNIST',\n",
       " 'thanachievingthestateoftheartperformanceonMNIST\\n(whichthe describedarchitecturedoesnot).\\nFigure 1(a) shows the fraction of correct predictions\\nby the two networks on held-out test data, as training\\nprogresses. The batch-normalized network enjoys the\\nhigher test accuracy. To investigate why, we studied inputs to the sigmoid, in the original network Nand batchnormalizednetwork Ntr\\nBN(Alg.2)overthecourseoftraining. InFig.1(b,c)weshow,foronetypicalactivationfrom\\nthe last hidden layer of each network, how its distribution evolves. The distributions in the original network\\nchange signiﬁcantly over time, both in their mean and\\nthe variance, which complicates the training of the subsequent layers. In contrast, the distributions in the batch normalizednetworkaremuchmorestableastrainingprogresses,whichaidsthe training.\\n4.2 ImageNetclassiﬁcation\\nWe applied Batch Normalization to a new variant of the\\nInception network (Szegedyetal., 2014), trained on the\\nImageNet classiﬁcation task (Russakovskyet al., 2014).\\nThe network has a large number of convolutional and\\npooling layers, with a softmax layer to predict the image\\nclass, out of 1000 possibilities. Convolutional layers use',\n",
       " 'The network has a large number of convolutional and\\npooling layers, with a softmax layer to predict the image\\nclass, out of 1000 possibilities. Convolutional layers use\\nReLU asthenonlinearity. Themaindifferenceto thenetwork described in (Szegedyet al., 2014) is that the 5×5\\nconvolutionallayersare replacedby two consecutivelayers of3×3convolutionswith up to 128ﬁlters. The network contains 13.6·106parameters, and, other than the\\ntop softmax layer, has no fully-connected layers. Moredetails are given in the Appendix. We refer to this model\\nasInception intherestofthetext. Themodelwastrained\\nusing a version of Stochastic Gradient Descent with momentum(Sutskeveretal.,2013),usingthemini-batchsize\\nof32. Thetrainingwasperformedusingalarge-scale,distributed architecture (similar to (Deanet al., 2012)). All\\nnetworksare evaluatedastrainingprogressesbycomputing the validation accuracy @1, i.e. the probability of\\npredicting the correct label out of 1000 possibilities, on\\naheld-outset,usinga singlecropperimage.\\nInourexperiments,weevaluatedseveralmodiﬁcations',\n",
       " 'aheld-outset,usinga singlecropperimage.\\nInourexperiments,weevaluatedseveralmodiﬁcations\\nofInceptionwithBatchNormalization. Inallcases,Batch\\nNormalizationwasappliedtotheinputofeachnonlinearity, in a convolutional way, as described in section 3.2,\\nwhilekeepingtherestofthearchitectureconstant.\\n4.2.1 AcceleratingBN Networks\\nSimplyaddingBatchNormalizationtoanetworkdoesnot\\ntake full advantage of our method. To do so, we further\\nchanged the network and its training parameters, as follows:\\nIncrease learning rate. In a batch-normalized model,\\nwe have been able to achieve a training speedup from\\nhigherlearningrates,with noill sideeffects(Sec.3.3).\\nRemoveDropout. As describedin Sec. 3.4, Batch NormalizationfulﬁllssomeofthesamegoalsasDropout. Removing Dropout from Modiﬁed BN-Inception speeds up\\ntraining,withoutincreasingoverﬁtting.\\nReduce the L2weight regularization. While in Inception anL2loss on the model parameters controls overﬁtting, in Modiﬁed BN-Inception the weight of this loss is',\n",
       " 'reduced by a factor of 5. We ﬁnd that this improves the\\naccuracyontheheld-outvalidationdata.\\nAccelerate the learning rate decay. In training Inception, learning rate was decayed exponentially. Because\\nour network trains faster than Inception, we lower the\\nlearningrate 6timesfaster.\\nRemove Local Response Normalization While Inception and other networks (Srivastavaet al., 2014) beneﬁt\\nfrom it, we found that with Batch Normalization it is not\\nnecessary.\\nShufﬂetrainingexamplesmorethoroughly. Weenabled\\nwithin-shardshufﬂingofthetrainingdata,whichprevents\\nthesameexamplesfromalwaysappearinginamini-batch\\ntogether. This led to about 1% improvements in the validation accuracy, which is consistent with the view of\\nBatch Normalization as a regularizer (Sec. 3.4): the randomization inherent in our method should be most beneﬁcialwhenitaffectsanexampledifferentlyeachtimeitis\\nseen.\\nReduce the photometric distortions. Because batchnormalized networks train faster and observe each trainingexamplefewertimes,weletthetrainerfocusonmore\\n“real”imagesbydistortingthemless.\\n6',\n",
       " '“real”imagesbydistortingthemless.\\n6\\n5M 10M 15M 20M 25M 30M0.40.50.60.70.8\\nInception\\nBN−Baseline\\nBN−x5\\nBN−x30\\nBN−x5−Sigmoid\\nSteps to match Inception\\nFigure 2: Single crop validation accuracy of Inception\\nand its batch-normalized variants, vs. the number of\\ntrainingsteps.Model Stepsto72.2% Maxaccuracy\\nInception 31.0·10672.2%\\nBN-Baseline 13.3·10672.7%\\nBN-x5 2.1·10673.0%\\nBN-x30 2.7·10674.8%\\nBN-x5-Sigmoid 69.8%\\nFigure 3: For Inception and the batch-normalized\\nvariants, the number of training steps required to\\nreach the maximum accuracy of Inception(72.2%),\\nand the maximum accuracy achieved by the network.\\n4.2.2 Single-NetworkClassiﬁcation\\nWe evaluated the following networks, all trained on the\\nLSVRC2012 training data, and tested on the validation\\ndata:\\nInception : the network described at the beginning of\\nSection4.2,trainedwiththeinitiallearningrateof0.001 5.',\n",
       " 'data:\\nInception : the network described at the beginning of\\nSection4.2,trainedwiththeinitiallearningrateof0.001 5.\\nBN-Baseline : Same as Inception with Batch Normalizationbeforeeachnonlinearity.\\nBN-x5: Inception with Batch Normalization and the\\nmodiﬁcations in Sec. 4.2.1. The initial learning rate was\\nincreased by a factor of 5, to 0.0075. The same learning\\nrateincreasewithoriginalInceptioncausedthemodelparameterstoreachmachineinﬁnity.\\nBN-x30: LikeBN-x5, but with the initial learning rate\\n0.045(30timesthatofInception).\\nBN-x5-Sigmoid : LikeBN-x5, but with sigmoid nonlinearityg(t) =1\\n1+exp(−x)instead of ReLU. We also attempted to train the original Inception with sigmoid, but\\nthemodelremainedat theaccuracyequivalenttochance.\\nIn Figure 2, we show the validation accuracy of the\\nnetworks, as a function of the number of training steps.\\nInception reached the accuracy of 72.2% after 31·106\\ntraining steps. The Figure 3 shows, for each network,\\nthe number of training steps required to reach the same',\n",
       " 'Inception reached the accuracy of 72.2% after 31·106\\ntraining steps. The Figure 3 shows, for each network,\\nthe number of training steps required to reach the same\\n72.2%accuracy,aswellasthemaximumvalidationaccuracy reached by the network and the number of steps to\\nreachit.\\nBy onlyusingBatch Normalization( BN-Baseline ),we\\nmatchtheaccuracyofInceptioninlessthanhalfthenumber of training steps. By applying the modiﬁcations in\\nSec. 4.2.1, we signiﬁcantly increase the training speed of\\nthe network. BN-x5needs 14 times fewer steps than Inception to reach the 72.2% accuracy. Interestingly, increasing the learning rate further ( BN-x30) causes the\\nmodel to train somewhat slowerinitially, but allows it to\\nreachahigherﬁnalaccuracy. Itreaches74.8%after 6·106\\nsteps, i.e. 5 times fewer steps than required by Inception\\ntoreach72.2%.\\nWe also veriﬁed that the reduction in internal covariate shift allows deep networks with Batch Normalizationto be trained when sigmoid is used as the nonlinearity,',\n",
       " 'We also veriﬁed that the reduction in internal covariate shift allows deep networks with Batch Normalizationto be trained when sigmoid is used as the nonlinearity,\\ndespite the well-known difﬁculty of training such networks. Indeed, BN-x5-Sigmoid achieves the accuracy of\\n69.8%. WithoutBatchNormalization,Inceptionwithsigmoidneverachievesbetterthan 1/1000accuracy.\\n4.2.3 Ensemble Classiﬁcation\\nThe current reported best results on the ImageNet Large\\nScale Visual RecognitionCompetitionare reachedby the\\nDeep Image ensemble of traditional models (Wuet al.,\\n2015) and the ensemble model of (Heet al., 2015). The\\nlatterreportsthetop-5errorof4.94%,asevaluatedbythe\\nILSVRCserver. Herewereportatop-5validationerrorof\\n4.9%, and test error of 4.82% (according to the ILSVRC\\nserver). This improvesupon the previousbest result, and\\nexceedstheestimatedaccuracyofhumanratersaccording\\nto(Russakovskyet al.,2014).\\nForourensemble,weused6networks. Eachwasbased\\nonBN-x30,modiﬁedviasomeofthefollowing: increased',\n",
       " 'Forourensemble,weused6networks. Eachwasbased\\nonBN-x30,modiﬁedviasomeofthefollowing: increased\\ninitial weights in the convolutionallayers; using Dropout\\n(with the Dropout probability of 5% or 10%, vs. 40%\\nfor the original Inception); and using non-convolutional,\\nper-activation Batch Normalization with last hidden layers of the model. Each network achieved its maximum\\naccuracyafter about 6·106training steps. The ensemble\\nprediction was based on the arithmetic average of class\\nprobabilities predicted by the constituent networks. The\\ndetailsofensembleandmulticropinferencearesimilarto\\n(Szegedyet al., 2014).\\nWe demonstrate in Fig. 4 that batch normalization allowsusto set new state-of-the-artby a healthymarginon\\ntheImageNetclassiﬁcationchallengebenchmarks.\\n5 Conclusion\\nWe have presented a novel mechanism for dramatically\\naccelerating the training of deep networks. It is based on\\nthe premise that covariate shift, which is known to complicate the trainingof machine learning systems, also ap7\\nModel Resolution Crops Models Top-1error Top-5error\\nGoogLeNetensemble 224 144 7 - 6.67%',\n",
       " 'Model Resolution Crops Models Top-1error Top-5error\\nGoogLeNetensemble 224 144 7 - 6.67%\\nDeepImagelow-res 256 - 1 - 7.96%\\nDeepImagehigh-res 512 - 1 24.88 7.42%\\nDeepImageensemble variable - - - 5.98%\\nBN-Inceptionsinglecrop 224 1 1 25.2% 7.82%\\nBN-Inceptionmulticrop 224 144 1 21.99% 5.82%\\nBN-Inceptionensemble 224 144 6 20.1% 4.9%*\\nFigure 4: Batch-Normalized Inception comparison with previous stat e of the art on the provided validation set comprising50000images. *BN-Inceptionensemblehasreached4 .82%top-5erroronthe100000imagesofthetestsetof\\ntheImageNetasreportedbythe test server.\\nplies to sub-networks and layers, and removing it from\\ninternal activations of the network may aid in training.\\nOur proposed method draws its power from normalizing\\nactivations, and from incorporating this normalization in\\nthe network architecture itself. This ensures that the normalization is appropriately handled by any optimization\\nmethod that is being used to train the network. To enable stochastic optimization methods commonly used in\\ndeep network training, we perform the normalization for',\n",
       " 'method that is being used to train the network. To enable stochastic optimization methods commonly used in\\ndeep network training, we perform the normalization for\\neachmini-batch,andbackpropagatethegradientsthrough\\nthe normalization parameters. Batch Normalization adds\\nonly two extra parameters per activation, and in doing so\\npreserves the representation ability of the network. We\\npresentedanalgorithmforconstructing,training,andper forming inference with batch-normalized networks. The\\nresulting networks can be trained with saturating nonlinearities, are more tolerant to increased training rates, an d\\noftendonotrequireDropoutforregularization.\\nMerely adding Batch Normalization to a state-of-theartimageclassiﬁcationmodelyieldsasubstantialspeedup\\nin training. By further increasing the learning rates, removing Dropout, and applying other modiﬁcations afforded by Batch Normalization, we reach the previous\\nstate of the art with onlya small fractionof trainingsteps\\n–andthenbeatthestateoftheartinsingle-networkimage\\nclassiﬁcation. Furthermore, by combining multiple models trained with Batch Normalization, we perform better\\nthanthebestknownsystemonImageNet,byasigniﬁcant\\nmargin.',\n",
       " 'thanthebestknownsystemonImageNet,byasigniﬁcant\\nmargin.\\nInterestingly, our method bears similarity to the standardization layer of (G¨ ulc ¸ehre& Bengio, 2013), though\\nthe two methodsstem from very differentgoals, and perform different tasks. The goal of Batch Normalization\\nis to achieve a stable distribution of activation values\\nthroughout training, and in our experiments we apply it\\nbefore the nonlinearity since that is where matching the\\nﬁrst and second moments is more likely to result in a\\nstable distribution. On the contrary,(G¨ ulc ¸ehre&Bengio ,\\n2013) apply the standardizationlayer to the outputof the\\nnonlinearity, which results in sparser activations. In our\\nlarge-scaleimage classiﬁcation experiments,we havenot\\nobservedthenonlinearity inputstobesparse,neitherwith\\nnor without Batch Normalization. Other notable differ-entiating characteristics of Batch Normalization include\\nthe learned scale and shift that allow the BN transform\\nto representidentity (the standardizationlayer did not re quirethissinceitwasfollowedbythelearnedlineartransform that, conceptually, absorbs the necessary scale and',\n",
       " 'to representidentity (the standardizationlayer did not re quirethissinceitwasfollowedbythelearnedlineartransform that, conceptually, absorbs the necessary scale and\\nshift), handling of convolutional layers, deterministic i nferencethatdoesnotdependonthemini-batch,andbatchnormalizingeachconvolutionallayerin thenetwork.\\nIn this work, we have not explored the full range of\\npossibilitiesthatBatchNormalizationpotentiallyenabl es.\\nOur future work includes applications of our method to\\nRecurrent Neural Networks (Pascanuet al., 2013), where\\ntheinternalcovariateshiftandthevanishingorexploding\\ngradients may be especially severe, and which would allowustomorethoroughlytestthehypothesisthatnormalizationimprovesgradientpropagation(Sec.3.3). Weplan\\ntoinvestigatewhetherBatchNormalizationcanhelpwith\\ndomain adaptation, in its traditional sense – i.e. whether\\nthe normalization performed by the network would allow it to more easily generalize to new data distributions,perhapswithjustarecomputationofthepopulation\\nmeansandvariances(Alg.2). Finally,webelievethatfurthertheoreticalanalysisofthealgorithmwouldallowstil l\\nmoreimprovementsandapplications.\\nReferences',\n",
       " 'moreimprovementsandapplications.\\nReferences\\nBengio, Yoshua and Glorot, Xavier. Understanding the\\ndifﬁcultyoftrainingdeepfeedforwardneuralnetworks.\\nInProceedings of AISTATS 2010 , volume 9, pp. 249–\\n256,May2010.\\nDean,Jeffrey,Corrado,GregS.,Monga,Rajat,Chen,Kai,\\nDevin,Matthieu,Le,QuocV., Mao,MarkZ.,Ranzato,\\nMarc’Aurelio,Senior,Andrew,Tucker,Paul,Yang,Ke,\\nand Ng, Andrew Y. Large scale distributed deep networks. In NIPS,2012.\\nDesjardins, Guillaume and Kavukcuoglu,Koray. Natural\\nneuralnetworks. (unpublished).\\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive\\nsubgradientmethodsfor onlinelearning and stochastic\\n8\\noptimization. J.Mach.Learn.Res. ,12:2121–2159,July\\n2011. ISSN1532-4435.\\nG¨ ulc ¸ehre, C ¸aglar and Bengio, Yoshua. Knowledge matters: Importanceof prior informationfor optimization.',\n",
       " 'G¨ ulc ¸ehre, C ¸aglar and Bengio, Yoshua. Knowledge matters: Importanceof prior informationfor optimization.\\nCoRR,abs/1301.4083,2013.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving Deep\\ninto Rectiﬁers: Surpassing Human-Level Performance\\non ImageNet Classiﬁcation. ArXiv e-prints , February\\n2015.\\nHyv¨ arinen, A. and Oja, E. Independentcomponent analysis: Algorithms and applications. Neural Netw. , 13\\n(4-5):411–430,May2000.\\nJiang, Jing. A literature survey on domain adaptation of\\nstatistical classiﬁers, 2008.\\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.\\nGradient-based learning applied to document recognition.Proceedings of the IEEE , 86(11):2278–2324,\\nNovember1998a.\\nLeCun, Y., Bottou, L., Orr, G., and Muller, K. Efﬁcient\\nbackprop. InOrr,G.andK.,Muller(eds.), NeuralNetworks: Tricks ofthetrade .Springer,1998b.',\n",
       " 'backprop. InOrr,G.andK.,Muller(eds.), NeuralNetworks: Tricks ofthetrade .Springer,1998b.\\nLyu, S and Simoncelli, E P. Nonlinear image representation using divisive normalization. In Proc. Computer\\nVision and Pattern Recognition , pp. 1–8. IEEE Computer Society, Jun 23-28 2008. doi: 10.1109/CVPR.\\n2008.4587821.\\nNair,VinodandHinton,GeoffreyE. Rectiﬁedlinearunits\\nimprove restricted boltzmann machines. In ICML, pp.\\n807–814.Omnipress,2010.\\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\\nOn the difﬁculty of training recurrent neural networks.\\nInProceedingsofthe30thInternationalConferenceon\\nMachineLearning,ICML 2013,Atlanta,GA,USA,1621June2013 ,pp.1310–1318,2013.\\nPovey, Daniel, Zhang, Xiaohui, and Khudanpur, Sanjeev. Parallel training of deep neural networks with\\nnatural gradient and parameter averaging. CoRR,\\nabs/1410.7455,2014.',\n",
       " 'natural gradient and parameter averaging. CoRR,\\nabs/1410.7455,2014.\\nRaiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep\\nlearning made easier by linear transformations in perceptrons. In International Conference on Artiﬁcial IntelligenceandStatistics(AISTATS) ,pp.924–932,2012.\\nRussakovsky,Olga,Deng,Jia,Su,Hao,Krause,Jonathan,\\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,\\nAlexander C., and Fei-Fei, Li. ImageNet Large Scale\\nVisual RecognitionChallenge,2014.Saxe, Andrew M., McClelland, James L., and Ganguli,\\nSurya. Exact solutions to the nonlinear dynamics\\nof learning in deep linear neural networks. CoRR,\\nabs/1312.6120,2013.\\nShimodaira, Hidetoshi. Improving predictive inference\\nunder covariate shift by weighting the log-likelihood\\nfunction. JournalofStatisticalPlanningandInference ,\\n90(2):227–244,October2000.',\n",
       " 'under covariate shift by weighting the log-likelihood\\nfunction. JournalofStatisticalPlanningandInference ,\\n90(2):227–244,October2000.\\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\\nA simple way to preventneural networksfrom overﬁtting.J. Mach. Learn. Res. , 15(1):1929–1958, January\\n2014.\\nSutskever, Ilya, Martens, James, Dahl, George E., and\\nHinton, Geoffrey E. On the importance of initialization and momentum in deep learning. In ICML\\n(3), volume 28 of JMLR Proceedings , pp. 1139–1147.\\nJMLR.org,2013.\\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,\\nPierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. CoRR,\\nabs/1409.4842,2014.',\n",
       " 'abs/1409.4842,2014.\\nWiesler, Simon and Ney, Hermann. A convergenceanalysis of log-lineartraining. In Shawe-Taylor,J., Zemel,\\nR.S.,Bartlett,P.,Pereira,F.C.N.,andWeinberger,K.Q.\\n(eds.),AdvancesinNeuralInformationProcessingSystems24,pp.657–665,Granada,Spain,December2011.\\nWiesler, Simon, Richard, Alexander, Schl¨ uter, Ralf, and\\nNey, Hermann. Mean-normalized stochastic gradient\\nfor large-scale deep learning. In IEEE International\\nConference on Acoustics, Speech, and Signal Processing,pp.180–184,Florence,Italy,May2014.\\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and\\nSun,Gang. Deepimage: Scalingupimagerecognition,\\n2015.\\nAppendix\\nVariantofthe Inception Model Used\\nFigure 5 documents the changes that were performed\\ncompared to the architecture with respect to the\\nGoogleNet archictecture. For the interpretation of this\\ntable, please consult (Szegedyetal., 2014). The notable\\narchitecture changes compared to the GoogLeNet model\\ninclude:',\n",
       " 'table, please consult (Szegedyetal., 2014). The notable\\narchitecture changes compared to the GoogLeNet model\\ninclude:\\n•The 5×5 convolutional layers are replaced by two\\nconsecutive 3×3 convolutional layers. This increases the maximum depth of the network by 9\\n9\\nweight layers. Also it increases the number of parameters by 25% and the computational cost is increasedbyabout30%.\\n•The number 28×28 inception modules is increased\\nfrom2to 3.\\n•Inside the modules, sometimes average, sometimes\\nmaximum-poolingis employed. This is indicated in\\ntheentriescorrespondingtothepoolinglayersofthe\\ntable.\\n•There are no across the board pooling layers between any two Inception modules, but stride-2 convolution/pooling layers are employed before the ﬁlterconcatenationin themodules3c,4e.\\nOur model employed separable convolution with depth\\nmultiplier 8on the ﬁrst convolutionallayer. This reduces\\nthecomputationalcost while increasingthememoryconsumptionat trainingtime.\\n10\\ntypepatch size/\\nstrideoutput\\nsizedepth #1×1#3×3\\nreduce#3×3double#3×3\\nreducedouble\\n#3×3Pool+proj',\n",
       " 'strideoutput\\nsizedepth #1×1#3×3\\nreduce#3×3double#3×3\\nreducedouble\\n#3×3Pool+proj\\nconvolution* 7×7/2112×112×641\\nmaxpool 3×3/256×56×64 0\\nconvolution 3×3/156×56×192 1 64 192\\nmaxpool 3×3/228×28×192 0\\ninception (3a) 28×28×256 364 64 64 64 96 avg+ 32\\ninception (3b) 28×28×320 364 64 96 64 96 avg+ 64\\ninception (3c) stride 2 28×28×576 3 0128 160 64 96max +pass through\\ninception (4a) 14×14×576 3224 64 96 96 128 avg+ 128\\ninception (4b) 14×14×576 3192 96 128 96 128 avg+ 128\\ninception (4c) 14×14×576 3160 128 160 128 160 avg+ 128\\ninception (4d) 14×14×576 396 128 192 160 192 avg+ 128\\ninception (4e) stride 2 14×14×1024 3 0128 192 192 256max +pass through\\ninception (5a) 7×7×1024 3352 192 320 160 224 avg+ 128',\n",
       " 'inception (5a) 7×7×1024 3352 192 320 160 224 avg+ 128\\ninception (5b) 7×7×1024 3352 192 320 192 224 max+ 128\\navgpool 7×7/11×1×1024 0\\nFigure5: Inceptionarchitecture\\n11',\n",
       " '1\\nMultiscale Combinatorial Grouping\\nfor Image Segmentation and\\nObject Proposal Generation\\nJordi Pont-Tuset*, Pablo Arbel ´aez*, Jonathan T. Barron, Member, IEEE,\\nFerran Marques, Senior Member, IEEE, Jitendra Malik, Fellow, IEEE\\nAbstract —We propose a uniﬁed approach for bottom-up hierarchical image segmentation and object proposal generation for\\nrecognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we ﬁrst develop a fast normalized cuts algorithm.\\nWe then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a\\ngrouping strategy that combines our multiscale regions into highly-accurate object proposals by exploring efﬁciently their combinatorial\\nspace. We also present Single-scale Combinatorial Grouping (SCG), a faster version of MCG that produces competitive proposals in\\nunder ﬁve second per image. We conduct an extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD,\\nand COCO datasets, showing that MCG produces state-of-the-art contours, hierarchical regions, and object proposals.\\nIndex Terms —Image segmentation, object proposals, normalized cuts.\\nF\\n1 I NTRODUCTION',\n",
       " 'Index Terms —Image segmentation, object proposals, normalized cuts.\\nF\\n1 I NTRODUCTION\\nTWOparadigms have shaped the ﬁeld of object recognition in the last decade. The ﬁrst one, popularized\\nby the Viola-Jones face detection algorithm [1], formulates object localization as window classiﬁcation. The basic scanning-window architecture, relying on histograms\\nof gradients and linear support vector machines, was\\nintroduced by Dalal and Triggs [2] in the context of\\npedestrian detection and is still at the core of seminal\\nobject detectors on the PASCAL challenge such as Deformable Part Models [3].\\nThe second paradigm relies on perceptual grouping to\\nprovide a limited number of high-quality and categoryindependent object proposals, which can then be described with richer representations and used as input to\\nmore sophisticated learning methods. Examples in this\\nfamily are [4], [5]. Recently, this approach has dominated\\nthe PASCAL segmentation challenge [6], [7], [8], [9], improved object detection [10], ﬁne-grained categorization\\n[11] and proven competitive in large-scale classiﬁcation\\n[12].\\nSince the power of this second paradigm is critically\\ndependent on the accuracy and the number of object',\n",
       " '[11] and proven competitive in large-scale classiﬁcation\\n[12].\\nSince the power of this second paradigm is critically\\ndependent on the accuracy and the number of object\\nproposals, an increasing body of research has delved\\n\\x0fJ. Pont-Tuset and F. Marques are with the Department of Signal Theory\\nand Communications, Universitat Polit` ecnica de Catalunya, BarcelonaTech\\n(UPC), Spain. E-mail: fjordi.pont,ferran.marques g@upc.edu\\n\\x0fP . Arbel´ aez is with the Department of Biomedical Engineering, Universidad de los Andes, Colombia. E-mail: pa.arbelaez@uniandes.edu.co\\n\\x0fJ. T. Barron, and J. Malik are with the Department of Electrical\\nEngineering and Computer Science, University of California at Berkeley,\\nBerkeley, CA 94720. E-mail: fbarron,malikg@eecs.berkeley.edu\\n* The ﬁrst two authors contributed equally\\nFig. 1. Top: original image, instance-level ground truth\\nfrom COCO and our multiscale hierarchical segmentation. Bottom: our best object proposals among 150.\\ninto the problem of their generation [13], [14], [15], [12],',\n",
       " 'into the problem of their generation [13], [14], [15], [12],\\n[16], [17], [18], [19]. However, those approaches typically\\nfocus on learning generic properties of objects from\\na set of examples, while reasoning on a ﬁxed set of\\nregions and contours produced by external bottom-up\\nsegmenters such as [20], [21].\\nIn this paper, we propose a uniﬁed approach to\\nmultiscale hierarchical segmentation and object proposal\\ngeneration called Multiscale Combinatorial Grouping\\n(MCG). Fig. 1 shows an example of our results and Fig. 2\\nan overview of our pipeline. Our main contributions are:\\n\\x0fAn efﬁcient normalized cuts algorithm, which in\\npractice provides a 20\\x02speed-up to the eigenvector\\ncomputation required for contour globalization [20],\\n[22] (Sect. 3.1).\\n\\x0fA state-of-the-art hierarchical segmenter that leverages multiscale information (Sect. 3.3).\\n\\x0fA grouping algorithm that produces accurate object\\nproposals by efﬁciently exploring the combinatorial\\nspace of our multiscale regions (Sect. 5).arXiv:1503.00848v4  [cs.CV]  1 Mar 2016\\n2',\n",
       " 'space of our multiscale regions (Sect. 5).arXiv:1503.00848v4  [cs.CV]  1 Mar 2016\\n2\\nF ixed-Scale\\nSegmentation R escaling &\\nA lignment C ombination  R es oluti on \\nC ombinatorial\\nGrouping\\n \\nI mage P yramid  Segmentati on P yramid  Al i gned  H iera rchies  Object Proposals  \\n Multiscale Hierarchy\\nFig. 2. Multiscale Combinatorial Grouping . Starting from a multiresolution image pyramid, we perform hierarchical\\nsegmentation at each scale independently. We align these multiple hierarchies and combine them into a single\\nmultiscale segmentation hierarchy. Our grouping component then produces a ranked list of object proposals by\\nefﬁciently exploring the combinatorial space of these regions.\\nWe conduct a comprehensive and large-scale empirical\\nvalidation. On the BSDS500 (Sect. 4) we report signiﬁcant\\nprogress in contour detection and hierarchical segmentation. On the VOC2012, SBD, and COCO segmentation\\ndatasets (Sect. 6), our proposals obtain overall state-ofthe-art accuracy both as segmented proposals and as',\n",
       " 'datasets (Sect. 6), our proposals obtain overall state-ofthe-art accuracy both as segmented proposals and as\\nbounding boxes. MCG is efﬁcient, its good generalization power makes it parameter free in practice, and it\\nprovides a ranked set of proposals that are competitive\\nin all regimes of number of proposals.\\n2 R ELATED WORK\\nFor space reasons, we focus our review on recent normalized cut algorithms and object proposals for recognition.\\nFast normalized cuts : The efﬁcient computation of\\nnormalized-cuts eigenvectors has been the subject of\\nrecent work, as it is often the computational bottleneck\\nin grouping algorithms. Taylor [23] presented a technique for using a simple watershed oversegmentation\\nto reduce the size of the eigenvector problem, sacriﬁcing accuracy for speed. We take a similar approach\\nof solving the eigenvector problem in a reduced space,\\nthough we use simple image-pyramid operations on\\nthe afﬁnity matrix (instead of a separate segmentation\\nalgorithm) and we see no loss in performance despite a\\n20\\x02speed improvement. Maire and Yu [24] presented\\na novel multigrid solver for producing eigenvectors at\\nmultiple scales, which speeds up ﬁne-scale eigenvector\\ncomputation by leveraging coarse-scale solutions. Our',\n",
       " 'multiple scales, which speeds up ﬁne-scale eigenvector\\ncomputation by leveraging coarse-scale solutions. Our\\ntechnique also uses the scale-space structure of an image,\\nbut instead of solving the problem at multiple scales,\\nwe simply reduce the scale of the problem, solve it at\\na reduced scale, and then upsample the solution while\\npreserving the structure of the image. As such, our\\ntechnique is faster and much simpler, requiring only a\\nfew lines of code wrapped around a standard sparse\\neigensolver.Object Proposals : Class-independent methods that\\ngenerate object hypotheses can be divided into those\\nwhose output is an image window and those that generate segmented proposals.\\nAmong the former, Alexe et al. [16] propose an objectness measure to score randomly-sampled image windows based on low-level features computed on the\\nsuperpixels of [21]. Manen et al. [25] propose to use the\\nRandomized Prim’s algorithm, Zitnick et al. [26] group\\ncontours directly to produce object windows, and Cheng\\net al. [27] generate box proposals at 300 images per\\nsecond. In contrast to these approaches, we focus on\\nthe ﬁner-grained task of pixel-accurate object extraction,',\n",
       " 'second. In contrast to these approaches, we focus on\\nthe ﬁner-grained task of pixel-accurate object extraction,\\nrather than on window selection. However, by just taking the bounding box around our segmented proposals,\\nour results are also state of the art as window proposals.\\nAmong the methods that produce segmented proposals, Carreira and Sminchisescu [18] hypothesize a set of\\nplacements of fore- and background seeds and, for each\\nconﬁguration, solve a constrained parametric min-cut\\n(CPMC) problem to generate a pool of object hypotheses.\\nEndres and Hoiem [19] base their category-independent\\nobject proposals on an iterative generation of a hierarchy\\nof regions, based on the contour detector of [20] and\\nocclusion boundaries of [28]. Kim and Grauman [17]\\npropose to match parts of the shape of exemplar objects,\\nregardless of their class, to detected contours by [20].\\nThey infer the presence and shape of a proposal object\\nby adapting the matched object to the computed superpixels.\\nUijlings et al. [12] present a selective search algorithm\\nbased on segmentation. Starting with the superpixels\\nof [21] for a variety of color spaces, they produce a set of\\nsegmentation hierarchies by region merging, which are',\n",
       " 'based on segmentation. Starting with the superpixels\\nof [21] for a variety of color spaces, they produce a set of\\nsegmentation hierarchies by region merging, which are\\nused to produce a set of object proposals. While we also\\ntake advantage of different hierarchies to gain diversity,\\nwe leverage multiscale information rather than different\\n3\\ncolor spaces.\\nRecently, two works proposed to train a cascade of\\nclassiﬁers to learn which sets of regions should be\\nmerged to form objects. Ren and Shankhnarovich [29]\\nproduce full region hierarchies by iteratively merging\\npairs of regions and adapting the classiﬁers to different\\nscales. Weiss and Taskar [30] specialize the classiﬁers also\\nto size and class of the annotated instances to produce\\nobject proposals.\\nMalisiewicz and Efros [4] took one of the ﬁrst steps\\ntowards combinatorial grouping, by running multiple\\nsegmenters with different parameters and merging up\\nto three adjacent regions. In [8], another step was taken\\nby considering hierarchical segmentations at three different scales and combining pairs and triplets of adjacent\\nregions from the two coarser scales to produce object\\nproposals.\\nThe most recent wave of object proposal algorithms',\n",
       " 'regions from the two coarser scales to produce object\\nproposals.\\nThe most recent wave of object proposal algorithms\\nis represented by [13], [14], and [15], which all keep the\\nquality of the seminal proposal works while improving\\nthe speed considerably. Kr ¨ahenb ¨uhl and Koltun [13]\\nﬁnd object proposal by identifying critical level sets\\nin geodesic distance transforms, based on seeds placed\\nin learnt places in the image. Rantalankila et al. [14]\\nperform a global and local search in the space of sets\\nof superpixels. Humayun et al. [15] reuse a graph to\\nperform many parametric min-cuts over different seeds\\nin order to speed the process up.\\nA substantial difference between our approach and\\nprevious work is that, instead of relying on precomputed hierarchies or superpixels, we propose a uniﬁed approach that produces and groups high-quality\\nmultiscale regions. With respect to the combinatorial approaches of [4], [8], our main contribution is to develop\\nefﬁcient algorithms to explore a much larger combinatorial space by taking into account a set of object examples,\\nincreasing thus the likelihood of having complete objects\\nin the pool of proposals. Our approach has therefore',\n",
       " 'increasing thus the likelihood of having complete objects\\nin the pool of proposals. Our approach has therefore\\nthe ﬂexibility to adapt to speciﬁc applications and types\\nof objects, and can produce proposals at any trade-off\\nbetween their number and their accuracy.\\n3 T HESEGMENTATION ALGORITHM\\nConsider a segmentation of the image into regions that\\npartition its domain S=fSigi. A segmentation hierarchy\\nis a family of partitions fS\\x03;S1;:::;SLgsuch that: (1)\\nS\\x03is the ﬁnest set of superpixels , (2)SLis the complete\\ndomain, and (3) regions from coarse levels are unions of\\nregions from ﬁne levels. A hierarchy where each level Si\\nis assigned a real-valued index \\x15ican be represented by a\\ndendrogram, a region tree where the height of each node\\nis its index. Furthermore, it can also be represented as an\\nultrametric contour map (UCM), an image obtained by\\nweighting the boundary of each pair of adjacent regions\\nin the hierarchy by the index at which they are merged\\n[31], [32]. This representation uniﬁes the problems of\\ncontour detection and hierarchical image segmentation:\\n*\\n##\\n#',\n",
       " '[31], [32]. This representation uniﬁes the problems of\\ncontour detection and hierarchical image segmentation:\\n*\\n##\\n#\\nFig. 3. Duality between a UCM and a region tree :\\nSchematic view of the dual representation of a segmentation hierarchy as a region dendrogram and as an\\nultrametric contour map.\\na threshold at level \\x15iin the UCM produces the segmentationSi.\\nFigure 3 schematizes these concepts. First, the lower\\nleft corner shows the probability of boundary of a UCM.\\nOne of the main properties of a UCM is that when we\\nthreshold the contour strength at a certain value, we\\nobtain a closed boundary map, and thus a partition.\\nThresholding at different \\x15i, therefore, we obtain the\\nso-called merging-sequence partitions (left column in\\nFigure 3); named after the fact that a step in this sequence\\ncorresponds to merging the set of regions sharing the\\nboundary of strength exactly \\x15i.\\nFor instance, the boundary between the wheels and\\nthe ﬂoor has strength \\x151, thus thresholding the contour\\nabove\\x151makes the wheels merge with the ﬂoor. If we\\nrepresent the regions in a partition as nodes of a graph,\\nwe can then represent the result of merging them as their',\n",
       " 'represent the regions in a partition as nodes of a graph,\\nwe can then represent the result of merging them as their\\nparent in a tree. The result of sweeping all \\x15ivalues\\ncan therefore be represented as a region tree, whose root\\nis the region representing the whole image (right part\\nof Figure 3). Given that each merging is associated with\\na contour strength, the region tree is in fact a region\\ndendogram.\\nAs an example, in the gPb-ucm algorithm of [20],\\nbrightness, color and texture gradients at three ﬁxed disk\\nsizes are ﬁrst computed. These local contour cues are\\nglobalized using spectral graph-partitioning, resulting in\\nthe gPb contour detector. Hierarchical segmentation is\\nthen performed by iteratively merging adjacent regions\\nbased on the average gPb strength on their common\\nboundary. This algorithm produces therefore a tree of\\nregions at multiple levels of homogeneity in brightness,\\ncolor and texture, and the boundary strength of its UCM\\ncan be interpreted as a measure of contrast.\\nCoarse-to-ﬁne is a powerful processing strategy in\\ncomputer vision. We exploit it in two different ways\\nto develop an efﬁcient, scalable and high-performance\\nsegmentation algorithm: (1) To speed-up spectral graph',\n",
       " 'to develop an efﬁcient, scalable and high-performance\\nsegmentation algorithm: (1) To speed-up spectral graph\\npartitioning and (2) To create aligned segmentation hierarchies.\\n4\\n3.1 Fast Downsampled Eigenvector Computation\\nThe normalized cuts criterion is a key globalization\\nmechanism of recent high-performance contour detectors such as [20], [22]. Although powerful, such spectral\\ngraph partitioning has a signiﬁcant computational cost\\nand memory footprint that limit its scalability. In this\\nsection, we present an efﬁcient normalized cuts approximation which in practice preserves full performance for\\ncontour detection, while having low memory requirements and providing a 20\\x02speed-up.\\nGiven a symmetric afﬁnity matrix A, we would like\\nto compute the ksmallest eigenvectors of the Laplacian\\nofA. Directly computing such eigenvectors can be very\\ncostly even with sophisticated solvers, due to the large\\nsize ofA. We therefore present a technique for efﬁciently\\napproximating the eigenvector computation by taking\\nadvantage of the multiscale nature of our problem: A\\nmodels afﬁnities between pixels in an image, and images\\nnaturally lend themselves to multiscale or pyramid-like',\n",
       " 'models afﬁnities between pixels in an image, and images\\nnaturally lend themselves to multiscale or pyramid-like\\nrepresentations and algorithms.\\nOur algorithm is inspired by two observations: 1) if\\nAis bistochastic (the rows and columns of Asum to 1)\\nthen the eigenvectors of the Laplacian Aare equal to\\nthe eigenvectors of the Laplacian of A2, and 2) because\\nof the scale-similar nature of images, the eigenvectors\\nof a “downsampled” version of Ain which every other\\npixel has been removed should be similar to the eigenvectors of A. Let us deﬁne pixel decimate (A), which\\ntakes an afﬁnity matrix Aand returns the set of indices\\nof rows/columns in Acorresponding to a decimated\\nversion of the image from which Awas constructed. That\\nis, ifi=pixel decimate (A), thenA[i;i]is a decimated\\nmatrix in which alternating rows and columns of the imagehave been removed. Computing the eigenvectors of\\nA[i;i]works poorly, as decimation disconnects pixels in\\nthe afﬁnity matrix, but the eigenvectors of the decimated',\n",
       " 'A[i;i]works poorly, as decimation disconnects pixels in\\nthe afﬁnity matrix, but the eigenvectors of the decimated\\nsquared afﬁnity matrix A2[i;i]are similar to those of A,\\nbecause by squaring the matrix before decimation we\\nintuitively allow each pixel to propagate information to\\nall of its neighbors in the graph, maintaining connections even after decimation. Our algorithm works by\\nefﬁciently computing A2[i;i]asA[:;i]TA[:;i]1(the naive\\napproach of ﬁrst squaring Aand then decimating it is\\nprohibitively expensive), computing the eigenvectors of\\nA2[i;i], and then “upsampling” those eigenvectors back\\nto the space of the original image by pre-multiplying\\nbyA[:;i]. This squaring-and-decimation procedure can\\nbe applied recursively several times, each application\\nimproving efﬁciency while slightly sacriﬁcing accuracy.\\nPseudocode for our algorithm, which we call\\n“DNCuts” (Downsampled Normalized Cuts) is given in',\n",
       " 'Pseudocode for our algorithm, which we call\\n“DNCuts” (Downsampled Normalized Cuts) is given in\\nAlgorithm 1, where Ais our afﬁnity matrix and dis\\nthe number of times that our squaring-and-decimation\\noperation is applied. Our algorithm repeatedly applies\\nour joint squaring-and-decimation procedure, computes\\n1. The Matlab-like notation A[:; i]indicates keeping the columns of\\nmatrix Awhose indices are in the set i.Algorithm 1 dncuts (A;d;k )\\n1:A0 A\\n2:fors= [1;2;:::;d ]do\\n3:is pixel decimate (As\\x001)\\n4:Bs As\\x001[ :; is]\\n5:Cs diag (Bs~1)\\x001Bs\\n6:As CT\\nsBs\\n7:end for\\n8:Xd ncuts (Ad;k)\\n9:fors= [d;d\\x001;:::; 1]do\\n10:Xs\\x001 CsXs\\n11:end for\\n12:return whiten (X0)\\nFig. 4. Example of segmentation projection . In order to\\n“snap” the boundaries of a segmentation R(left) to those',\n",
       " 'Fig. 4. Example of segmentation projection . In order to\\n“snap” the boundaries of a segmentation R(left) to those\\nof a segmentation S(middle), since they do not align, we\\ncompute\\x19(R;S)(right) by assigning to each segment in\\nSits mode among the labels of R.\\nthe smallest keigenvectors of the ﬁnal “downsampled” matrix Adby using a standard sparse eigensolver\\nncuts (Ad;k), and repeatedly “upsamples” those eigenvectors. Because our Ais not bistochastic and decimation\\nis not an orthonormal operation, we must do some\\nnormalization throughout the algorithm (line 5) and\\nwhiten the resulting eigenvectors (line 10). We found that\\nvalues ofd= 2 ord= 3 worked well in practice. Larger\\nvalues ofdyielded little speed improvement (as much of\\nthe cost is spent downsampling A0) and start negatively\\naffecting accuracy. Our technique is similar to Nystrom’s\\nmethod for computing the eigenvectors of a subset of A,\\nbut our squaring-and-decimation procedure means that\\nwe do not depend on long-range connections between\\npixels.\\n3.2 Aligning Segmentation Hierarchies',\n",
       " 'but our squaring-and-decimation procedure means that\\nwe do not depend on long-range connections between\\npixels.\\n3.2 Aligning Segmentation Hierarchies\\nIn order to leverage multi-scale information, our approach combines segmentation hierarchies computed\\nindependently at multiple image resolutions. However, since subsampling an image removes details and\\nsmooths away boundaries, the resulting UCMs are misaligned, as illustrated in the second panel of Fig. 2. In this\\nsection, we propose an algorithm to align an arbitrary\\nsegmentation hierarchy to a target segmentation and,\\nin Sect. 5, we show its effectiveness for multi-scale\\nsegmentation.\\nThe basic operation is to “snap” the boundaries of a\\nsegmentationR=fRigito a segmentation S=fSjgj, as\\nillustrated in Fig. 4. For this purpose, we deﬁne L(Sj),\\n5\\nthe new label of a region Sj2S, as the majority label of\\nits pixels inR:\\nL(Sj) = arg max\\nijSj\\\\Rij\\njSjj(1)\\nWe call the segmentation deﬁned by this new labeling of\\nall the regions of Stheprojection ofRontoSand denote\\nit by\\x19(R;S).',\n",
       " 'We call the segmentation deﬁned by this new labeling of\\nall the regions of Stheprojection ofRontoSand denote\\nit by\\x19(R;S).\\nIn order to project an UCM onto a target segmentation\\nS, which we denote \\x19(UCM;S), we project in turn each\\nof the levels of the hierarchy onto S. Note that, since\\nall the levels are projected onto the same segmentation,\\nthe family of projections is by construction a hierarchy of segmentations. This procedure is summarized in\\npseudo-code in Algorithm 2.\\nAlgorithm 2 UCM Rescaling and Alignment\\nRequire: An UCM with a set of levels [t1;:::;tK]\\nRequire: A target segmentation S\\x03\\n1:UCM\\x19 0\\n2:fort= [t1;:::;tK]do\\n3:S sampleHierarchy (UCM;t)\\n4:S rescaleSegmentation (S;S\\x03)\\n5:S \\x19(S;S\\x03)\\n6:contours extractBoundary (S)\\n7: UCM\\x19 max(UCM\\x19;t\\x03contours )\\n8:end for\\n9:return UCM\\x19\\nObserve that the routines sampleHierarchy and\\nextractBoundary can be computed efﬁciently because',\n",
       " '8:end for\\n9:return UCM\\x19\\nObserve that the routines sampleHierarchy and\\nextractBoundary can be computed efﬁciently because\\nthey involve only thresholding operations and connected\\ncomponents labeling. The complexity is thus dominated\\nby rescaleSegmentation in Step 4, a nearest neighbor\\ninterpolation, and the projection in Step 5, which are\\ncomputedKtimes.\\n3.3 Multiscale Hierarchical Segmentation\\nSingle-scale segmentation : We consider as input\\nthe following local contour cues: (1) brightness, color and\\ntexture differences in half-disks of three sizes [33], (2)\\nsparse coding on patches [22], and (3) structured forest\\ncontours [34]. We globalize the contour cues independently using our fast eigenvector gradients of Sect. 3.1,\\ncombine global and local cues linearly, and construct\\nan UCM based on the mean contour strength. We tried\\nlearning weights using gradient ascent on the F-measure\\non the training set [20], but evaluating the ﬁnal hierarchies rather than open contours. We observed that this\\nobjective favors the quality of contours at the expense of\\nregions and obtained better overall results by optimizing\\nthe Segmentation Covering metric [20].',\n",
       " 'objective favors the quality of contours at the expense of\\nregions and obtained better overall results by optimizing\\nthe Segmentation Covering metric [20].\\nHierarchy Alignment : We construct a multiresolution pyramid with Nscales by subsampling / supersampling the original image and applying our singlescale segmenter. In order to preserve thin structures\\nand details, we declare as set of possible boundarylocations the ﬁnest superpixels in the highest-resolution.\\nThen, applying recursively Algorithm 2, we project each\\ncoarser UCM onto the next ﬁner scale until aligning it\\nto the highest resolution superpixels.\\nMultiscale Hierarchy : After alignment, we have a\\nﬁxed set of boundary locations, and Nstrengths for\\neach of them, coming from the different scales. We\\nformulate this problem as binary boundary classiﬁcation\\nand train a classiﬁer that combines these Nfeatures into\\na single probability of boundary estimation. We experimented with several learning strategies for combining\\nUCM strengths: (a) Uniform weights transformed into\\nprobabilities with Platt’s method. (b) SVMs and logistic\\nregression, with both linear and additive kernels. (c)',\n",
       " 'probabilities with Platt’s method. (b) SVMs and logistic\\nregression, with both linear and additive kernels. (c)\\nRandom Forests. (d) The same algorithm as for singlescale. We found the results with all learning methods\\nsurprisingly similar, in agreement with the observation\\nreported by [33]. This particular learning problem, with\\nonly a handful of dimensions and millions of data points,\\nis relatively easy and performance is mainly driven by\\nour already high-performing and well calibrated features. We therefore use the simplest option (a).\\n4 E XPERIMENTS ON THE BSDS500\\nWe conduct extensive experiments on the BSDS500 [35],\\nusing the standard evaluation metrics and following the\\nbest practice rules of that dataset. We also report results\\nwith a recent evaluation metric Fop[36], [37], PrecisionRecall for objects and parts, using the publicly-available\\ncode.\\nSingle-scale Segmentation : Table 1-top shows the\\nperformance of our single-scale segmenter for different\\ntypes of input contours on the validation set of the\\nBSDS500. We obtain high-quality hierarchies for all the\\ncues considered, showing the generality of our approach.\\nFurthermore, when using them jointly (row ’Comb.’\\nin top panel), our segmenter outperforms the versions\\nwith individual cues, suggesting its ability to leverage',\n",
       " 'Furthermore, when using them jointly (row ’Comb.’\\nin top panel), our segmenter outperforms the versions\\nwith individual cues, suggesting its ability to leverage\\ndiversiﬁed inputs. In terms of efﬁciency, our fast normalized cuts algorithm provides an average 20 \\x02speedup over [20], starting from the same local cues, with\\nno signiﬁcant loss in accuracy and with a low memory\\nfootprint.\\nMultiscale Segmentation : Table 1-bottom evaluates\\nour full approach in the same experimental conditions as\\nthe upper panel. We observe a consistent improvement\\nin performance in all the metrics for all the inputs, which\\nvalidates our architecture for multiscale segmentation.\\nWe experimented with the range of scales and found\\nN=f0:5;1;2gadequate for our purposes. A ﬁner\\nsampling or a wider range of scales did not provide\\nnoticeable improvements. We tested also two degraded\\nversions of our system (not shown in the table). For\\nthe ﬁrst one, we resized contours to the original image\\nresolution, created UCMs and combined them with the\\nsame method as our ﬁnal system. For the second one, we\\ntransformed per-scale UCMs to the original resolution,\\n6\\nBoundary Region',\n",
       " 'same method as our ﬁnal system. For the second one, we\\ntransformed per-scale UCMs to the original resolution,\\n6\\nBoundary Region\\nFb Fop SC PRI VI\\nInput ODS OIS ODS OIS ODS OIS ODS OIS ODS OISSingle-ScalePb [33] 0.702 0.733 0.334 0.370 0.577 0.636 0.801 0.847 1.692 1.490\\nSC [22] 0.697 0.725 0.264 0.306 0.540 0.607 0.777 0.835 1.824 1.659\\nSF [34] 0.719 0.737 0.338 0.399 0.582 0.651 0.803 0.851 1.608 1.432\\nComb. 0.719 0.750 0.358 0.403 0.602 0.655 0.809 0.855 1.619 1.405MultiscalePb [33] 0.713 0.745 0.350 0.389 0.598 0.656 0.807 0.856 1.601 1.418\\nSC [22] 0.705 0.734 0.331 0.384 0.579 0.647 0.799 0.851 1.637 1.460',\n",
       " 'SC [22] 0.705 0.734 0.331 0.384 0.579 0.647 0.799 0.851 1.637 1.460\\nSF [34] 0.725 0.744 0.370 0.420 0.600 0.660 0.810 0.854 1.557 1.390\\nComb. 0.725 0.757 0.371 0.408 0.611 0.670 0.813 0.862 1.548 1.367\\nTABLE 1\\nBSDS500 val set. Control experiments for single-scale\\n(top) and multiscale (bottom) hierarchical segmentation\\nwith different input contour detectors\\nbut omitted the strength transfer to the ﬁnest superpixels\\nbefore combining them. The ﬁrst ablated version produces interpolation artifacts and smooths away details,\\nwhile the second one suffers from misalignment. Both\\nfail to improve performance over the single-scale result,\\nwhich provides additional empirical support for our\\nmultiscale approach. We also observed a small degradation in performance when forcing the input contour\\ndetector to use only the original image resolution, which\\nindicates the advantages of considering multiscale information at all stages of processing.\\nSince there are no drastic changes in our results when\\ntaking as input the different individual cues or their combination, in the sequel we use the version with structured',\n",
       " 'Since there are no drastic changes in our results when\\ntaking as input the different individual cues or their combination, in the sequel we use the version with structured\\nforests for efﬁciency reasons, which we denote MCGUCM-Our .\\nComparison with state-of-the-art. : Figure 5 compares our multiscale hierarchical segmenter MCG ( )\\nand our single-scale hierarchical segmenter SCG ( )\\non the BSDS500 test set against all the methods for\\nwhich there is publicly available code. We also compare\\nto the recent ISCRA [29] hierarchies ( ), provided\\nprecomputed by the authors. We obtain consistently the\\nbest results to date on BSDS500 for all operating regimes,\\nboth in terms of boundary and region quality.\\nNote that the range of object scales in the BSDS500\\nis limited, which translates into modest absolute gains\\nfrom MCG ( ) with respect to SCG ( ) in terms\\nof boundary evaluation (left-hand plot), but more signiﬁcant improvements in terms of objects and parts\\n(right-hand plot). We will also observe more substantial\\nimprovements with respect to gPb-UCM ( ) when we\\nmove to PASCAL, SBD, and COCO in Section 6 (e.g. see\\nFig. 9).',\n",
       " 'move to PASCAL, SBD, and COCO in Section 6 (e.g. see\\nFig. 9).\\nGround-Truth Hierarchy : In order to gain further\\ninsights, we transfer the strength of each ground-truth\\nsegmentation to our highest-resolution superpixels SN\\x03\\nand construct a combined hierarchy. This approximation to the semantic hierarchy, Ground-Truth Hierarchy\\n(GTH) in Fig. 5, is an upper-bound for our approach as\\nboth share the same boundary locations and the only\\ndifference is their strength. Since the strength of GTHis proportional to the number of subjects marking it, it\\nprovides naturally the correct semantic ordering, where\\nouter object boundaries are stronger than internal parts.\\nRecently, Maire et al. [38] developed an annotation\\ntool where the user encodes explicitly the “perceptual\\nstrength” of each contour. Our approach provides an\\nalternative where the semantic hierarchy is reconstructed\\nby sampling ﬂat annotations from multiple subjects.\\n5 O BJECT PROPOSAL GENERATION\\nThe image segmentation algorithm presented in the\\nprevious sections builds on low-level features, so its\\nregions are unlikely to represent accurately complete\\nobjects with heterogeneous parts. In this context, object\\nproposal techniques create a set of hypotheses, possibly\\noverlapping, which are more likely to represent full',\n",
       " 'objects with heterogeneous parts. In this context, object\\nproposal techniques create a set of hypotheses, possibly\\noverlapping, which are more likely to represent full\\nobject instances.\\nOur approach to object proposal generation is to\\ncombinatorially look for sets of regions from our segmentation hierarchies that merged together are likely\\nto represent complete objects. In this section, we ﬁrst\\ndescribe the efﬁcient computation of certain region descriptors on a segmentation tree. Then, we describe how\\nwe use these techniques to efﬁciently explore the sets of\\nmerged regions from the hierarchy. Finally, we explain\\nhow we train the parameters of our algorithm for object\\nproposals and how we rank the candidates by their\\nprobability of representing an object.\\nFast computation of descriptors : Let us assume, for\\ninstance, we want to compute the area of all regions\\nin the hierarchy. Intuitively, working strictly on the\\nmerging-sequence partitions, we would need to scan all\\npixels in all partitions. On the other hand, working on\\nthe region tree allows us to scan the image only once to\\ncompute the area of the leaves, and then propagate the\\narea to all parents as the addition of the areas of their\\nchildren.\\nAs a drawback, the algorithms become intricate in\\nterms of coding and necessary data structures. Take, for',\n",
       " 'area to all parents as the addition of the areas of their\\nchildren.\\nAs a drawback, the algorithms become intricate in\\nterms of coding and necessary data structures. Take, for\\ninstance, the computation of the neighbors of a certain\\nregion, which is trivial via scanning the partition on the\\nmerging sequence (look for region labels in the adjacent\\nboundary pixels), but need tailored data structures and\\nalgorithms in the region tree.\\nFormally, let us assume the image has ppixels, and we\\nbuild a hierarchy based on ssuperpixels (leaves of the\\ntree), andmmergings (different UCM strength values).\\nThe cost of computing the area on all regions using the\\nmerging-sequence partitions will be the cost of scanning\\nall pixels in these partitions, thus p\\x01(m+1). In contrast,\\nthe cost using the region tree will involve scanning the\\nimage once, and then propagating the area values, so\\np+m, which is notably faster.\\nWe built tailored algorithms and data structures to\\ncompute the bounding box, perimeter, and neighbors of\\na region using the region tree representation.\\n7\\nBoundaries\\n0 0:10:20:30:40:50:60:70:80:9 100:10:20:30:40:50:60:70:80:91\\nRecallPrecision',\n",
       " 'RecallPrecision\\nHuman [0.81-0.21] ISCRA [0.72]\\nGTH [0.96] NCuts [0.63]\\nMCG-Our [0.74] EGB [0.61]\\nSCG-Our [0.74] MShift [0.60]\\ngPb-UCM [0.73] Quadtree [0.41]Objects and Parts\\n0 0:10:20:30:40:50:60:70:80:9 100:10:20:30:40:50:60:70:80:91\\nRecallPrecisionHuman [0.56-0.05]\\nGTH [0.64]\\nMCG-Our [0.38]\\nSCG-Our [0.35]\\nISCRA [0.35]\\ngPb-UCM [0.35]\\nMShift [0.23]\\nNCuts [0.21]\\nEGB [0.16]\\nQuadtree [0.06]\\nFig. 5. BSDS500 test set. Precision-Recall curves for boundaries [35] (left) and for objects and parts [36] (right). The\\nmarker on each curve is placed on the Optimal Dataset Scale (ODS), and its F measure is presented in brackets in the',\n",
       " 'marker on each curve is placed on the Optimal Dataset Scale (ODS), and its F measure is presented in brackets in the\\nlegend. The isolated red asterisks refer to the human performance assessed on the same image (one human partition\\nagainst the rest of human annotations on the same image) and on a different image (one human partition against the\\nhuman partitions of a different, randomly selected, image).\\nCombinatorial Grouping of Proposals : We can\\ncast object segmentation as selecting some regions in\\nthe hierarchy, or in other words, as a combinatorial\\noptimization problem on the hierarchy. To illustrate this\\nprinciple, Figure 6(a) shows the simpliﬁed representation\\nof the hierarchy in Figure 3. Figure 6(b) and (c) show two\\nobject proposals, and their representation in the region\\nhierarchy.\\nR4\\nR5\\nR1R2R3R6\\nR9\\nR7\\nR1R2R3R4R8\\nR5R6\\n(a)\\nR9\\nR7\\nR1R2R3R4R8\\nR5R6\\n(b)\\nR9\\nR7\\nR1R2R3R4R8\\nR5R6\\n(c)',\n",
       " 'R5R6\\n(b)\\nR9\\nR7\\nR1R2R3R4R8\\nR5R6\\n(c)\\nFig. 6. Object segmentation as combinatorial optimization : Examples of objects (b), (c), formed by selecting regions from a hierarchy (a).\\nSince hierarchies are built taking only low-level features into account, and do not use semantic information,\\nobjects will usually not be optimally represented using a\\nsingle region in the hierarchy. As an example, Figure 6(c)\\nshows the optimum representation of the car, consisting\\nof three regions.\\nA sensible approach to create object proposals is therefore to explore the set of n-tuples of regions. The main\\nidea behind MCG is to explore this set efﬁciently, taking\\nadvantage of the region tree representation, via the fastcomputation of region neighbors.\\nThe whole set of tuples, however, is huge, and so it\\nis not feasible to explore it exhaustively. Our approach\\nranks the proposals using the height of their regions in\\nthe tree (UCM strength) and explores the tree from the\\ntop, but only down to a certain threshold. To speed the\\nprocess up, we design a top-down algorithm to compute\\nthe region neighbors and thus only compute them down\\nto a certain depth in the tree.',\n",
       " 'process up, we design a top-down algorithm to compute\\nthe region neighbors and thus only compute them down\\nto a certain depth in the tree.\\nTo further improve the results, we not only consider\\nthen-tuples from the resulting MCG-UCM-Our hierarchy, but also the rest of hierarchies computed at different\\nscales. As we will show in the experiments, diversity\\nsigniﬁcantly improves the proposal results.\\nParameter Learning via Pareto Front Optimization :\\nMCG takes a set of diverse hierarchies and computes the\\nn-tuples up to a certain UCM strength. We can interpret\\nthen-tuples from each hierarchy as a ranked list of Ni\\nproposals that are put together to create the ﬁnal set of\\nNpproposals.\\nAt training time, we would like to ﬁnd, for different values of Np, the number of proposals from each\\nranked list Nisuch that the joint pool of Npproposals\\nhas the best achievable quality. We frame this learning\\nproblem as a Pareto front optimization [39], [40] with\\ntwo conﬂicting objective functions: number of proposals\\nand achievable quality. At test time, we select a working\\npoint on the Pareto front, represented by the\\x08\\nNi',\n",
       " 'and achievable quality. At test time, we select a working\\npoint on the Pareto front, represented by the\\x08\\nNi\\t\\nvalues, based either on the number of proposals Npwe\\ncan handle or on the minimum achievable quality our\\n8\\napplication needs, and we combine the Nitop proposals\\nfrom each hierarchy list.\\nFormally, assuming Rranked lists Li, an exhaustive\\nlearning algorithm would consider all possible values\\nof theR-tuplefN1;:::;NRg, whereNi2f0;:::;jLijg;\\nadding up toQR\\n1jLijparameterizations to try, which is\\nintractable in our setting.\\nFigure 7 illustrates the learning process. To reduce the\\ndimensionality of the search space, we start by selecting\\ntwo ranked lists L1,L2(green curves) and we sample\\nthe list atSlevels of number of proposals (green dots).\\nWe then scan the full S2different parameterizations to\\ncombine the proposals from both (blue dots). In other\\nwords, we analyze the sets of proposals created by\\ncombining the top N1fromL1(green dots) and the top\\nN2fromL2.\\n1011021031040.30.60.9\\nNumber of proposalsQuality\\n1011021031040.30.60.9\\nNumber of proposalsQuality',\n",
       " 'N2fromL2.\\n1011021031040.30.60.9\\nNumber of proposalsQuality\\n1011021031040.30.60.9\\nNumber of proposalsQuality\\n1011021031040.30.60.9\\nNumber of proposalsQualityRanked lists of object proposals\\nL 1 L RPareto front\\nreduction of parameters\\n{N 1 · · ·  N R}\\nFig. 7. Pareto front learning : Training the combinatorial\\ngeneration of proposals using the Pareto front\\nThe key step of the optimization consists in discarding\\nthose parameterizations whose quality point is not in\\nthe Pareto front (red curve). (i.e., those parameterizations\\nthat can be substituted by another with better quality\\nwith the same number of proposals, or by one with the\\nsame quality with less proposals.) We sample the Pareto\\nfront toSpoints and we iterate the process until all the\\nranked lists are combined.\\nEach point in the ﬁnal Pareto front corresponds to a\\nparticular parameterization fN1;:::;NRg. At train time,\\nwe choose a point on this curve, either at a given\\nnumber of proposals Ncor at the achievable quality\\nwe are interested in (black triangle) and store the parameters\\x08\\nN1;:::;NR\\t\\n. At test time, we combine the\\x08',\n",
       " 'we are interested in (black triangle) and store the parameters\\x08\\nN1;:::;NR\\t\\n. At test time, we combine the\\x08\\nN1;:::;NR\\t\\ntop proposals from each ranked list. The\\nnumber of sampled conﬁgurations using the proposed\\nalgorithm is (R\\x001)S2, that is, we have reduced an\\nexponential problem ( SR) to a quadratic one.\\nRegressed Ranking of Proposals : To further reduce\\nthe number of proposals, we train a regressor from\\nlow-level features, as in [18]. Since the proposals are\\nall formed by a set of regions from a reduced set of\\nhierarchies, we focus on features that can be computed\\nefﬁciently in a bottom-up fashion, as explained previously.\\nWe compute the following features:\\n\\x0fSize and location : Area and perimeter of the candidate; area, position, and aspect ratio of the bounding\\nbox; and the area balance between the regions in thecandidate.\\n\\x0fShape : Perimeter (and sum of contour strength)\\ndivided by the squared root of the area; and area of the\\nregion divided by that of the bounding box.\\n\\x0fContours : Sum of contour strength at the boundaries, mean contour strength at the boundaries; minimum and maximum UCM threshold of appearance and',\n",
       " '\\x0fContours : Sum of contour strength at the boundaries, mean contour strength at the boundaries; minimum and maximum UCM threshold of appearance and\\ndisappearance of the regions forming the candidate.\\nWe train a Random Forest using these features to regress\\nthe object overlap with the ground truth, and diversify\\nthe ranking based on Maximum Marginal Relevance\\nmeasures [18]. We tune the random forest learning on\\nhalf training set and validating on the other half. For\\nthe ﬁnal results, we train on the training set and evaluate\\nour proposals on the validation set of PASCAL 2012.\\n6 E XPERIMENTS ON PASCAL VOC, SBD,\\nAND COCO\\nThis section presents our large-scale empirical validation\\nof the object proposal algorithm described in the previous section. We perform experiments in three annotated\\ndatabases, with a variety of measures that demonstrate\\nthe state-of-the-art performance of our algorithm.\\nDatasets and Evaluation Measures : We conduct experiments in the following three annotated datasets: the\\nsegmentation challenge of PASCAL 2012 Visual Object\\nClasses (SegVOC12) [41], the Berkeley Semantic Boundaries Dataset (SBD) [42], and the Microsoft Common\\nObjects in Context (COCO) [43]. They all consist of\\nimages with annotated objects of different categories.',\n",
       " 'Objects in Context (COCO) [43]. They all consist of\\nimages with annotated objects of different categories.\\nTable 2 summarizes the number of images and object\\ninstances in each database.\\nNumber of Number of Number of\\nClasses Images Objects\\nSegVOC12 20 2 913 9 847\\nSBD 20 12 031 32 172\\nCOCO 80 123 287 910 983\\nTABLE 2\\nSizes of the databases\\nRegarding the performance metrics, we measure the\\nachievable quality with respect to the number of proposals, that is, the quality we would have if an oracle\\nselected the best proposal among the pool. This aligns\\nwith the fact that object proposals are a preprocessing\\nstep for other algorithms that will represent and classify\\nthem. We want, therefore, the achievable quality within\\nthe proposals to be as high as possible, while reducing\\nthe number of proposals to make the ﬁnal system as fast\\nas possible.\\nAs a measure of quality of a speciﬁc proposal with\\nrespect to an annotated object, we consider the Jaccard\\nindexJ, also known as overlap or intersection over\\nunion; which is deﬁned as the size of the intersection\\nof the two pixel sets over the size of their union.\\n9\\nTo compute the overall quality for the whole database,',\n",
       " 'of the two pixel sets over the size of their union.\\n9\\nTo compute the overall quality for the whole database,\\nwe ﬁrst select the best proposal for each annotated\\ninstance with respect to J. The Jaccard index at instance\\nlevel (Ji) is then deﬁned as the mean best overlap for all\\nthe ground-truth instances in the database, also known\\nas Best Spatial Support score (BSS) [4] or Average Best\\nOverlap (ABO) [12].\\nComputing the mean of the best overlap on all objects,\\nas done by Ji, hides the distribution of quality among\\ndifferent objects. As an example, Ji= 0:5can mean\\nthat the algorithm covers half the objects perfectly and\\ncompletely misses the other half, or can also mean that\\nall the objects are covered exactly at J= 0:5. This\\ninformation might be useful to decide which algorithm\\nto use. Computing a histogram of the best overlap would\\nprovide very rich information, but then the resulting\\nplot would be 3D (number of proposals, bins, and bin\\ncounts). Alternatively, we propose to plot different percentiles of the histogram.\\nInterestingly, a certain percentile of the histogram\\nof best overlaps consists in computing the number of\\nobjects whose best overlap is above a certain Jaccard',\n",
       " 'Interestingly, a certain percentile of the histogram\\nof best overlaps consists in computing the number of\\nobjects whose best overlap is above a certain Jaccard\\nthreshold, which can be interpreted as the best achievable recall of the technique over a certain threshold. We\\ncompute the recall at three different thresholds: J= 0:5,\\nJ=0:7, andJ=0:85.\\nLearning Strategy Evaluation : We ﬁrst estimate the\\nloss in performance due to not sweeping all the possible\\nvalues offN1;:::;NRgin the combination of proposal\\nlists via the proposed greedy strategy. To do so, we will\\ncompare this strategy with the full combination on a\\nreduced problem to make the latter feasible. Speciﬁcally,\\nwe combine the 4 ranked lists coming from the singletons at all scales, instead of the full 16 lists coming from\\nsingletons, pairs, triplets, and 4-tuples. We also limit the\\nsearch to 20 000 proposals, further speeding the process\\nup.\\nIn this situation, the mean loss in achievable quality\\nalong the full curve of parameterization is Ji= 0:0002 ,\\nwith a maximum loss of Ji=0:004(0:74%). In exchange,\\nour proposed learning strategy on the full 16 ranked lists\\ntakes about 20 seconds to compute on the training set of',\n",
       " 'our proposed learning strategy on the full 16 ranked lists\\ntakes about 20 seconds to compute on the training set of\\nSegVOC12, while the singleton-limited full combination\\ntakes 4 days (the full combination would take months).\\nCombinatorial Grouping : We now evaluate the\\nPareto front optimization strategy in the training set of\\nSegVOC12. As before, we extract the lists of proposals\\nfrom the three scales and the multiscale hierarchy, for\\nsingletons, pairs, triplets, and 4-tuples of regions, leading\\nto 16 lists, ranked by the minimum UCM strength of the\\nregions forming each proposal.\\nFigure 8 shows the Pareto front evolution of Jiwith\\nrespect to the number of proposals for up to 1, 2, 3, and\\n4 regions per proposal (4, 8, 12, and 16 lists, respectively)\\nat training and testing time on SegVOC12. As baselines,\\nwe plot the raw singletons from MCG-UCM-Our, gPbUCM, and Quadtree; as well as the uniform combination\\nof scales.103104105020406080100\\nNumber of proposalsRegion distribution percentageScale 2.0\\nScale 1.0\\nScale 0.5\\nMulti-Scale\\nFig. 10. Region distribution learnt by the Pareto front\\noptimization on SegVOC12.',\n",
       " 'Scale 1.0\\nScale 0.5\\nMulti-Scale\\nFig. 10. Region distribution learnt by the Pareto front\\noptimization on SegVOC12.\\nThe improvement of considering the combination of\\nall 1-region proposals ( ) from the 3 scales and the\\nMCG-UCM-Our with respect to the raw MCG-UCM-Our\\n( ) is signiﬁcant, which corroborates the gain in diversity obtained from hierarchies at different scales. In turn,\\nthe addition of 2- and 3-region proposals ( and )\\nnoticeably improves the achievable quality. This shows\\nthat hierarchies do not get full objects in single regions,\\nwhich makes sense given that they are built using lowlevel features only. The improvement when adding 4tuples ( ) is marginal at the number of proposals we\\nare considering. When analyzing the equal distribution\\nof proposals from the four scales ( ), we see that the\\nless proposals we consider, the more relevant the Pareto\\noptimization becomes. At the selected working point, the\\ngain of the Pareto optimization is 2 points.\\nFigure 10 shows the distribution of proposals from\\neach of the scales combined in the Pareto front. We\\nsee that the coarse scale (0.5) is the most picked at\\nlow number of proposals, and the rest come into play\\nwhen increasing their number, since one can afford more',\n",
       " 'see that the coarse scale (0.5) is the most picked at\\nlow number of proposals, and the rest come into play\\nwhen increasing their number, since one can afford more\\ndetailed proposals. The multi-scale hierarchy is the one\\nwith less weight, since it is created from the other three.\\nPareto selection and ranking : Back to Figure 8,\\nthe red asterisk ( ) marks the selected conﬁguration\\x08\\nN1;:::;NR\\t\\nin the Pareto front (black triangle in Figure 7), which is selected at a practical level of proposals.\\nThe red plus sign ( ) represents the set of proposals after\\nremoving those duplicate proposals whose overlap leads\\nto a Jaccard higher than 0.95. The proposals at this point\\nare the ones that are ranked by the learnt regressor ( ).\\nAt test time (right-hand plot), we directly combine the\\nlearnt\\x08\\nN1;:::;NR\\t\\nproposals from each ranked list.\\nNote that the Pareto optimization does not overﬁt, given\\nthe similar result in the training and validation datasets.\\nWe then remove duplicates and rank the results. In this\\ncase, note the difference between the regressed result in',\n",
       " 'the similar result in the training and validation datasets.\\nWe then remove duplicates and rank the results. In this\\ncase, note the difference between the regressed result in\\nthe training and validation sets, which reﬂects overﬁtting, but despite this we found it beneﬁcial with respect\\nto the non-regressed result.\\n10\\nTraining\\n1021031041051060:40:50:60:70:80:9\\nNumber of proposalsJaccard index at instance level ( Ji)Validation\\n1021031041051060:40:50:60:70:80:9\\nNumber of proposalsJaccard index at instance level ( Ji)Pareto up to 4-tuples\\nPareto up to triplets\\nPareto up to pairs\\nPareto only singletons\\nRaw Ours-multi singl.\\nRaw gPb-UCM singl.\\nRaw Quadtree singl.\\nEqual distribution\\nSelected conﬁguration\\nFiltered candidates\\nRegressed ranking\\nFig. 8. Pareto front evaluation. Achievable quality of our proposals for singletons, pairs, triplets, and 4-tuples; and\\nthe raw proposals from the hierarchies on PASCAL SegVOC12 training (left) and validation (right) sets.\\nPASCAL SegVOC12',\n",
       " 'the raw proposals from the hierarchies on PASCAL SegVOC12 training (left) and validation (right) sets.\\nPASCAL SegVOC12\\n1021031040:30:40:50:60:70:80:9\\nNumber of proposalsJaccard index at instance level ( Ji)SBD\\n1021031040:30:40:50:60:70:80:9\\nNumber of proposalsJaccard index at instance level ( Ji)COCO\\n1021031040:30:40:50:60:70:80:9\\nNumber of proposalsJaccard index at instance level ( Ji)MCG-Our\\nSCG-Our\\nCPMC [18]\\nCI [19]\\nGOP [13]\\nGLS [14]\\nRIGOR [15]\\nShSh [17]\\nSeSe [12]\\nMCG-UCM-Our\\ngPb-UCM\\nQuadtree\\nFig. 9. Object Proposals: Jaccard index at instance level. Results on SegVOC12, SBD, and COCO.\\nIn the validation set of SegVOC12, the full set of\\nproposals (i.e., combining the full 16 lists) would contain\\nmillions of proposals per image. The multiscale combinatorial grouping allows us to reduce the number of',\n",
       " 'proposals (i.e., combining the full 16 lists) would contain\\nmillions of proposals per image. The multiscale combinatorial grouping allows us to reduce the number of\\nproposals to 5 086 with a very high achievable Jiof0:81\\n(). The regressed ranking ( ) allows us to further\\nreduce the number of proposals below this point.\\nSegmented Proposals: Comparison with State of\\nthe Art : We ﬁrst compare our results against those\\nmethods that produce segmented object proposals [13],\\n[14], [15], [12], [16], [17], [18], [19], using the implementations from the respective authors. We train MCG on\\nthe training set of SegVOC12, and we use the learnt\\nparameters on the validation sets of SegVOC12, SBD,\\nand COCO.\\nFigure 9 shows the achievable quality at instance level\\n(Ji) of all methods on the validation set of SegVOC12,\\nSBD, and COCO. We plot the raw regions of MCGUCM-Our, gPb-UCM, and QuadTree as baselines where\\navailable. We also evaluate a faster single-scale version of\\nMCG ( Single-scale Combinatorial Grouping - SCG ), which\\ntakes the hierarchy at the native scale only and combines',\n",
       " 'MCG ( Single-scale Combinatorial Grouping - SCG ), which\\ntakes the hierarchy at the native scale only and combines\\nup to 4 regions per proposal. This approach decreases\\nthe computational load one order of magnitude whilekeeping competitive results.\\nMCG proposals ( ) signiﬁcantly outperform the\\nstate-of-the-art at all regimes. The bigger the database is,\\nthe better MCG results are with respect to the rest, which\\nshows that our techniques better generalize to unseen\\nimages (recall that MCG is trained only in SegVOC12).\\nAs commented on the measures description, Jishows\\nmean aggregate results, so they can mask the distribution of quality among objects in the database. Figure 11\\nshows the recall at three different Jaccard levels. First,\\nthese plots further highlight how challenging COCO\\nis, since we observe a signiﬁcant drop in performance,\\nmore pronounced than when measured by JiandJc.\\nAnother interesting result comes from observing the\\nevolution of the plots for the three different Jaccard\\nvalues. Take for instance the performance of GOP ( )\\nagainst MCG-Our ( ) in SBD. While for J=0:5GOP\\nslightly outperforms MCG, the higher the threshold, the\\nbetter MCG. Overall, MCG has specially good results',\n",
       " 'slightly outperforms MCG, the higher the threshold, the\\nbetter MCG. Overall, MCG has specially good results\\nat higherJvalues. In other words, if one looks for\\nproposals of very high accuracy, MCG is the method\\nwith highest recall, at all regimes and in all databases.\\nIn all measures and databases, SCG ( ) obtains very\\ncompetitive results, especially if we take into account\\n11\\nPASCAL SegVOC12\\n10210310400:10:20:30:40:50:60:70:80:9\\nJ=0:5\\nJ=0:7\\nJ=0:85\\nNumber of proposalsRecallSBD\\n10210310400:10:20:30:40:50:60:70:80:9\\nJ= 0:5\\nJ= 0:7\\nJ= 0:85\\nNumber of proposalsRecallCOCO\\n10210310400:10:20:30:40:50:60:70:80:9\\nJ= 0:5\\nJ= 0:7\\nJ= 0:85\\nNumber of proposalsRecallMCG-Our\\nSCG-Our\\nCPMC [18]\\nCI [19]\\nGOP [13]\\nGLS [14]\\nRIGOR [15]\\nShSh [17]\\nSeSe [12]',\n",
       " 'CI [19]\\nGOP [13]\\nGLS [14]\\nRIGOR [15]\\nShSh [17]\\nSeSe [12]\\nQuadtree\\nFig. 11. Segmented Object Proposals: Recall at different Jaccard levels. Percentage of annotated objects for\\nwhich there is a proposal whose overlap with the segmented ground-truth shapes (not boxes) is above J= 0:5,\\nJ= 0:7, andJ= 0:85, for different number of proposals per image. Results on SegVOC12, SBD, and COCO.\\nPASCAL SegVOC12\\n10210310400:10:20:30:40:50:60:70:80:9\\nJ= 0:5\\nJ= 0:7\\nJ= 0:85\\nNumber of proposalsRecallSBD\\n10210310400:10:20:30:40:50:60:70:80:9\\nJ= 0:5\\nJ= 0:7\\nJ= 0:85\\nNumber of proposalsRecallCOCO\\n10210310400:10:20:30:40:50:60:70:80:9\\nJ= 0:5\\nJ= 0:7\\nJ= 0:85\\nNumber of proposalsRecallMCG-Our\\nSCG-Our\\nCPMC [18]',\n",
       " 'J= 0:5\\nJ= 0:7\\nJ= 0:85\\nNumber of proposalsRecallMCG-Our\\nSCG-Our\\nCPMC [18]\\nCI [19]\\nGOP [13]\\nGLS [14]\\nRIGOR [15]\\nShSh [17]\\nSeSe [12]\\nRP [25]\\nEB [26]\\nBING [27]\\nObj [16]\\nQuadtree\\nFig. 12. Bounding-Box Proposals: Recall at different Jaccard levels. Percentage of annotated objects for which\\nthere is a bounding box proposal whose overlap with the ground-truth boxes is above J= 0:5,J= 0:7, andJ= 0:85,\\nfor different number of proposals per image. Results on SegVOC12, SBD, and COCO.\\nthat it is 7\\x02faster than MCG, as we will see in next\\nsections.\\nThe complementarity of MCG with respect to other\\nproposal techniques, and their combination using the\\nPareto front optimization is studied in [44].\\nBoxes Proposals: Comparison with State of the\\nArt: Although focused in providing segmented object\\nproposals, MCG may also be used to provide boxes\\nproposals, by taking the bounding box of the segmented',\n",
       " 'Art: Although focused in providing segmented object\\nproposals, MCG may also be used to provide boxes\\nproposals, by taking the bounding box of the segmented\\nproposals and deduplicating them. We add the state\\nof the art in boxes proposals [26], [27], [25], and [16]\\nto the comparison. Figure 12 shows the recall values\\nof the boxes results when compared to the bounding\\nboxes of the annotated objects, for three different Jaccard\\nthresholds.\\nWhile many of the techniques speciﬁcally tailored to\\nboxes proposals are competitive at J= 0:5, their perfor-mance drops signiﬁcantly at higher Jaccard thresholds.\\nDespite being tailored to segmented proposals, MCG\\nclearly outperforms the state of the art if you look\\nfor precise localization of the bounding boxes. Again,\\nSCG is very competitive, especially taking its speed into\\naccount.\\nMCG and SCG Time per Image : Table 3 shows\\nthe time per image of the main steps of our approach,\\nfrom the raw image to the contours, the hierarchical\\nsegmentation, and the proposal generation. All times are\\ncomputed using a single core on a Linux machine. Our\\nfull MCG takes about 25 s. per image to compute the',\n",
       " 'segmentation, and the proposal generation. All times are\\ncomputed using a single core on a Linux machine. Our\\nfull MCG takes about 25 s. per image to compute the\\nmultiscale hierarchies, and 17 s. to generate and rank\\nthe5 038 proposals on the validation set of SegVOC12.\\nOur single-scale SCG produces a segmentation hierarchy\\nof better quality than gPb-ucm [20] in less than 3 seconds\\nand with signiﬁcant less memory footprint.\\n12\\nFig. 13. COCO Qualitative Results : Image, ground truth, multi-scale UCM and best MCG proposals among the 500\\nbest ranked. (More qualitative examples in the supplemental material.)\\nContour Hierarchical CandidateTotalDetection Segmentation Generation\\nMCG 4.6\\x061.3 20.5\\x065.6 17.0\\x069.8 42.2\\x0614.8\\nSCG 1.0\\x060.3 2.7\\x060.7 2.6\\x060.4 6.2\\x061.1\\nTABLE 3\\nTime in seconds per image of MCG and SCG\\nTable 4 shows the time-per-image results compared to\\nthe rest of state of the art in segmented proposals generation, all run on the same single-core Linux machine.\\nProposal\\nGeneration',\n",
       " 'the rest of state of the art in segmented proposals generation, all run on the same single-core Linux machine.\\nProposal\\nGeneration\\nMCG (Our) 42.2\\x0614.8\\nSCG (Our) 6.2\\x061.1\\nGOP [13] 1.0\\x060.3\\nGLS [14] 7.9\\x061.7\\nSeSe [12] 15.9\\x065.2\\nRIGOR [15] 31.6\\x0616.0\\nCPMC [18] \\x15120\\nCI [19] \\x15120\\nShSh [17] \\x15120\\nTABLE 4\\nTime comparison for all considered state-of-the-art\\ntechniques that produce segmented proposals. All run\\non the same single-core Linux machine.Practical considerations : One of the key aspects of\\nobject proposals is the size of the pool they generate.\\nDepending on the application, one may need more precision and thus a bigger pool, or one might need speed and\\nthus a small pool in exchange for some loss of quality.\\nMCG and SCG provide a ranked set of around 5 000 and\\n2 000 proposals, respectively, and one can take the N\\nﬁrst in case the speciﬁc application needs a smaller pool.\\nFrom a practical point of view, this means that one does',\n",
       " 'ﬁrst in case the speciﬁc application needs a smaller pool.\\nFrom a practical point of view, this means that one does\\nnot need to re-parameterize them for the speciﬁc needs\\nof a certain application.\\nIn contrast, the techniques that do not provide a\\nranking of the proposals, need to be re-parameterized\\nto adapt them to a different number of proposals, which\\nis not desirable in practice.\\nOn top of that, the results show that MCG and SCG\\nhave outstanding generalization power to unseen images\\n(recall that the results for SBD and COCO have been\\nobtained using the learnt parameters on SegVOC12),\\nmeaning that MCG and SCG offer the best chance to\\nobtain competitive results in an unseen database without\\nneed to re-train.\\nFigure 13 shows some qualitative results on COCO.\\n7 C ONCLUSIONS\\nWe proposed Multiscale Combinatorial Grouping\\n(MCG), a uniﬁed approach for bottom-up segmentation\\nand object proposal generation. Our approach produces\\n13\\nstate-of-the-art contours, hierarchical regions, and object\\nproposals. At its core are a fast eigenvector computation\\nfor normalized-cut segmentation and an efﬁcient\\nalgorithm for combinatorial merging of hierarchical',\n",
       " 'proposals. At its core are a fast eigenvector computation\\nfor normalized-cut segmentation and an efﬁcient\\nalgorithm for combinatorial merging of hierarchical\\nregions. We also present Single-scale Combinatorial\\nGrouping (SCG), a speeded up version of our technique\\nthat produces competitive results in under ﬁve seconds\\nper image.\\nWe perform an extensive validation in BSDS500,\\nSegVOC12, SBD, and COCO, showing the quality, robustness and scalability of MCG. Recently, an independent study [45], [46] provided further evidence to the\\ninterest of MCG among the current state-of-the-art in\\nobject proposal generation. Moreover, our object candidates have already been employed as integral part of\\nhigh performing recognition systems [47].\\nIn order to promote reproducible research on perceptual grouping, all the resources of this project – code,\\npre-computed results, and evaluation protocols – are\\npublicly available2.\\nAcknowledgements : The last iterations of this work\\nhave been done while Jordi Pont-Tuset has been at Prof.\\nLuc Van Gool’s Computer Vision Lab (CVL) of ETHZ,\\nSwitzerland. This work has been partly developed in the\\nframework of the project BIGGRAPH-TEC2013-43935-R',\n",
       " 'Switzerland. This work has been partly developed in the\\nframework of the project BIGGRAPH-TEC2013-43935-R\\nand the FPU grant AP2008-01164; ﬁnanced by the Spanish Ministerio de Econom´ ıa y Competitividad , and the European Regional Development Fund (ERDF). This work\\nwas partially supported by ONR MURI N000141010933.\\nREFERENCES\\n[1] P . Viola and M. Jones, “Robust real-time face detection,” IJCV ,\\nvol. 57, no. 2, 2004.\\n[2] N. Dalal and B. Triggs, “Histograms of oriented gradients for\\nhuman detection,” in CVPR , 2005.\\n[3] P . Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan,\\n“Object detection with discriminatively trained part based models,” TP AMI , vol. 32, no. 9, 2010.\\n[4] T. Malisiewicz and A. A. Efros, “Improving spatial support for\\nobjects via multiple segmentations,” in BMVC , 2007.\\n[5] C. Gu, J. Lim, P . Arbelaez, and J. Malik, “Recognition using',\n",
       " '[5] C. Gu, J. Lim, P . Arbelaez, and J. Malik, “Recognition using\\nregions,” in CVPR , 2009.\\n[6] J. Carreira, F. Li, and C. Sminchisescu, “Object recognition by\\nsequential ﬁgure-ground ranking,” IJCV , vol. 98, no. 3, pp. 243–\\n262, 2012.\\n[7] A. Ion, J. Carreira, and C. Sminchisescu, “Probabilistic joint image\\nsegmentation and labeling by ﬁgure-ground composition,” IJCV ,\\nvol. 107, no. 1, pp. 40–57, 2014.\\n[8] P . Arbel ´aez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and\\nJ. Malik, “Semantic segmentation using regions and parts,” in\\nCVPR , 2012.\\n[9] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu, “Semantic\\nsegmentation with second-order pooling,” in ECCV , 2012.',\n",
       " 'segmentation with second-order pooling,” in ECCV , 2012.\\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\\nhierarchies for accurate object detection and semantic segmentation,” in CVPR , 2014.\\n[11] N. Zhang, J. Donahue, R. Girshick, and T. Darrell, “Part-based\\nr-cnns for ﬁne-grained category detection,” in ECCV , 2014.\\n[12] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M.\\nSmeulders, “Selective search for object recognition,” IJCV , vol.\\n104, no. 2, pp. 154–171, 2013.\\n2.www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/[13] P . Kr ¨ahenb ¨uhl and V . Koltun, “Geodesic object proposals,” in\\nECCV , 2014.\\n[14] P . Rantalankila, J. Kannala, and E. Rahtu, “Generating object',\n",
       " 'ECCV , 2014.\\n[14] P . Rantalankila, J. Kannala, and E. Rahtu, “Generating object\\nsegmentation proposals using global and local search,” in CVPR ,\\n2014.\\n[15] A. Humayun, F. Li, and J. M. Rehg, “RIGOR: Recycling Inference\\nin Graph Cuts for generating Object Regions,” in CVPR , 2014.\\n[16] B. Alexe, T. Deselaers, and V . Ferrari, “Measuring the objectness\\nof image windows,” TP AMI , vol. 34, pp. 2189–2202, 2012.\\n[17] J. Kim and K. Grauman, “Shape sharing for object segmentation,”\\ninECCV , 2012.\\n[18] J. Carreira and C. Sminchisescu, “CPMC: Automatic object\\nsegmentation using constrained parametric min-cuts,” TP AMI ,\\nvol. 34, no. 7, pp. 1312–1328, 2012.\\n[19] I. Endres and D. Hoiem, “Category-independent object proposals\\nwith diverse ranking,” TP AMI , vol. 36, no. 2, pp. 222–234, 2014.',\n",
       " 'with diverse ranking,” TP AMI , vol. 36, no. 2, pp. 222–234, 2014.\\n[20] P . Arbel ´aez, M. Maire, C. C. Fowlkes, and J. Malik, “Contour\\ndetection and hierarchical image segmentation,” TP AMI , vol. 33,\\nno. 5, pp. 898–916, 2011.\\n[21] P . F. Felzenszwalb and D. P . Huttenlocher, “Efﬁcient graph-based\\nimage segmentation,” IJCV , vol. 59, p. 2004, 2004.\\n[22] X. Ren and L. Bo, “Discriminatively trained sparse code gradients\\nfor contour detection,” in NIPS , 2012.\\n[23] C. J. Taylor, “Towards fast and accurate segmentation,” CVPR ,\\n2013.\\n[24] M. Maire and S. X. Yu, “Progressive multigrid eigensolvers for\\nmultiscale spectral segmentation,” ICCV , 2013.',\n",
       " 'multiscale spectral segmentation,” ICCV , 2013.\\n[25] S. Man ´en, M. Guillaumin, and L. Van Gool, “Prime Object Proposals with Randomized Prim’s Algorithm,” in ICCV , 2013.\\n[26] C. L. Zitnick and P . Doll ´ar, “Edge boxes: Locating object proposals\\nfrom edges,” in ECCV , 2014.\\n[27] M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P . H. S. Torr, “BING:\\nBinarized normed gradients for objectness estimation at 300fps,”\\ninCVPR , 2014.\\n[28] D. Hoiem, A. Efros, and M. Hebert, “Recovering occlusion boundaries from an image,” IJCV , vol. 91, no. 3, pp. 328–346, 2011.\\n[29] Z. Ren and G. Shakhnarovich, “Image segmentation by cascaded\\nregion agglomeration,” in CVPR , 2013.\\n[30] D. Weiss and B. Taskar, “Scalpel: Segmentation cascades with',\n",
       " 'region agglomeration,” in CVPR , 2013.\\n[30] D. Weiss and B. Taskar, “Scalpel: Segmentation cascades with\\nlocalized priors and efﬁcient learning,” in CVPR , 2013.\\n[31] L. Najman and M. Schmitt, “Geodesic saliency of watershed\\ncontours and hierarchical segmentation,” TP AMI , vol. 18, no. 12,\\npp. 1163–1173, 1996.\\n[32] P . Arbel ´aez, “Boundary extraction in natural images using ultrametric contour maps,” in POCV , June 2006.\\n[33] D. Martin, C. Fowlkes, and J. Malik, “Learning to detect natural\\nimage boundaries using local brightness, color and texture cues,”\\nTP AMI , vol. 26, no. 5, pp. 530–549, 2004.\\n[34] P . Doll ´ar and C. Zitnick, “Structured forests for fast edge detection,” ICCV , 2013.\\n[35] http://www.eecs.berkeley.edu/Research/\\nProjects/CS/vision/grouping/resources.html.',\n",
       " '[35] http://www.eecs.berkeley.edu/Research/\\nProjects/CS/vision/grouping/resources.html.\\n[36] J. Pont-Tuset and F. Marques, “Supervised evaluation of image\\nsegmentation and object proposal techniques,” P AMI , 2015.\\n[37] ——, “Measures and meta-measures for the supervised evaluation\\nof image segmentation,” in CVPR , 2013.\\n[38] M. Maire, S. X. Yu, and P . Perona, “Hierarchical scene annotation,”\\ninBMVC , 2013.\\n[39] M. Everingham, H. Muller, and B. Thomas, “Evaluating image\\nsegmentation algorithms using the pareto front,” in ECCV , 2006.\\n[40] M. Ehrgott, Multicriteria optimization . Springer, 2005.\\n[41] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman, “The PASCAL Visual Object Classes\\nChallenge 2012 (VOC2012) Results,” http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.',\n",
       " 'Challenge 2012 (VOC2012) Results,” http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.\\n[42] B. Hariharan, P . Arbelaez, L. Bourdev, S. Maji, and J. Malik,\\n“Semantic contours from inverse detectors,” in ICCV , 2011.\\n[43] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\\nP . Doll ´ar, and C. Zitnick, “Microsoft COCO: Common Objects in\\nContext,” in ECCV , 2014.\\n[44] J. Pont-Tuset and L. V . Gool, “Boosting object proposals: From\\npascal to COCO,” in ICCV , 2015.\\n[45] J. Hosang, R. Benenson, and B. Schiele, “How good are detection\\nproposals, really?” in BMVC , 2014.\\n[46] J. Hosang, R. Benenson, P . Doll ´ar, and B. Schiele, “What makes\\nfor effective detection proposals?” P AMI , 2015.\\n14',\n",
       " 'for effective detection proposals?” P AMI , 2015.\\n14\\n[47] B. Hariharan, P . Arbel ´aez, R. Girshick, and J. Malik, “Simultaneous detection and segmentation,” in ECCV , 2014.\\nJordi Pont-Tuset is a post-doctoral researcher\\nat ETHZ, Switzerland, in Prof. Luc Van Gool’s\\ncomputer vision group (2015). He received the\\ndegree in Mathematics in 2008, the degree in\\nElectrical Engineering in 2008, the M.Sc. in\\nResearch on Information and Communication\\nTechnologies in 2010, and the Ph.D with honors\\nin 2014; all from the Universitat Polit `ecnica de\\nCatalunya, BarcelonaTech (UPC). He worked at\\nDisney Research, Z ¨urich (2014).\\nPablo Arbel ´aezreceived a PhD with honors in\\nApplied Mathematics from the Universit ´e ParisDauphine in 2005. He was a Research Scientist\\nwith the Computer Vision Group at UC Berkeley from 2007 to 2014. He currently holds a\\nfaculty position at Universidad de los Andes in\\nColombia. His research interests are in computer vision, where he has worked on a number\\nof problems, including perceptual grouping, object recognition and the analysis of biomedical\\nimages.',\n",
       " 'Colombia. His research interests are in computer vision, where he has worked on a number\\nof problems, including perceptual grouping, object recognition and the analysis of biomedical\\nimages.\\nJonathan T. Barron is a senior research scientist at Google, working on computer vision\\nand computational photography. He received a\\nPhD in Computer Science from the University\\nof California, Berkeley in 2013, where he was\\nadvised by Jitendra Malik, and he received a\\nHonours BSc in Computer Science from the\\nUniversity of Toronto in 2007. His research interests include computer vision, machine learning,\\ncomputational photography, shape reconstruction, and biological image analysis. He received\\na National Science Foundation Graduate Research Fellowship in 2009,\\nand the C.V. Ramamoorthy Distinguished Research Award in 2013.\\nFerran Marques received the degree in Electrical Engineering and the Ph.D. from the Universitat Polit `ecnica de Catalunya, BarcelonaT ech (UPC), where he is currently Professor at\\nthe department of Signal Theory and Communications. In the term 2002-2004, he served\\nas President of the European Association for\\nSignal Processing (EURASIP). He has served\\nas Associate Editor of the IEEE Transactions\\non Image Processing (2009-2012) and as Area\\nEditor for Signal Processing: Image Communication, Elsevier (2010-2014). In 2011, he received the Jaume Vicens',\n",
       " 'on Image Processing (2009-2012) and as Area\\nEditor for Signal Processing: Image Communication, Elsevier (2010-2014). In 2011, he received the Jaume Vicens\\nVives distinction for University Teaching Quality. Currently, he serves\\nas Dean of the Electrical Engineering School (ETSETB-TelecomBCN)\\nat UPC. He has published over 150 conference and journal papers, 2\\nbooks, and holds 4 international patents.\\nJitendra Malik is Arthur J. Chick Professor in\\nthe Department of Electrical Engineering and\\nComputer Science at the University of California\\nat Berkeley, where he also holds appointments\\nin vision science and cognitive science. He received the PhD degree in Computer Science\\nfrom Stanford University in 1985. In January\\n1986, he joined UC Berkeley as a faculty member in the EECS department where he served as\\nChair of the Computer Science Division during\\n2002-2006, and of the Department of EECS during 2004-2006. Jitendra Malik’s group has worked on computer vision,\\ncomputational modeling of biological vision, computer graphics and\\nmachine learning. Several well-known concepts and algorithms arose in\\nthis work, such as anisotropic diffusion, normalized cuts, high dynamic\\nrange imaging, shape contexts and poselets. According to Google\\nScholar, ten of his papers have received more than a thousand citations\\neach. He has graduated 33 PhD students. Jitendra was awarded the',\n",
       " 'Scholar, ten of his papers have received more than a thousand citations\\neach. He has graduated 33 PhD students. Jitendra was awarded the\\nLonguet-Higgins Award for “A Contribution that has Stood the Test of\\nTime” twice, in 2007 and 2008. He is a Fellow of the IEEE and the\\nACM, a member of the National Academy of Engineering, and a fellow\\nof the American Academy of Arts and Sciences. He received the PAMI\\nDistinguished Researcher Award in computer vision in 2013 and the\\nK.S. Fu prize in 2014.',\n",
       " 'High-for-Low and Low-for-High:\\nEfﬁcient Boundary Detection from Deep Object Features\\nand its Applications to High-Level Vision\\nGedas Bertasius\\nUniversity of Pennsylvania\\ngberta@seas.upenn.eduJianbo Shi\\nUniversity of Pennsylvania\\njshi@seas.upenn.eduLorenzo Torresani\\nDartmouth College\\nlt@dartmouth.edu\\nAbstract\\nMost of the current boundary detection systems rely exclusively on low-level features, such as color and texture.\\nHowever, perception studies suggest that humans employ\\nobject-level reasoning when judging if a particular pixel\\nis a boundary. Inspired by this observation, in this work\\nwe show how to predict boundaries by exploiting objectlevel features from a pretrained object-classiﬁcation network. Our method can be viewed as a High-for-Low approach where high-level object features inform the low-level\\nboundary detection process. Our model achieves state-ofthe-art performance on an established boundary detection\\nbenchmark and it is efﬁcient to run.\\nAdditionally, we show that due to the semantic nature of',\n",
       " 'benchmark and it is efﬁcient to run.\\nAdditionally, we show that due to the semantic nature of\\nour boundaries we can use them to aid a number of highlevel vision tasks. We demonstrate that by using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can\\nview this process as a Low-for-High scheme, where lowlevel boundaries aid high-level vision tasks.\\nThus, our contributions include a boundary detection\\nsystem that is accurate, efﬁcient, generalizes well to multiple datasets, and is also shown to improve existing stateof-the-art high-level vision methods on three distinct tasks.\\n1. Introduction\\nIn the vision community, boundary detection has always\\nbeen considered a low-level problem. However, psychological studies suggest that when a human observer perceives\\nboundaries, object level reasoning is used [13, 25, 17]. Despite these ﬁndings, most of the boundary detection methods rely exclusively on low-level color and gradient features. In this work, we present a method that uses objectlevel features to detect boundaries. We argue that using\\nobject-level information to predict boundaries is more sim-Low-Level Task High-Level Tasks\\nBD SBL SS OP\\nODS MF AP PI-IOU MR',\n",
       " 'object-level information to predict boundaries is more sim-Low-Level Task High-Level Tasks\\nBD SBL SS OP\\nODS MF AP PI-IOU MR\\nSotA 0.76 [27] 28.0 [11] 19.9 [11] 45.8 [5] 0.88 [31]\\nHFL 0.77 62.5 54.6 48.8 0.90\\nTable 1: Summary of results achieved by our proposed\\nmethod (H FL) and state-of-the-art methods (SotA). We provide results on four vision tasks: Boundary Detection (BD),\\nSemantic Boundary Labeling (SBL), Semantic Segmentation (SS), and Object Proposal (OP). The evaluation metrics\\ninclude ODS F-score for BD task, max F-score (MF) and\\naverage precision (AP) for SBL task, per image intersection over union (PI-IOU) for SS task, and max recall (MR)\\nfor OP task. Our method produces better results in each of\\nthese tasks according to these metrics.\\nilar to how humans reason. Our boundary detection scheme\\ncan be viewed as a High-for-Low approach where we use\\nhigh-level object features as cues for a low-level boundary\\ndetection process. Throughout the rest of the paper, we refer to our proposed boundaries as High-for-Low boundaries',\n",
       " 'high-level object features as cues for a low-level boundary\\ndetection process. Throughout the rest of the paper, we refer to our proposed boundaries as High-for-Low boundaries\\n(HFL).\\nWe present an efﬁcient deep network that uses objectlevel information to predict the boundaries. Our proposed\\narchitecture reuses features from the sixteen convolutional\\nlayers of the network of Simonyan et al. [29], which we\\nrefer to as VGG net. The VGG net has been trained for\\nobject classiﬁcation, and therefore, reusing its features allows our method to utilize high-level object information to\\npredict H FL boundaries. In the experimental section, we\\ndemonstrate that using object-level features produces semantically meaningful boundaries and also achieves above\\nstate-of-the-art boundary detection accuracy.\\nAdditionally, we demonstrate that we can successfully\\napply our H FL boundaries to a number of high-level vision\\ntasks. We show that by using H FL boundaries we improve\\nthe results of three existing state-of-the-art methods on the\\ntasks of semantic boundary labeling, semantic segmenta1arXiv:1504.06201v3  [cs.CV]  21 Sep 2015\\ntion and object proposal generation. Therefore, using H FL\\nboundaries to boost the results in high level vision tasks',\n",
       " 'tion and object proposal generation. Therefore, using H FL\\nboundaries to boost the results in high level vision tasks\\ncan be viewed as a Low-for-High scheme, where boundaries\\nserve as low-level cues to aid high-level vision tasks.\\nWe present the summarized results for the boundary detection and the three mentioned high-level vision tasks in\\nTable 1. Speciﬁcally, we compare our proposed method and\\nan appropriate state-of-the-art method for that task. As the\\nresults indicate, we achieve better results in each of the tasks\\nfor each presented evaluation metric. We present more detailed results for each of these tasks in the later sections.\\nIn summary, our contributions are as follows. First, we\\nshow that using object-level features for boundary detection produces perceptually informative boundaries that outperform prior state-of-the-art boundary detection methods.\\nSecond, we demonstrate that we can use H FL boundaries\\nto enhance the performance on the high-level vision tasks\\nof semantic boundary labeling, semantic segmentation and\\nobject proposal. Finally, our method can detect boundaries\\nin near-real time. Thus, we present a boundary detection\\nsystem that is accurate, efﬁcient, and is also applicable to\\nhigh level vision tasks.\\n2. Related Work\\nMost of the contour detection methods predict boundaries based purely on color, text, or other low-level features.',\n",
       " 'high level vision tasks.\\n2. Related Work\\nMost of the contour detection methods predict boundaries based purely on color, text, or other low-level features.\\nWe can divide these methods into three broad categories:\\nspectral methods, supervised discriminative methods and\\ndeep learning based methods.\\nSpectral methods formulate contour detection problem\\nas an eigenvalue problem. The solution to this problem is\\nthen used to reason about the boundaries. The most successful approaches in this genre are the MCG detector [2], gPb\\ndetector [1], PMI detector [14], and Normalized Cuts [28].\\nSome of the notable discriminative boundary detection\\nmethods include sketch tokens (ST) [18], structured edges\\n(SE) [6] and sparse code gradients (SCG) [23]. While SCG\\nuse supervised SVM learning [4], the latter two methods\\nrely on a random forest classiﬁer and models boundary detection as a classiﬁcation task.\\nRecently there have been attempts to apply deep learning\\nto the task of boundary detection. SCT [20] is a sparse coding approach that reconstructs an image using a learned dictionary and then detect boundaries. Both N4ﬁelds [10] and',\n",
       " 'DeepNet [16] approaches use Convolutional Neural Networks (CNNs) to predict edges. N4ﬁelds rely on dictionary learning and the use of the Nearest Neighbor algorithm\\nwithin a CNN framework while DeepNet uses a traditional\\nCNN architecture to predict contours.\\nThe most similar to our approach is DeepEdge [3],\\nwhich uses a multi-scale bifurcated network to perform contour detection using object-level features. However, we\\nshow that our method achieves better results even without\\n1100x1100\\n512 \\n500x375...\\n1024 \\nInput Image\\n5504 Convolutional     Feature Maps 500x375Upsampled     Image\\n5504-Dimensional    Feature Vectors  Predicted  Boundaries\\n|{z}\\nFeature Interpolation & Concatenation\\n1100x1100x64- Candidate Contour Points\\n35x35x512\\nShared Weights for all     Candidate Points\\n|{z}\\nFigure 1: An illustration of our architecture (best viewed in\\ncolor). First we extract a set of candidate contour points.\\nThen we upsample the image and feed it through 16convolutional layers pretrained for object classiﬁcation. For each\\ncandidate point, we ﬁnd its correspondence in each of the\\nfeature maps and perform feature interpolation. This yields',\n",
       " 'candidate point, we ﬁnd its correspondence in each of the\\nfeature maps and perform feature interpolation. This yields\\na5504 -dimensional feature vector for each candidate point.\\nWe feed each of these vectors to two fully connected layers\\nand store the predictions to produce a ﬁnal boundary map.\\nthe complicated multi-scale and bifurcated architecture of\\nDeepEdge. Additionally, unlike DeepEdge, our system can\\nrun in near-real time.\\nIn comparison to prior approaches, we offer several contributions. First, we propose to use object-level information to predict boundaries. We argue that such an approach\\nleads to semantic boundaries, which are more consistent\\nwith humans reasoning. Second, we avoid feature engineering by learning boundaries from human-annotated data.\\nFinally, we demonstrate excellent results for both low-level\\nand high-level vision tasks. For the boundary detection task,\\nour proposed H FL boundaries outperform all of the prior\\nmethods according to both F-score metrics. Additionally,\\nwe show that because H FL boundaries are based on objectlevel features, they can be used to improve performance in\\nthe high-level vision tasks of semantic boundary labeling,\\nsemantic segmentation, and object proposal generation.\\n3. Boundary Detection\\nIn this section, we describe our proposed architecture\\nand the speciﬁc details on how we predict H FL boundaries',\n",
       " '3. Boundary Detection\\nIn this section, we describe our proposed architecture\\nand the speciﬁc details on how we predict H FL boundaries\\nusing our method. The detailed illustration of our architecture is presented in Figure 1.\\nFigure 2: A visualization of selected convolutional feature maps from VGG network (resized to the input image dimension).\\nBecause VGG was optimized for an object classiﬁcation task, it produces high activation values on objects and their parts.\\n3.1. Selection of Candidate Contour Points\\nWe ﬁrst extract a set of candidate contour points with\\na high recall. Due to its efﬁciency and high recall performance, we use the SE edge detector [6]. In practice, we\\ncould eliminate this step and simply try to predict boundaries at every pixel. However, selecting a set of initial candidate contour points, greatly reduces the computational cost.\\nSince our goal is to build a boundary detector that is both accurate and efﬁcient, we use these candidate points to speed\\nup the computation of our method.\\n3.2. Object-Level Features\\nAfter selecting candidate contour points, we up-sample\\nthe original input image to a larger dimension (for example\\n1100\\x021100 ). The up-sampling is done to minimize the loss\\nof information due to the input shrinkage caused by pooling',\n",
       " 'the original input image to a larger dimension (for example\\n1100\\x021100 ). The up-sampling is done to minimize the loss\\nof information due to the input shrinkage caused by pooling\\nat the different layers. Afterwards, we feed the up-sampled\\nimage through 16convolutional layers of the VGG net.\\nWe use the VGG net as our model because it has been\\ntrained to recognize a large number of object classes (the\\n1000 categories of the ImageNet dataset [24]) and thus encodes object-level features that apply to many classes. To\\npreserve speciﬁc location information we utilize only the\\n16convolutional layers of the VGG net. We don’t use fully\\nconnected layers because they don’t preserve spatial information, which is crucial for accurate boundary detection.\\nWe visualize some of the selected convolutional maps in\\nFigure 2. Note the high activation values around the various objects in the images, which conﬁrms our hypothesis\\nthat the VGG net encodes object speciﬁc information in its\\nconvolutional feature maps.\\n3.3. Feature Interpolation\\nSimilarly to [26, 12, 19], we perform feature interpolation in deep layers. After the up-sampled image passes',\n",
       " '3.3. Feature Interpolation\\nSimilarly to [26, 12, 19], we perform feature interpolation in deep layers. After the up-sampled image passes\\nthrough all 16convolutional layers, for each selected candidate contour point we ﬁnd its corresponding point in the\\nfeature maps. Due to the dimension differences in convo-lutional maps these correspondences are not exact. Thus\\nwe perform feature interpolation by ﬁnding the four nearest\\npoints and averaging their activation values. This is done\\nin each of the 5504 feature maps. Thus, this results in a\\n5504 -dimensional vector for each candidate point.\\nWe note that the interpolation of convolutional feature\\nmaps is the crucial component that enables our system to\\npredict the boundaries efﬁciently. Without feature interpolation, our method would need to independently process\\nthe candidate edge points by analyzing a small image patch\\naround each point, as for example done in DeepEdge [3]\\nwhich feeds one patch at a time through a deep network.\\nHowever, when the number of candidate points is large\\n(e.g., DeepEdge considers about 15K points at each of 4 different scales), their patches overlap signiﬁcantly and thus a\\nlarge amount of computation is wasted by recalculating ﬁlter response values over the same pixels. Instead, we can',\n",
       " 'large amount of computation is wasted by recalculating ﬁlter response values over the same pixels. Instead, we can\\ncompute the features for all candidate points with a single\\npass through the network by performing deep convolution\\nover the entire image (i.e., feeding the entire image rather\\nthan one patch at a time) and then by interpolating the convolutional feature maps at the location of each candidate\\nedge point so as to produce its feature descriptor. Thanks to\\nthis speedup, our method has a runtime of 1:2seconds (using a K40 GPU), which is better than the runtimes of prior\\ndeep-learning based edge detection methods [27, 10, 16, 3].\\n3.4. Learning to Predict Boundaries\\nAfter performing feature interpolation, we feed the\\n5504 -dimensional feature vectors corresponding to each of\\nthe candidate contour points to two fully connected layers\\nthat are optimized to the human agreement criterion. To be\\nmore precise, we deﬁne our prediction objective as a fraction of human annotators agreeing on the presence of the\\nboundary at a particular pixel. Therefore, a learning objective aims at mimicking the judgement of the human labelers.\\nFinally, to detect H FL boundaries, we accumulate the\\npredictions from the fully connected layers for each of the\\ncandidate points and produce a boundary probability map\\nas illustrated in Figure 1.',\n",
       " 'Finally, to detect H FL boundaries, we accumulate the\\npredictions from the fully connected layers for each of the\\ncandidate points and produce a boundary probability map\\nas illustrated in Figure 1.\\n3.5. Implementation Details\\nIn this section, we describe the details behind the training\\nprocedure of our model. We use the Caffe library [15] to\\nimplement our network architecture.\\nIn the training stage, we freeze the weights in all of the\\nconvolutional layers. To learn the weights in the two fully\\nconnected layers we train our model to optimize the least\\nsquares error of the regression criterion that we described\\nin the previous subsection. To enforce regularization we set\\na dropout rate of 0:5in the fully connected layers.\\nOur training dataset includes 80Kpoints from the\\nBSDS500 dataset [22]. As described in the previous subsection, the labels represent the fraction of human annotators agreeing on the boundary presence. We divide the label space into four quartiles, and select an equal number of\\nsamples for each quartile to balance the training dataset. In\\naddition to the training dataset, we also sample a hold-out\\ndataset of size 40;000. We use this for the hard-positive\\nmining [21] in order to reduce the number of false-negative\\npredictions.',\n",
       " 'dataset of size 40;000. We use this for the hard-positive\\nmining [21] in order to reduce the number of false-negative\\npredictions.\\nFor the ﬁrst 25epochs we train the network on the original80;000training samples. After the ﬁrst 25epochs, we\\ntest the network on the hold-out dataset and detect false negative predictions made by our network. We then augment\\nthe original 80;000 training samples with the false negatives and the same number of randomly selected true negatives. For the remaining 25epochs, we train the network on\\nthis augmented dataset.\\n3.6. Boundary Detection Results\\nIn this section, we present our results on the BSDS500\\ndataset [22], which is the most established benchmark for\\nboundary detection. The quality of the predicted boundaries is evaluated using three standard measures: ﬁxed contour threshold (ODS), per-image best threshold (OIS), and\\naverage precision (AP).\\nWe compare our approach to the state-of-the-art based\\non two different sets of BSDS500 ground truth boundaries.\\nFirst, we evaluate the accuracy by matching each of the\\npredicted boundary pixels with the ground truth boundaries\\nthat were annotated by anyof the human annotators. This',\n",
       " 'First, we evaluate the accuracy by matching each of the\\npredicted boundary pixels with the ground truth boundaries\\nthat were annotated by anyof the human annotators. This\\nset of ground truth boundaries is referred to as “any”. We\\npresent the results for “any” ground truth boundaries in the\\nlower half of Table 2. As indicated by the results, H FL\\nboundaries outperform all the prior methods according to\\nboth F-score measures.\\nRecently, there has been some criticism raised about\\nthe procedure for boundary detection evaluation on the\\nBSDS500 dataset. One issue with the BSDS500 dataset\\ninvolves the so called “orphan” boundaries: the bound-Consensus GT ODS OIS AP FPS\\nSCG [23] 0.6 0.64 0.56 1/280\\nDeepNet [16] 0.61 0.64 0.61 1=5z\\nPMI [14] 0.61 0.68 0.56 1/900\\nDeepEdge [3] 0.62 0.64 0.64 1=1000z\\nN4-ﬁelds [10] 0.64 0.67 0.64 1=6z\\nHFL 0.65 0.68 0.67 5=6z\\nAny GT ODS OIS AP FPS',\n",
       " 'HFL 0.65 0.68 0.67 5=6z\\nAny GT ODS OIS AP FPS\\nSE [6] 0.75 0.77 0.80 2.5\\nMCG [2] 0.75 0.78 0.76 1/24\\nN4-ﬁelds [10] 0.75 0.77 0.78 1=6z\\nDeepEdge [3] 0.75 0.77 0.81 1=1000z\\nMSC [30] 0.76 0.78 0.79 DeepContour [27] 0.76 0.77 0.8 1=30z\\nHFL 0.77 0.79 0.8 5=6z\\nTable 2: Boundary detection results on BSDS500 benchmark. Upper half of the table illustrates the results for “consensus” ground-truth criterion while the lower half of the\\ntable depicts the results for “any” ground-truth criterion. In\\nboth cases, our method outperforms all prior methods according to both ODS (optimal dataset scale) and OIS (optimal image scale) metrics. We also report the run-time of\\nour method (zGPU time) in the FPS column (frames per\\nsecond), which shows that our algorithm is faster than prior\\napproaches based on deep learning [27, 10, 16, 3].',\n",
       " 'second), which shows that our algorithm is faster than prior\\napproaches based on deep learning [27, 10, 16, 3].\\nFigure 5: Qualitative results on the BSDS benchmark. The\\nﬁrst column of images represent input images. The second\\ncolumn illustrates SE [6], while the third column depicts\\nHFL boundaries. Notice that SE boundaries are predicted\\nwith low conﬁdence if there is no signiﬁcant change in color\\nbetween the object and the background. Instead, because\\nour model is deﬁned in terms of object-level features, it can\\npredict object boundaries with high conﬁdence even if there\\nis no signiﬁcant color variation in the scene.\\naries that are marked by only one or two human annotators. These “orphan” boundaries comprise around 30% of\\nBSDS500 dataset but most of them are considered uninformative. However, the standard evaluation benchmark rewards the methods that predict these boundaries. To resolve\\n500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500024x 10−3Weight Magnitude\\nFeature Map Number (Sorted in the Order from Early to Deep Layers)Figure 6: We train a linear regression model and visualize\\nits weight magnitudes in order to understand which features\\nare used most heavily in the boundary prediction (this linear',\n",
       " 'its weight magnitudes in order to understand which features\\nare used most heavily in the boundary prediction (this linear\\nregression is used only for the visualization purposes and\\nnot for the accuracy analysis). Note how heavily weighted\\nfeatures lie in the deepest layers of the network, i.e., the layers that are most closely associated with object information.\\nthis issue we also evaluate our H FL boundaries on the so\\ncalled “consensus” set of ground truth boundaries. These\\n“consensus” boundaries involve only boundaries that are\\nmarked by allof the human annotators and hence, are considered perceptually meaningful. In the upper half of Table 2, we present the results achieved by our method on the\\n“consensus” set of the ground truth boundaries. Our H FL\\nboundaries outperform or tie all the prior methods in each\\nof the three evaluation metrics, thus suggesting that H FL\\nboundaries are similar to the boundaries that humans annotated. We also report the runtimes in Table 2 and note that\\nour method runs faster than previous deep-learning based\\nedge detection systems [27, 10, 16, 3].\\nOur proposed model computes a highly nonlinear function of the 5504-dimensional feature vector of each candidate point. Thus, it is difﬁcult to assess which features are\\nused most heavily by our edge predictor. However, we can',\n",
       " 'used most heavily by our edge predictor. However, we can\\ngain a better insight by replacing the nonlinear function with\\na simple linear model. In Fig. 6 we show the weight magnitudes of a simple linear regression model (we stress that\\nthis linear model is used only for feature visualization purposes). From this Figure, we observe that many important\\nfeatures are located in the deepest layers of the VGG network. As shown in [7], these layers encode high-level object information, which conﬁrms our hypothesis that highlevel information is useful for boundary detection.\\nFinally, we present some qualitative results achieved by\\nour method in Figure 5. These examples illustrate the effective advantage that H FL boundaries provide over another\\nstate-of-the-art edge detection system, the SE system [6].\\nSpeciﬁcally, observe the parts of the image where there is a\\nboundary that separates an object from the background but\\nwhere the color change is pretty small. Notice that because\\nthe SE boundary detection is based on low-level color and\\ntexture features, it captures these boundaries with very low\\nconﬁdence. In comparison, because H FL boundaries rely\\non object-level features, it detects these boundaries with\\nhigh conﬁdence.4. High-Level Vision Applications\\nIn this section, we describe our proposed Low-for-High',\n",
       " 'high conﬁdence.4. High-Level Vision Applications\\nIn this section, we describe our proposed Low-for-High\\npipeline: using low-level boundaries to aid a number of\\nhigh-level vision tasks. We focus on the tasks of semantic\\nboundary labeling, semantic segmentation and object proposal generation. We show that using H FL boundaries improves the performance of state-of-the-art methods in each\\nof these high-level vision tasks.\\n4.1. Semantic Boundary Labeling\\nThe task of semantic boundary labeling requires not only\\nto predict the boundaries but also to associate a speciﬁc\\nobject class to each of the boundaries. This implies that\\ngiven our predicted boundaries we also need to label them\\nwith object class information. We approach this problem\\nby adopting the ideas from the recent work on Fully Convolutional Networks (FCN) [19]. Given an input image,\\nwe concurrently feed it to our boundary-predicting network\\n(described in Section 3), and also through the FCN that was\\npretrained for 20Pascal VOC classes and the background\\nclass. While our proposed network produces H FL boundaries, the FCN model predicts class probabilities for each\\nof the pixels. We can then merge the two output maps as\\nfollows. For a given boundary point we consider a 9\\x029',\n",
       " 'of the pixels. We can then merge the two output maps as\\nfollows. For a given boundary point we consider a 9\\x029\\ngrid around that point from each of the 21FCN object-class\\nprobability maps. We calculate the maximum value inside\\neach grid, and then label the boundary at a given pixel with\\nthe object-class that corresponds to the maximum probability across these 21maps. We apply this procedure for each\\nof the boundary points, in order to associate object-class\\nlabels to the boundaries. Note that we consider the grids\\naround the boundary pixel because the output of the FCN\\nhas a poor localization, and considering the grids rather than\\nindividual pixels leads to higher accuracy.\\nWe can also merge H FL boundaries with the state-ofthe-art DeepLab-CRF segmentation [5] to obtain higher accuracy. We do this in a similar fashion as just described.\\nFirst, around a given boundary point we extract a 9\\x029\\ngrid from the DeepLab-CRF segmentation. We then compute the mode value in the grid (excluding the background\\nclass), and use the object-class corresponding to the mode\\nvalue as a label for the given boundary point. We do this for\\neach of the boundary points. By merging H FL boundaries\\nand the output of FCN or DeepLab-CRF, we get semantic',\n",
       " 'each of the boundary points. By merging H FL boundaries\\nand the output of FCN or DeepLab-CRF, we get semantic\\nboundaries that are highly localized and also contain objectspeciﬁc information.\\n4.1.1 Semantic Boundary Labeling Results\\nIn this section, we present semantic boundary labeling results on the SBD dataset [11], which includes ground truth\\nboundaries that are also labeled with one of 20Pascal VOC\\nclasses. The boundary detection accuracy for each class is\\nMethod (Metric) aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean\\nInvDet (MF) 42.6 49.5 15.7 16.8 36.7 43.0 40.8 22.6 18.1 26.6 10.2 18.0 35.2 29.4 48.2 14.3 26.8 11.2 22.2 32.0 28.0\\nHFL-FC8 (MF) 71.6 59.6 68.0 54.1 57.2 68.0 58.8 69.3 43.3 65.8 33.3 67.9 67.5 62.2 69.0 43.8 68.5 33.9 57.7 54.8 58.7',\n",
       " 'HFL-CRF (MF) 73.9 61.4 74.6 57.2 58.8 70.4 61.6 71.9 46.5 72.3 36.2 71.1 73.0 68.1 70.3 44.4 73.2 42.6 62.4 60.1 62.5\\nInvDet (AP) 38.4 29.6 9.6 9.9 24.2 33.6 31.3 17.3 10.7 16.4 3.7 12.1 28.5 20.4 45.7 7.6 16.1 5.7 14.6 22.7 19.9\\nHFL-FC8 (AP) 66.0 50.7 58.9 40.6 47.1 62.9 51.0 59.0 25.6 54.6 15.3 57.8 57.3 55.9 62.2 27.5 55.6 18.0 50.1 40.6 47.8\\nHFL-CRF (AP) 71.2 55.2 69.3 45.7 48.9 71.1 56.8 65.7 29.1 65.9 17.7 64.5 68.3 64.7 65.9 29.1 66.5 25.7 60.0 49.8 54.6',\n",
       " 'Table 3: Results of semantic boundary labeling on the SBD benchmark using the Max F-score (MF) and Average Precision\\n(AP) metrics. Our method (H FL) outperforms Inverse Detectors [11] for all 20categories according to both metrics. Note\\nthat using the CRF output to label the boundaries produces better results than using the outputs from the FC8 layer of FCN.\\nevaluated using the maximum F-score (MF), and average\\nprecision (AP) measures.\\nLabeling boundaries with the semantic object information is a novel and still relatively unexplored problem.\\nTherefore, we found only one other approach (Inverse Detectors) that tried to tackle this problem [11]. The basic idea behind Inverse Detectors consists of several steps.\\nFirst, generic boundaries in the image are detected. Then,\\na number of object proposal boxes are generated. These\\ntwo sources of information are then used to construct the\\nfeatures. Finally, a separate classiﬁer is used to label the\\nboundaries with the object-speciﬁc information.\\nTable 3 shows that our approach signiﬁcantly outperforms Inverse Detectors according to both the maximum Fscore and the average precision metrics for all twenty categories. As described in Section 4.1 we evaluate the two',\n",
       " 'variants of our method. Denoted by H FL-FC8 is the variant for which we label H FL boundaries with the outputs\\nfrom the last layer (FC8) of the pretrained FCN. We denote\\nwith H FL-CRF the result of labeling our boundaries with\\nthe output from the DeepLab-CRF [5]. Among these two\\nvariants, we show that the latter one produces better results.\\nThis is expected since the CRF framework enforces spatial\\ncoherence in the semantic segments.\\nIn Figure 7, we present some of the qualitative results\\nproduced by our method. We note that even with multiple\\nobjects in the image, our method successfully recognizes\\nand localizes boundaries of each of the classes.\\n4.2. Semantic Segmentation\\nFor the semantic segmentation task, we propose to\\nenhance the DeepLab-CRF [5] with our predicted H FL\\nboundaries. DeepLab-CRF is a system comprised of a Fully\\nConvolutional Network (described in Section 4.1) and a\\ndense CRF applied on top of FCN predictions.\\nSpeciﬁcally, in the CRF, the authors propose to use\\na Gaussian kernel and a bilateral term including position\\nand color terms as the CRF features (see [5]). While in\\nmost cases the proposed scheme works well, DeepLab-CRF',\n",
       " 'a Gaussian kernel and a bilateral term including position\\nand color terms as the CRF features (see [5]). While in\\nmost cases the proposed scheme works well, DeepLab-CRF\\nsometimes produces segmentations that are not spatially coherent, particularly for images containing small object reFigure 7: A visualization of the predicted semantic boundary labels. Images in the ﬁrst column are input examples.\\nColumns two and three show semantic H FL boundaries of\\ndifferent object classes. Note that even with multiple objects appearing simultaneously, our method outputs precise\\nsemantic boundaries.\\ngions.\\nWe propose to address this issue by adding features\\nbased on our predicted H FL boundaries in the CRF framework. Note that we use predicted boundaries from Section 3\\nand not the boundaries labeled with the object information\\nthat we obtained in Section 4.1. We use the Normalized\\nCut [28] framework to generate our features.\\nFirst, we construct a pixel-wise afﬁnity matrix Wusing\\nour H FL boundaries. We measure the similarity between\\ntwo pixels as:\\nWij= exp (\\x00max\\np2ijfM(p)2\\n\\x1b2g)\\nwhereWijrepresents the similarity between pixels iand\\nj,pdenotes the boundary point along the line segment ij\\nconnecting pixels iandj,Mdepicts the magnitude of the',\n",
       " 'j,pdenotes the boundary point along the line segment ij\\nconnecting pixels iandj,Mdepicts the magnitude of the\\nboundary at pixel p, and\\x1bdenotes the smoothness parameter, which is usually set to 14% of the maximum boundary\\nvalue in the image.\\nThe intuitive idea is that two pixels are similar (i.e.\\nWij= 1) if there is no boundary crossing the line connecting these two pixels (i.e. M(p) = 08p2ij) or if the\\nboundary strength is low. We note that it is not necessary\\nto build a full afﬁnity matrix W. We build a sparse afﬁnMetric Method (Dataset) aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean\\nPP-IOUDL-CRF (VOC) 78.6 41.1 83.5 75.3 72.9 83.1 76.6 80.8 37.8 72.1 66.5 64.7 65.8 75.7 80.5 34.4 75.9 47.4 86.6 77.9 68.9',\n",
       " 'DL-CRF+H FL (VOC) 77.9 41.2 83.1 74.4 73.2 85.5 76.1 80.6 35.7 71.0 66.6 64.3 65.9 75.2 80.2 32.8 75.2 47.0 87.1 77.9 68.5\\nDL-CRF (SBD) 74.2 68.0 81.9 64.6 71.8 86.3 78.3 84.3 41.6 78.0 49.9 82.0 78.5 77.1 80.1 54.3 75.6 49.8 79.5 70.1 71.4\\nDL-CRF+H FL (SBD) 75.1 69.2 81.6 64.8 71.3 86.4 78.1 84.1 41.2 77.8 50.4 81.6 78.2 78.5 80.7 53.8 74.9 49.1 79.5 70.4 71.4\\nPI-IOUDL-CRF (VOC) 46.1 28.0 48.5 54.5 45.5 57.6 34.1 47.3 19.5 61.4 41.6 42.5 34.4 61.8 62.1 22.1 50.5 41.0 61.2 31.9 44.6',\n",
       " 'DL-CRF+H FL (VOC) 47.5 27.6 50.4 63.5 47.7 57.9 38.7 47.2 21.1 57.3 41.2 43.7 36.0 66.4 61.1 21.3 53.9 42.1 70.9 34.6 46.5\\nDL-CRF (SBD) 59.4 36.5 58.0 38.6 32.0 58.1 44.7 59.6 25.8 51.8 28.1 59.0 46.9 50.3 61.8 22.2 45.9 33.4 62.1 41.0 45.8\\nDL-CRF+H FL (SBD) 63.4 42.5 58.4 41.3 32.5 61.2 45.7 61.4 28.4 55.5 31.5 61.4 51.8 54.6 62.1 24.9 52.6 34.2 67.1 45.1 48.8\\nTable 4: Semantic segmentation results on the SBD and VOC 2007 datasets. We measure the results according to PP-IOU\\n(per pixel) and PI-IOU (per image) evaluation metrics. We denote the original DeepLab-CRF system and our proposed',\n",
       " '(per pixel) and PI-IOU (per image) evaluation metrics. We denote the original DeepLab-CRF system and our proposed\\nmodiﬁcation as DL-CRF and DL-CRF+H FL, respectively. According to the PP-IOU metric, our proposed features (DLCRF+H FL) yield almost equivalent results as the original DeepLab-CRF system. However, based on PI-IOU metric, our\\nproposed features improve the mean accuracy by 3%and1:9%on SBD and VOC 2007 datasets respectively.\\nity matrix connecting every pair of pixels iandjthat have\\ndistance 5or less from each other.\\nAfter building a boundary-based afﬁnity matrix Wwe\\nsetDii=P\\ni6=jWijand compute eigenvectors vof the\\ngeneralized eigenvalue system:\\n(D\\x00W)v=\\x15Dv\\nWe then resize the eigenvectors vto the original image\\ndimensions, and use them as additional features to the CRF\\npart of DeepLab-CRF system. In our experiments, we use\\nthe16eigenvectors corresponding to the smallest eigenvalues, which results in 16extra feature channels.\\nNote that the eigenvectors contain soft segmentation information. Because H FL boundaries predict object-level',\n",
       " 'Note that the eigenvectors contain soft segmentation information. Because H FL boundaries predict object-level\\ncontours with high conﬁdence, the eigenvectors often capture regions corresponding to objects. We visualize a few\\nselected eigenvectors in Figure 8. In the experimental section, we demonstrate that our proposed features make the\\noutput produced by DeepLab-CRF more spatially coherent\\nand improve the segmentation accuracy according to one of\\nthe metrics.\\nWe also note that our proposed features are applicable to any generic method that incorporates CRF. For instance, even if DeepLab-CRF used an improved DeepLab\\nnetwork architecture, our features would still be beneﬁcial\\nbecause they contribute directly to the CRF part and not the\\nDeepLab network part of the system.\\n4.2.1 Semantic Segmentation Results\\nIn this section, we present semantic segmentation results\\non the SBD [11] and also Pascal VOC 2007 [8] datasets,\\nwhich both provide ground truth segmentations for 20Pascal VOC classes. We evaluate the results in terms of two\\nmetrics. The ﬁrst metric measures the accuracy in terms of\\npixel intersection-over-union averaged per pixel (PP-IOU)\\nacross the 20 classes. According to this metric, the accuracy\\nis computed on a per pixel basis. As a result, the images that',\n",
       " 'across the 20 classes. According to this metric, the accuracy\\nis computed on a per pixel basis. As a result, the images that\\ncontain large object regions are given more importance.\\nFigure 8: In this ﬁgure, the ﬁrst column depicts an input\\nimage while the second and third columns illustrate two selected eigenvectors for that image. The eigenvectors contain\\nsoft segmentation information. Because H FL boundaries\\ncapture object-level boundaries, the resulting eigenvectors\\nprimarily segment regions corresponding to the objects.\\nWe observe that while DeepLab-CRF works well on the\\nimages containing large object regions, it produces spatially\\ndisjoint outputs for the images with smaller and object regions (see Figure 9). This issue is often being overlooked,\\nbecause according to the PP-IOU metric, the images with\\nlarge object regions are given more importance and thus\\ncontribute more to the accuracy. However, certain applications may require accurate segmentation of small objects.\\nTherefore, in addition to PP-IOU, we also consider the PIIOU metric (pixel intersection-over-union averaged per image across the 20 classes), which gives equal weight to each\\nof the images.\\nFor both of the metrics we compare the semantic segmentation results of a pure DeepLab-CRF [5] and also a',\n",
       " 'of the images.\\nFor both of the metrics we compare the semantic segmentation results of a pure DeepLab-CRF [5] and also a\\nmodiﬁcation of DeepLab-CRF with our proposed features\\nadded to the CRF framework. We present the results for\\nboth of the metrics in Table 4.\\nBased on these results, we observe that according to\\nthe ﬁrst metric (PP-IOU), our proposed features yield almost equivalent results as the original DeepLab-CRF system. However, according to the second metric (PI-IOU) our\\nfeatures yield an average improvement of 3%and1:9%in\\nFigure 9: An illustration of the more challenging semantic\\nsegmentation examples. The ﬁrst column depicts the predictions achieved by DeepLab-CRF, while the second column illustrates the results after adding our proposed features to the CRF framework. The last column represents\\nground truth segmentations. Notice how our proposed features render the predicted semantic segments more spatially\\ncoherent and overall more accurate.\\nSBD and VOC 2007 datasets respectively.\\nWe also visualize the qualitative results produced by both\\napproaches in Figure 9. Notice how our proposed features\\nmake the segmentations look smoother relative to the segmentations produced by the original DeepLab-CRF system.\\nOnce again, we want to stress that our H FL features',\n",
       " 'make the segmentations look smoother relative to the segmentations produced by the original DeepLab-CRF system.\\nOnce again, we want to stress that our H FL features\\nare applicable to any method that uses the CRF. Therefore,\\nbased on the results presented in this section, we believe that\\nour proposed features could be beneﬁcial in a wide array of\\nproblems that involve the use of the CRF framework.\\n4.3. Object Proposals\\nFinally, we show that our method produces object-level\\nboundaries that can be successfully exploited in an object\\nproposal scheme. Speciﬁcally we adopt the EdgeBoxes approach [31], which can be applied to any generic boundaries to produce a list of object proposal boxes. The original EdgeBoxes method uses SE boundaries to generate the\\nboxes. However, SE boundaries are predicted using lowlevel color and texture features, rather than object-level\\nfeatures. Thus, here we validate the hypothesis that the\\nEdgeBoxes proposals can be improved by replacing the SE\\nboundaries with our H FL boundaries.\\n4.3.1 Object Proposal Results\\nIn this section, we present object proposal results on the\\nPascal VOC 2012 dataset [9]. We evaluate the quality of\\nbounding-box proposals according to three metrics: area\\nunder the curve (AUC), the number of proposals needed to',\n",
       " 'Pascal VOC 2012 dataset [9]. We evaluate the quality of\\nbounding-box proposals according to three metrics: area\\nunder the curve (AUC), the number of proposals needed to\\nreach recall of 75%, and the maximum recall over 5000 object bounding-boxes. Additionally, we compute the accuracy for each of the metrics for three different intersectionMethodIoU 0.65 IoU 0.7 IoU 0.75\\nAUC N@75% Recall AUC N@75% Recall AUC N@75% Recall\\nSE 0:52 413 0.93 0:47 658 0.88 0.41 inf 0:75\\nHFL 0.53 365 0.95 0.48 583 0.9 0.41 2685 0.77\\nTable 5: Comparison of object proposal results. We\\ncompare the quality of object proposals using Structured\\nEdges [6] and H FL boundaries. We evaluate the performance for three different IOU values and demonstrate that\\nusing H FL boundaries produces better results for each evaluation metric and for each IOU value.\\nover union (IOU) values: 0:65;0:7, and 0:75. We present\\nthese results in Table 5. As described in Section 4.3, we use\\nEdgeBoxes [31], a package that uses generic boundaries, to\\ngenerate object proposals. We compare the quality of the',\n",
       " 'EdgeBoxes [31], a package that uses generic boundaries, to\\ngenerate object proposals. We compare the quality of the\\ngenerated object proposals when using SE boundaries and\\nHFL boundaries. We demonstrate that for each IOU value\\nand for each of the three evaluation metrics, H FL boundaries produce better or equivalent results. This conﬁrms our\\nhypothesis that H FL boundaries can be used effectively for\\nhigh-level vision tasks such as generating object proposals.\\n5. Conclusions\\nIn this work, we presented an efﬁcient architecture that\\nuses object-level information to predict semantically meaningful boundaries. Most prior edge detection methods rely\\nexclusively on low-level features, such as color or texture, to\\ndetect the boundaries. However, perception studies suggest\\nthat humans employ object-level reasoning when deciding\\nwhether a given pixel is a boundary [13, 25, 17]. Thus,\\nwe propose a system that focuses on the semantic objectlevel cues rather than low level image information to detect\\nthe boundaries. For this reason we refer to our boundary\\ndetection scheme as a High-for-Low approach, where highlevel object features inform the low-level boundary detection process. In this paper we demonstrated that our proposed method produces boundaries that accurately separate\\nobjects and the background in the image and also achieve\\nhigher F-score compared to any prior work.',\n",
       " 'objects and the background in the image and also achieve\\nhigher F-score compared to any prior work.\\nAdditionally, we showed that, because H FL boundaries\\nare based on object-level features, they can be employed to\\naid a number of high level vision tasks in a Low-for-High\\nfashion. We use our boundaries to boost the accuracy of\\nstate-of-the-art methods on the high-level vision tasks of semantic boundary labeling, semantic segmentation, and object proposals generation. We show that using H FL boundaries leads to better results in each of these tasks.\\nTo conclude, our boundary detection method is accurate,\\nefﬁcient, applicable to a variety of datasets, and also useful\\nfor multiple high-level vision tasks. We plan to release the\\nsource code for H FL upon the publication of the paper .\\n6. Acknowledgements\\nWe thank Mohammad Haris Baig for the suggestions and\\nhelp with the software. This research was funded in part by\\nNSF award CNS-1205521.\\nReferences\\n[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour\\ndetection and hierarchical image segmentation. IEEE Trans.\\nPattern Anal. Mach. Intell. , 33(5):898–916, May 2011. 2',\n",
       " 'detection and hierarchical image segmentation. IEEE Trans.\\nPattern Anal. Mach. Intell. , 33(5):898–916, May 2011. 2\\n[2] P. Arbelaez, J. Pont-Tuset, J. Barron, F. Marqués, and J. Malik. Multiscale combinatorial grouping. In Computer Vision\\nand Pattern Recognition (CVPR) , 2014. 2, 4\\n[3] G. Bertasius, J. Shi, and L. Torresani. Deepedge: A multiscale bifurcated deep network for top-down contour detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2015. 2, 3, 4, 5\\n[4] C. J. C. Burges. A tutorial on support vector machines for\\npattern recognition. Data Mining and Knowledge Discovery ,\\n2:121–167, 1998. 2\\n[5] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.\\nYuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. CoRR , abs/1412.7062,\\n2014. 1, 5, 6, 7',\n",
       " '2014. 1, 5, 6, 7\\n[6] P. Dollár and C. L. Zitnick. Fast edge detection using structured forests. PAMI , 2015. 2, 3, 4, 5, 8\\n[7] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional\\nactivation feature for generic visual recognition. CoRR ,\\nabs/1310.1531, 2013. 5\\n[8] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman. The PASCAL Visual Object Classes\\nChallenge 2007 (VOC2007) Results. http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.\\n7\\n[9] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman. The PASCAL Visual Object Classes\\nChallenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.\\n8',\n",
       " 'Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.\\n8\\n[10] Y . Ganin and V . S. Lempitsky. N4-ﬁelds: Neural network\\nnearest neighbor ﬁelds for image transforms. ACCV , 2014.\\n2, 3, 4, 5\\n[11] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.\\nSemantic contours from inverse detectors. In International\\nConference on Computer Vision (ICCV) , 2011. 1, 5, 6, 7\\n[12] B. Hariharan, P. A. Arbeláez, R. B. Girshick, and J. Malik.\\nHypercolumns for object segmentation and ﬁne-grained localization. CoRR , abs/1411.5752, 2014. 3\\n[13] P.-J. Hsieh, E. Vul, and N. Kanwisher. Recognition alters the\\nspatial pattern of fmri activation in early retinotopic cortex.\\nJournal of Neurophysiology , 103(3):1501–1507, 2010. 1, 8',\n",
       " 'spatial pattern of fmri activation in early retinotopic cortex.\\nJournal of Neurophysiology , 103(3):1501–1507, 2010. 1, 8\\n[14] P. Isola, D. Zoran, D. Krishnan, and E. H. Adelson. Crisp\\nboundary detection using pointwise mutual information. In\\nECCV , 2014. 2, 4[15] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint\\narXiv:1408.5093 , 2014. 4\\n[16] J. J. Kivinen, C. K. Williams, N. Heess, and D. Technologies. Visual boundary prediction: A deep neural prediction\\nnetwork and quality dissection. AISTATS , 1(2):9, 2014. 2, 3,\\n4, 5\\n[17] Z. Kourtzi and N. Kanwisher. Representation of perceived\\nobject shape by the human lateral occipital complex. Science, 293:1506–1509, 2001. 1, 8',\n",
       " 'object shape by the human lateral occipital complex. Science, 293:1506–1509, 2001. 1, 8\\n[18] J. Lim, C. L. Zitnick, and P. Dollár. Sketch tokens: A learned\\nmid-level representation for contour and object detection. In\\nCVPR , 2013. 2\\n[19] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\\nnetworks for semantic segmentation. CVPR (to appear) ,\\nNov. 2015. 3, 5\\n[20] M. Maire, S. X. Yu, and P. Perona. Reconstructive sparse\\ncode transfer for contour detection and semantic labeling. In\\nAsian Conference on Computer Vision (ACCV) , 2014. 2\\n[21] T. Malisiewicz, A. Gupta, and A. A. Efros. Ensemble of\\nexemplar-svms for object detection and beyond. In ICCV ,\\n2011. 4\\n[22] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database\\nof human segmented natural images and its application to\\nevaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l Conf. Computer Vision , volume 2, pages 416–423, July 2001. 4',\n",
       " 'evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l Conf. Computer Vision , volume 2, pages 416–423, July 2001. 4\\n[23] X. Ren and L. Bo. Discriminatively Trained Sparse Code\\nGradients for Contour Detection. In Advances in Neural Information Processing Systems , December 2012. 2, 4\\n[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\\nRecognition Challenge, 2014. 3\\n[25] J. L. Sanguinetti, J. J. Allen, and M. A. Peterson. The ground\\nside of an object perceived as shapeless yet processed for\\nsemantics. Psychological science , page 0956797613502814,\\n2013. 1, 8\\n[26] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR ,\\nabs/1312.6229, 2013. 3',\n",
       " 'and Y . LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR ,\\nabs/1312.6229, 2013. 3\\n[27] W. Shen, X. Wang, Y . Wang, X. Bai, and Z. Zhang. Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection. June 2015. 1, 3, 4, 5\\n[28] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , 22:888–905, 1997. 2, 6\\n[29] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR ,\\nabs/1409.1556, 2014. 1\\n[30] A. Sironi, V . Lepetit, and P. Fua. Multiscale centerline detection by learning a scale-space distance transform. June 2014.\\n4\\n[31] C. L. Zitnick and P. Dollár. Edge boxes: Locating object\\nproposals from edges. In ECCV , 2014. 1, 8',\n",
       " 'Semantic Segmentation with Boundary Neural Fields\\nGedas Bertasius\\nUniversity of Pennsylvania\\ngberta@seas.upenn.eduJianbo Shi\\nUniversity of Pennsylvania\\njshi@seas.upenn.eduLorenzo Torresani\\nDartmouth College\\nlt@dartmouth.edu\\nAbstract\\nThe state-of-the-art in semantic segmentation is currently represented by fully convolutional networks (FCNs).\\nHowever, FCNs use large receptive ﬁelds and many pooling layers, both of which cause blurring and low spatial\\nresolution in the deep layers. As a result FCNs tend to produce segmentations that are poorly localized around object\\nboundaries. Prior work has attempted to address this issue\\nin post-processing steps, for example using a color-based\\nCRF on top of the FCN predictions. However, these approaches require additional parameters and low-level features that are difﬁcult to tune and integrate into the original\\nnetwork architecture. Additionally, most CRFs use colorbased pixel afﬁnities, which are not well suited for semantic\\nsegmentation and lead to spatially disjoint predictions.\\nTo overcome these problems, we introduce a Boundary',\n",
       " 'segmentation and lead to spatially disjoint predictions.\\nTo overcome these problems, we introduce a Boundary\\nNeural Field (BNF), which is a global energy model integrating FCN predictions with boundary cues. The boundary information is used to enhance semantic segment coherence and to improve object localization. Speciﬁcally, we\\nﬁrst show that the convolutional ﬁlters of semantic FCNs\\nprovide good features for boundary detection. We then employ the predicted boundaries to deﬁne pairwise potentials\\nin our energy. Finally, we show that our energy decomposes semantic segmentation into multiple binary problems,\\nwhich can be relaxed for efﬁcient global optimization. We\\nreport extensive experiments demonstrating that minimization of our global boundary-based energy yields results superior to prior globalization methods, both quantitatively\\nas well as qualitatively.\\n1. Introduction\\nThe recent introduction of fully convolutional networks\\n(FCNs) [22] has led to signiﬁcant quantitative improvements on the task of semantic segmentation. However, despite their empirical success, FCNs suffer from some limitations. Large receptive ﬁelds in the convolutional layers and\\nthe presence of pooling layers lead to blurring and segmentation predictions at a signiﬁcantly lower resolution than the',\n",
       " 'the presence of pooling layers lead to blurring and segmentation predictions at a signiﬁcantly lower resolution than the\\nFigure 1: Examples illustrating shortcomings of prior semantic segmentation methods: the second column shows\\nresults obtained with a FCN [22], while the third column\\nshows the output of a Dense-CRF applied to FCN predictions [19, 7]. Segments produced by FCN are blob-like and\\nare poorly localized around object boundaries. Dense-CRF\\nproduces spatially disjoint object segments due to the use of\\na color-based pixel afﬁnity function that is unable to measure semantic similarity between pixels.\\noriginal image. As a result, their predicted segments tend to\\nbe blobby and lack ﬁne object boundary details. We report\\nin Fig. 1 some examples illustrating typical poor localization of objects in the outputs of FCNs.\\nRecently, Chen at al. [7] addressed this issue by applying a Dense-CRF post-processing step [19] on top of coarse\\nFCN segmentations. However, such an approach introduces\\nseveral problems of its own. First, the Dense-CRF adds new\\nparameters that are difﬁcult to tune and integrate into the\\noriginal network architecture. Additionally, most methods',\n",
       " 'parameters that are difﬁcult to tune and integrate into the\\noriginal network architecture. Additionally, most methods\\nbased on CRFs or MRFs use low-level pixel afﬁnity functions, such as those based on color. These low-level afﬁnities often fail to capture semantic relationships between objects and lead to poor segmentation results (see last column\\nin Fig. 1).\\nWe propose to address these shortcomings by means of\\na Boundary Neural Field (BNF), an architecture that employs a single semantic segmentation FCN to predict semantic boundaries and then use them to produce semantic\\nsegmentation maps via a global optimization. We demonstrate that even though the semantic segmentation FCN has\\nnot been optimized to detect boundaries, it provides good\\n1arXiv:1511.02674v2  [cs.CV]  24 May 2016\\n...\\nConvolutional Feature Maps RGB Input Image\\nSegmentation Mask in       Softmax Layer\\n   Minimize Global Energy\\nBoundaries\\n|{z}...\\nwn()++()w1\\nXi✓i(zi)+Xij✓ij(zi,zj)\\nLinear Combination of Feature MapsFigure 2: The architecture of our system (best viewed in\\ncolor). We employ a semantic segmentation FCN [7] for\\ntwo purposes: 1) to obtain semantic segmentation unaries',\n",
       " 'color). We employ a semantic segmentation FCN [7] for\\ntwo purposes: 1) to obtain semantic segmentation unaries\\nfor our global energy; 2) to compute object boundaries.\\nSpeciﬁcally, we deﬁne semantic boundaries as a linear combination of these feature maps (with a sigmoid function applied on top of the sum) and learn individual weights corresponding to each convolutional feature map. We integrate\\nthis boundary information in the form of pairwise potentials\\n(pixel afﬁnities) for our energy model.\\nfeatures for boundary detection. Speciﬁcally, the contributions of our work are as follows:\\n\\x0fWe show that semantic boundaries can be expressed\\nas a linear combination of interpolated convolutional\\nfeature maps inside an FCN. We introduce a boundary\\ndetection method that exploits this intuition to predict\\nobject boundaries with accuracy superior to the statethe-of-art.\\n\\x0fWe demonstrate that boundary-based pixel afﬁnities\\nare better suited for semantic segmentation than the\\ncommonly used color afﬁnity functions.\\n\\x0fFinally, we introduce a new global energy that decomposes semantic segmentation into multiple binary\\nproblems and relaxes the integrality constraint. We\\nshow that minimizing our proposed energy yields better qualitative and quantitative results relative to traditional globalization models such as MRFs or CRFs.',\n",
       " 'problems and relaxes the integrality constraint. We\\nshow that minimizing our proposed energy yields better qualitative and quantitative results relative to traditional globalization models such as MRFs or CRFs.\\n2. Related Work\\nBoundary Detection. Spectral methods comprise one\\nof the most prominent categories for boundary detection.\\nIn a typical spectral framework, one formulates a generalized eigenvalue system to solve a low-level pixel groupingproblem. The resulting eigenvectors are then used to predict the boundaries. Some of the most notable approaches\\nin this genre are MCG [2], gPb [1], PMI [17], and Normalized Cuts [29]. A weakness of spectral approaches is that\\nthey tend to be slow as they perform a global inference over\\nthe entire image.\\nTo address this issue, recent approaches cast boundary\\ndetection as a classiﬁcation problem and predict the boundaries in a local manner with high efﬁciency. The most notable examples in this genre include sketch tokens (ST) [20]\\nand structured edges (SE) [9], which employ fast random\\nforests. However, many of these methods are based on\\nhand-constructed features, which are difﬁcult to tune.',\n",
       " 'forests. However, many of these methods are based on\\nhand-constructed features, which are difﬁcult to tune.\\nThe issue of hand-constructed features have been recently addressed by several approaches based on deep learning, such as N4ﬁelds [11], DeepNet [18], DeepContour [27], DeepEdge [3], H FL [4] and HED [33]. All of\\nthese methods use CNNs in some way to predict the boundaries. Whereas DeepNet and DeepContour optimize ordinary CNNs to a boundary based optimization criterion from\\nscratch, DeepEdge and H FL employ pretrained models to\\ncompute boundaries. The most recent of these methods is\\nHED [33], which shows the beneﬁt of deeply supervised\\nlearning for boundary detection.\\nIn comparison to prior deep learning approaches, our\\nmethod offers several contributions. First, we exploit the inherent relationship between boundary detection and semantic segmentation to predict semantic boundaries. Specifically, we show that even though the semantic FCN has\\nnot been explicitly trained to predict boundaries, the convolutional ﬁlters inside the FCN provide good features for\\nboundary detection. Additionally, unlike DeepEdge [3] and\\nHFL [4], our method does not require a pre-processing step\\nto select candidate contour points, as we predict boundaries',\n",
       " 'HFL [4], our method does not require a pre-processing step\\nto select candidate contour points, as we predict boundaries\\non all pixels in the image. We demonstrate that our approach allows us to achieve state-of-the-art boundary detection results according to both F-score and Average Precision metrics. Additionally, due to the semantic nature of\\nour boundaries, we can successfully use them as pairwise\\npotentials for semantic segmentation in order to improve\\nobject localization and recover ﬁne structural details, typically lost by pure FCN-based approaches.\\nSemantic Segmentation. We can group most semantic segmentation methods into three broad categories. The\\nﬁrst category can be described as “two-stage” approaches,\\nwhere an image is ﬁrst segmented and then each segment\\nis classiﬁed as belonging to a certain object class. Some\\nof the most notable methods that belong to this genre include [24, 6, 12, 14].\\nThe primary weakness of the above methods is that they\\nare unable to recover from errors made by the segmentation\\nalgorithm. Several recent papers [15, 10] address this issue\\nby proposing to use deep per-pixel CNN features and then\\nclassify each pixel as belonging to a certain class. While\\nthese approaches partially address the incorrect segmentation problem, they perform predictions independently on',\n",
       " 'classify each pixel as belonging to a certain class. While\\nthese approaches partially address the incorrect segmentation problem, they perform predictions independently on\\neach pixel. This leads to extremely local predictions, where\\nthe relationships between pixels are not exploited in any\\nway, and thus the resulting segmentations may be spatially\\ndisjoint.\\nThe third and ﬁnal group of semantic segmentation\\nmethods can be viewed as front-to-end schemes where segmentation maps are predicted directly from raw pixels without any intermediate steps. One of the earliest examples of\\nsuch methods is the FCN introduced in [22]. This approach\\ngave rise to a number of subsequent related approaches\\nwhich have improved various aspects of the original semantic segmentation [7, 34, 8, 16, 21]. There have also been attempts at integrating the CRF mechanism into the network\\narchitecture [7, 34]. Finally, it has been shown that semantic\\nsegmentation can also be improved using additional training\\ndata in the form of bounding boxes [8].\\nOur BNF offers several contributions over prior work. To\\nthe best of our knowledge, we are the ﬁrst to present a model\\nthat exploits the relationship between boundary detection\\nand semantic segmentation within a FCN framework . We\\nintroduce pairwise pixel afﬁnities computed from semantic boundaries inside an FCN, and use these boundaries to',\n",
       " 'and semantic segmentation within a FCN framework . We\\nintroduce pairwise pixel afﬁnities computed from semantic boundaries inside an FCN, and use these boundaries to\\npredict the segmentations in a global fashion. Unlike [21],\\nwhich requires a large number of additional parameters to\\nlearn for the pairwise potentials, our global model only\\nneeds\\x195Kextra parameters, which is about 3orders of\\nmagnitudes less than the number of parameters in a typical deep convolutional network (e.g. VGG [30]). We empirically show that our proposed boundary-based afﬁnities\\nare better suited for semantic segmentation than color-based\\nafﬁnities. Additionally, unlike in [7, 34, 21], the solution to\\nour proposed global energy can be obtained in closed-form,\\nwhich makes global inference easier. Finally we demonstrate that our method produces better results than traditional globalization models such as CRFs or MRFs.\\n3. Boundary Neural Fields\\nIn this section, we describe Boundary Neural Fields.\\nSimilarly to traditional globalization methods, Boundary\\nNeural Fields are deﬁned by an energy including unary\\nand pairwise potentials. Minimization of the global energy yields the semantic segmentation. BNFs build both\\nunary and pairwise potentials from the input RGB image',\n",
       " 'and pairwise potentials. Minimization of the global energy yields the semantic segmentation. BNFs build both\\nunary and pairwise potentials from the input RGB image\\nand then combine them in a global manner. More precisely,\\nthe coarse segmentations predicted by a semantic FCN are\\nused to deﬁne the unary potentials of our BNF. Next, we\\nshow that the convolutional feature maps of the FCN can\\nbe used to accurately predict semantic boundaries. These\\nboundaries are then used to build pairwise pixel afﬁnities,\\nwhich are used as pairwise potentials by the BNF. Finally,we introduce a global energy function, which minimizes the\\nenergy corresponding to the unary and pairwise terms and\\nimproves the initial FCN segmentation. The detailed illustration of our architecture is presented in Figure 2. We now\\nexplain each of these steps in more detail.\\n3.1. FCN Unary Potentials\\nTo predict semantic unary potentials we employ the\\nDeepLab model [7], which is a fully convolutional adaptation of the VGG network [30]. The FCN consists of 16convolutional layers and 3fully convolutional layers. There are\\nmore recent FCN-based methods that have demonstrated\\neven better semantic segmentation results [8, 34, 16, 21].',\n",
       " 'more recent FCN-based methods that have demonstrated\\neven better semantic segmentation results [8, 34, 16, 21].\\nAlthough these more advanced architectures could be integrated into our framework to improve our unary potentials, in this work we focus on two aspects orthogonal to\\nthis prior work: 1) demonstrating that our boundary-based\\nafﬁnity function is better suited for semantic segmentation\\nthan the common color-based afﬁnities and 2) showing that\\nour proposed global energy achieves better qualitative and\\nquantitative semantic segmentation results in comparison to\\nprior globalization models.\\n3.2. Boundary Pairwise Potentials\\nIn this section, we describe our approach for building\\npairwise pixel afﬁnities using semantic boundaries. The basic idea behind our boundary detection approach is to express semantic boundaries as a function of convolutional\\nfeature maps inside the FCN. Due to the close relationship\\nbetween the tasks of semantic segmentation and boundary\\ndetection, we hypothesize that convolutional feature maps\\nfrom the semantic segmentation FCN can be employed as\\nfeatures for boundary detection.\\n3.2.1 Learning to Predict Semantic Boundaries.\\nWe propose to express semantic boundaries as a linear combination of interpolated FCN feature maps with a non-linear\\nfunction applied on top of this sum. We note that interpolation of feature maps has been successfully used in prior',\n",
       " 'function applied on top of this sum. We note that interpolation of feature maps has been successfully used in prior\\nwork (see e.g. [15]) in order to obtain dense pixel-level features from the low-resolution outputs of deep convolutional\\nlayers. Here we adopt interpolation to produce pixel-level\\nboundary predictions. There are several advantages to our\\nproposed formulation. First, because we express boundaries\\nas a linear combination of feature maps, we only need to\\nlearn a small number of parameters, corresponding to the\\nindividual weight values of each feature map in the FCN.\\nThis amounts to\\x195Klearning parameters, which is much\\nsmaller than the number of parameters in the entire network\\n(\\x1915M). In comparison, DeepEdge [3] and HFL [4] need\\n17M and 6M additional parameters to predict boundaries.\\nFurthermore, expressing semantic boundaries as a linear\\ncombination of FCN feature maps allows us to efﬁciently\\npredict boundary probabilities for all pixels in the image\\n(we resize the FCN feature maps to the original image dimensions). This eliminates the need to select candidate\\nboundary points in a pre-processing stage, which was instead required in prior boundary detection work [3, 4].\\nOur boundary prediction pipeline can be described as',\n",
       " 'boundary points in a pre-processing stage, which was instead required in prior boundary detection work [3, 4].\\nOur boundary prediction pipeline can be described as\\nfollows. First we use use SBD segmentations [13] to optimize our FCN for semantic segmentation task. We then\\ntreat FCN convolutional maps as features for the boundary detection task and use the boundary annotations from\\nBSDS 500 dataset [23] to learn the weights for each feature\\nmap. BSDS 500 dataset contains 200training, 100validation, 200testing images, and ground truth annotations by 5\\nhuman labelers for each of these images.\\nTo learn the weights corresponding to each convolutional\\nfeature map we ﬁrst sample 80Kpoints from the dataset.\\nWe deﬁne the target labels for each point as the fraction of\\nhuman annotators agreeing on that point being a boundary.\\nTo ﬁx the issue of label imbalance (there are many more\\nnon-boundaries than boundaries), we divide the label space\\ninto four quartiles, and select an equal number of samples\\nfor each quartile to balance the training dataset. Given these\\nsampled points, we then deﬁne our features as the values in\\nthe interpolated convolutional feature maps corresponding\\nto these points. To predict semantic boundaries we weigh\\neach convolutional feature map by its weight, sum them up',\n",
       " 'the interpolated convolutional feature maps corresponding\\nto these points. To predict semantic boundaries we weigh\\neach convolutional feature map by its weight, sum them up\\nand apply a sigmoid function on top of it. We obtain the\\nweights corresponding to each convolutional feature map by\\nminimizing the cross-entropy loss using a stochastic batch\\ngradient descent for 50epochs. To obtain crisper boundaries at test-time we post-process the boundary probabilities\\nusing non-maximum suppression.\\nTo give some intuition on how FCN feature maps contribute to boundary detection, in Fig. 3 we visualize the feature maps corresponding to the highest weight magnitudes.\\nIt is clear that many of these maps contain highly localized\\nboundary information.\\nBoundary Detection Results Before discussing how\\nboundary information is integrated in our energy for semantic segmentation, here we present experimental results assessing the accuracy of our boundary detection scheme. We\\ntested our boundary detector on the BSDS500 dataset [23],\\nwhich is the standard benchmark for boundary detection.\\nThe quality of the predicted boundaries is evaluated using\\nthree standard measures: ﬁxed contour threshold (ODS),\\nper-image best threshold (OIS), and average precision (AP).\\nIn Table 1 we show that our algorithm outperforms all\\nprior methods according to both F-score measures and the',\n",
       " 'per-image best threshold (OIS), and average precision (AP).\\nIn Table 1 we show that our algorithm outperforms all\\nprior methods according to both F-score measures and the\\nAverage Precision metric. In Fig. 4, we also visualize our\\npredicted boundaries. The second column shows the pixellevel softmax output computed from the linear combination of feature maps, while the third column depicts our ﬁFigure 3: An input image and convolutional feature maps\\ncorresponding to the largest weight magnitude values. Intuitively these are the feature maps that contribute most heavily to the task of boundary detection.\\nMethod ODS OIS AP\\nSCG [25] 0.739 0.758 0.773\\nSE [9] 0.746 0.767 0.803\\nMCG [2] 0.747 0.779 0.759\\nN4-ﬁelds [11] 0.753 0.769 0.784\\nDeepEdge [3] 0.753 0.772 0.807\\nDeepContour [27] 0.756 0.773 0.797\\nHFL [4] 0.767 0.788 0.795\\nHED [33] 0.782 0.804 0.833\\nBNF 0.788 0.807 0.851',\n",
       " 'HED [33] 0.782 0.804 0.833\\nBNF 0.788 0.807 0.851\\nTable 1: Boundary detection results on BSDS500 benchmark. Our proposed method outperforms all prior algorithms according to all three evaluation metrics.\\nnal boundaries after applying a non-maximum suppression\\npost-processing step.\\nWe note that our predicted boundaries achieve highconﬁdence predictions around objects. This is important as\\nwe employ these boundaries to improve semantic segmentation results, as discussed in the next subsection.\\n3.2.2 Constructing Pairwise Pixel Afﬁnities.\\nWe can use the predicted boundaries to build pairwise pixel\\nafﬁnities. Intuitively, we declare two pixels as similar (i.e.,\\nlikely to belong to the same segment) if there is no boundary crossing the straight path between these two pixels.\\nConversely, two pixels are dissimilar if there is a boundary crossing their connecting path. The larger the boundary\\nmagnitude of the crossed path, the more dissimilar the two\\npixels should be, since a strong boundary is likely to mark\\nthe separation of two distinct segments. Similarly to [1], we\\nencode this intuition with a following formulation:\\nwsb\\nij= exp (\\x00Mij\\n\\x1bsb) (1)',\n",
       " 'encode this intuition with a following formulation:\\nwsb\\nij= exp (\\x00Mij\\n\\x1bsb) (1)\\nFigure 4: A ﬁgure illustrating our boundary detection results. In the second column, we visualize the raw probability output of our boundary detector. In the third column,\\nwe present the ﬁnal boundary maps after non-maximum\\nsuppression. While most prior methods predict the boundaries where the sharpest change in color occurs, our method\\ncaptures semantic object-level boundaries, which we subsequently use to aid semantic segmentation.\\nwhereMijdenotes the maximum boundary value that\\ncrosses the straight line path between pixels iandj,\\x1bsbdepicts the smoothing parameter and wsb\\nijdenotes the semantic\\nboundary-based afﬁnity between pixels iandj.\\nSimilarly, we want to exploit high-level object information in the network to deﬁne another type of pixel similarity.\\nSpeciﬁcally, we use object class probabilities from the softmax (SM) layer to achieve this goal. Intuitively, if pixels i\\nandjhave different hard segmentation labels from the softmax layer, we set their similarity ( wsm\\nij) to0. Otherwise,\\nwe compute their similarity using the following equation:\\nwsm\\nij= exp (\\x00Dij\\n\\x1bsm) (2)',\n",
       " 'ij) to0. Otherwise,\\nwe compute their similarity using the following equation:\\nwsm\\nij= exp (\\x00Dij\\n\\x1bsm) (2)\\nwhereDijdenotes the difference in softmax output values corresponding to the most likely object class for pixels\\niandj, and\\x1bsmis a smoothing parameter. Then we can\\nwrite the ﬁnal afﬁnity measure as:\\nwij= exp (wsm\\nij)wsb\\nij (3)\\nWe exponentiate the term corresponding to the objectlevel afﬁnity because our boundary-based afﬁnity may be\\ntoo aggressive in declaring two pixels as dissimilar. To address this issue, we increase the importance of the objectlevel afﬁnity in (3) using the exponential function. However, in the experimental results section, we demonstrate\\nthat most of the beneﬁt from modeling pairwise potentials\\ncomes from wsb\\nijrather thanwsm\\nij.We then use this pairwise pixel afﬁnity measure to build\\na global afﬁnity matrix Wthat encodes relationships between pixels in the entire image. For a given pixel, we\\nsample\\x1910% of points in the neighborhood of radius 20\\naround that pixel, and store the resulting afﬁnities into W.',\n",
       " 'sample\\x1910% of points in the neighborhood of radius 20\\naround that pixel, and store the resulting afﬁnities into W.\\n3.3. Global Inference\\nThe last step in our proposed method is to combine semantic boundary information with the coarse segmentation\\nfrom the FCN softmax layer to produce an improved segmentation. We do this by introducing a global energy function that utilizes the afﬁnity matrix constructed in the previous section along with the segmentation from the FCN\\nsoftmax layer. Using this energy, we perform a global inference to get segmentations that are well localized around the\\nobject boundaries and that are also spatially smooth.\\nTypical globalization models such as MRFs [31],\\nCRFs [19] or Graph Cuts [5] produce a discrete label assignment for the segmentation problem by jointly modeling a multi-label distribution and solving a non-convex optimization. The common problem in doing so is that the\\noptimization procedure may get stuck in local optima.\\nWe introduce a new global energy function, which overcomes this issue and achieves better segmentation in comparison to prior globalization models. Similarly to prior\\nglobalization approaches, our goal is to minimize the energy\\ncorresponding to the sum of unary and pairwise potentials.\\nHowever, the key difference in our approach comes from the',\n",
       " 'globalization approaches, our goal is to minimize the energy\\ncorresponding to the sum of unary and pairwise potentials.\\nHowever, the key difference in our approach comes from the\\nrelaxation of some of the constraints. Speciﬁcally, instead\\nof modeling our problem as a joint multi-label distribution,\\nwe propose to decompose it into multiple binary problems,\\nwhich can be solved concurrently. This decomposition can\\nbe viewed as assigning pixels to foreground and background\\nlabels for each of the different object classes. Additionally,\\nwe relax the integrality constraint. Both of these relaxations\\nmake our problem more manageable and allow us to formulate a global energy function that is differentiable, and has\\na closed form solution.\\nIn [35], the authors introduce the idea of learning\\nwith global and local consistency in the context of semisupervised problems. Inspired by this work, we incorporate\\nsome of these ideas in the context of semantic segmentation. Before deﬁning our proposed global energy function,\\nwe introduce some relevant notation.\\nFor the purpose of illustration, suppose that we only have\\ntwo classes: foreground and background. Then we can denote an optimal continuous solution to such a segmentation\\nproblem with variable z\\x03. To denote similarity between pixelsiandjwe usewij. Then,diindicates the degree of a',\n",
       " 'problem with variable z\\x03. To denote similarity between pixelsiandjwe usewij. Then,diindicates the degree of a\\npixeli. In graph theory, the degree of a node denotes the\\nnumber of edges incident to that node. Thus, we set the degree of a pixel to di=Pn\\nj=1wijfor alljexcepti6=j.\\nFinally, with fiwe denote an initial segmentation probaInput\\n Softmax\\n Dense-CRF\\n BNF Boundaries\\n BNF Segmentation\\nFigure 5: A ﬁgure illustrating semantic segmentation results. Images in columns two and three represent FCN softmax and\\nDense-CRF predictions, respectively. Note that all methods use the same FCN unary potentials. Additionally, observe that\\nunlike FCN and Dense-CRF, our methods predicts segmentation that are both well localized around object boundaries and\\nthat are also spatially smooth.\\nbility, which in our case is obtained from the FCN softmax\\nlayer.\\nUsing this notation, we can then formulate our global\\ninference objective as:\\nz\\x03= argmin\\nz\\x16\\n2X\\nidi(zi\\x00fi\\ndi)2+1\\n2X\\nijwij(zi\\x00zj)2(4)\\nThis energy consists of two different terms. Similar to',\n",
       " 'idi(zi\\x00fi\\ndi)2+1\\n2X\\nijwij(zi\\x00zj)2(4)\\nThis energy consists of two different terms. Similar to\\nthe general globalization framework, our ﬁrst term encodes\\nthe unary energy while the second term includes the pairwise energy. We now explain the intuition behind each of\\nthese terms. The unary term attempts to ﬁnd a segmentation\\nassignment ( zi) that deviates little from the initial candidate\\nsegmentation computed from the softmax layer (denoted by\\nfi). Theziin the unary term is weighted by the degree di\\nof the pixel in order to produce larger unary costs for pixels\\nthat have many similar pixels within the neighborhood. Instead, the pairwise term ensures that pixels that are similar\\nshould be assigned similar zvalues. To balance the energies of the two terms we introduce a parameter \\x16and set it\\nto0:025throughout all our experiments.\\nWe can also express the same global energy function in\\nmatrix notation:\\nz\\x03= argmin\\nz\\x16\\n2D(z\\x00D\\x001f)T(z\\x00D\\x001f)+1\\n2zT(D\\x00W)z\\n(5)\\nwhere z\\x03is an\\x021vector containing an optimal continuous assignment for all npixels, Dis a diagonal degree',\n",
       " '2zT(D\\x00W)z\\n(5)\\nwhere z\\x03is an\\x021vector containing an optimal continuous assignment for all npixels, Dis a diagonal degree\\nmatrix, and Wis then\\x02npixel afﬁnity matrix. Finally, f\\ndenotes an\\x021vector containing the probabilities from the\\nsoftmax layer corresponding to a particular object class.\\nAn advantage of our energy is that it is differentiable. Ifwe denote the above energy as E(z)then the derivative of\\nthis energy can be written as follows:\\n@E(z)\\n@z=\\x16D(z\\x00D\\x001f) + (D\\x00W)z=0 (6)\\nWith simple algebraic manipulations we can then obtain\\na closed form solution to this optimization:\\nz\\x03= (D\\x00\\x0bW)\\x001\\x0cf (7)\\nwhere\\x0b=1\\n1+\\x16and\\x0c=\\x16\\n1+\\x16. In the general case where\\nwe havekobject classes we can write the solution as:\\nZ\\x03= (D\\x00\\x0bW)\\x001\\x0cF (8)\\nwhere Znow depicts a n\\x02kmatrix containing assignments for all kobject classes, while Fdenotesn\\x02kmatrix\\nwith object class probabilities from softmax layer. Due to',\n",
       " 'where Znow depicts a n\\x02kmatrix containing assignments for all kobject classes, while Fdenotesn\\x02kmatrix\\nwith object class probabilities from softmax layer. Due to\\nthe large size of D\\x00\\x0bWit is impractical to invert it. However, if we consider an image as a graph where each pixel\\ndenotes a vertex in the graph, we can observe that the term\\nD\\x00Win our optimization is equivalent to a Laplacian matrix of such graph. Since we know that a Laplacian matrix is\\npositive semi-deﬁnite, we can use the preconditioned conjugate gradient method [28] to solve the system in Eq. (9).\\nAlternatively, because our deﬁned global energy in Eq. (5)\\nis differentiable, we can efﬁciently solve this optimization\\nproblem using stochastic gradient descent. We choose the\\nformer option and solve the following system:\\n(D\\x00\\x0bW)z\\x03=\\x0cf (9)\\nTo obtain the ﬁnal discrete segmentation, for each pixel\\nwe assign the object class that corresponds to the largest\\ncolumn value in the row of Z(note that each row in Zrepresents a single pixel in the image, and each column in Z',\n",
       " 'we assign the object class that corresponds to the largest\\ncolumn value in the row of Z(note that each row in Zrepresents a single pixel in the image, and each column in Z\\nMetric Inference Method RGB Afﬁnity BNF Afﬁnity\\nPP-IOUBelief Propagation [31] 75.4 75.6\\nICM 74.2 75.8\\nTRWS [32] 75.9 76.7\\nQPBO [26] 76.9 77.2\\nBNF 74.6 77.6\\nPI-IOUBelief Propagation [31] 45.9 46.2\\nICM 45.7 48.8\\nTRWS [32] 51.5 52.0\\nQPBO [26] 55.3 57.2\\nBNF 53.0 58.5\\nTable 2: We compare semantic segmentation results\\nwhen using a color-based pixel afﬁnity and our proposed\\nboundary-based afﬁnity. We note that our proposed afﬁnity\\nimproves the performance of all globalization techniques.\\nNote that all of the inference methods use the same FCN\\nunary potentials . This suggests that for every method our',\n",
       " 'improves the performance of all globalization techniques.\\nNote that all of the inference methods use the same FCN\\nunary potentials . This suggests that for every method our\\nboundary-based afﬁnity is more beneﬁcial for semantic segmentation than the color-based afﬁnity.\\nrepresents one of the object classes). In the experimental\\nsection, we show that this solution produces better quantitative and qualitative results in comparison to commonly used\\nglobalization techniques.\\n4. Experimental Results\\nIn this section we present quantitative and qualitative results for semantic segmentation on the SBD [13] dataset,\\nwhich contains objects and their per-pixel annotations for\\n20Pascal VOC classes. We evaluate semantic segmentation results using two evaluation metrics. The ﬁrst metric\\nmeasures accuracy based on pixel intersection-over-union\\naveraged per pixels (PP-IOU) across the 20 classes. According to this metric, the accuracy is computed on a perpixel basis. As a result, the images that contain large object regions are given more importance. However, for certain applications we may need to accurately segment small\\nobjects. Therefore, similar to [4] we also consider the PIIOU metric (pixel intersection-over-union averaged per image across the 20 classes), which gives equal weight to each\\nof the images.',\n",
       " 'of the images.\\nWe compare Boundary Neural Fields with other commonly used global inference methods. These methods include Belief Propagation [31], Iterated Conditional Mode\\n(ICM), Graph Cuts [5], and Dense-CRF [19]. Note that in\\nall of our evaluations we use the same FCN unary potentials\\nfor every model.\\nOur evaluations provide evidence for three conclusions:\\n\\x0fIn Subsection 4.1, we show that our boundary-based\\npixel afﬁnities are better suited for semantic segmentation than the traditional color-based afﬁnities.\\n\\x0fIn Subsection 4.2, we demonstrate that our global minimization leads to better results than those achieved byother inference schemes.\\n\\x0fIn Fig. 5, we qualitatively compare the outputs of FCN\\nand Dense-CRF to our predicted segmentations. This\\ncomparison shows that the BNF segments are better\\nlocalized around the object boundaries and that they\\nare also spatially smooth.\\n4.1. Comparing Afﬁnity Functions for Semantic\\nSegmentation\\nIn Table 2, we consider two global models. Both models use the same unary potentials obtained from the FCN\\nsoftmax layer. However, the ﬁrst model uses the popular\\ncolor-based pairwise afﬁnities, while the second employs',\n",
       " 'softmax layer. However, the ﬁrst model uses the popular\\ncolor-based pairwise afﬁnities, while the second employs\\nour boundary-based afﬁnities. Each of these two models\\nis optimized using several inference strategies. The table\\nshows that using our boundary based-afﬁnity function improves the results of all global inference methods according to both evaluation metrics. Note that we cannot include Dense-CRF [19] in this comparison because it employs an efﬁcient message-passing technique and integrating our afﬁnities into this technique is a non-trivial task.\\nHowever, we compare our method with Dense-CRF in Subsection 4.2.\\nThe results in Table 2 suggest that our semantic boundary based pixel afﬁnity function yields better semantic segmentation results compared to the commonly-used color\\nbased afﬁnities. We note that we also compared the results\\nof our inference technique using other edge detectors, notably UCM [1] and H FL [4]. In comparison to UCM edges,\\nwe observed that our boundaries provide 1:0%and6:0%\\naccording to both evaluation metrics respectively. When\\ncomparing our boundaries with H FL method, we observed\\nsimilar segmentation performance, which suggests that our\\nmethod works best with the high quality semantic boundaries.',\n",
       " 'comparing our boundaries with H FL method, we observed\\nsimilar segmentation performance, which suggests that our\\nmethod works best with the high quality semantic boundaries.\\n4.2. Comparing Inference Methods for Semantic\\nSegmentation\\nAdditionally, we also present semantic segmentation results for both of the metrics (PP-IOU and PI-IOU) in Table 3. In this comparison, all the techniques use the same\\nFCN unary potentials. Additionally, all inference methods\\nexcept Dense-CRF use our afﬁnity measure (since the previous analysis suggested that our afﬁnities yield better performance). We use BNF-SB to denote the variant of our\\nmethod that uses only semantic boundary based afﬁnities.\\nAdditionally, we use BNF-SB-SM to indicate the version\\nof our method that uses both boundary and softmax -based\\nafﬁnities (see Eq. (3)).\\nBased on these results, we observe that our proposed\\ntechnique outperforms all the other globalization methods\\naccording to both metrics, by 0:3%and1:3%respectively.\\nMetric Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean',\n",
       " 'Metric Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean\\nPP-IOUFCN-Softmax 80.7 71.6 80.7 71.3 72.9 88.1 81.8 86.6 47.4 82.9 57.9 83.9 79.6 80.4 81.0 64.7 78.2 54.5 80.9 69.9 74.8\\nBelief Propagation [31] 81.4 72.2 82.4 72.2 74.3 88.8 82.4 87.2 48.4 83.8 58.4 84.6 80.5 80.9 81.5 65.1 79.5 55.5 81.5 71.2 75.6\\nICM 81.7 72.2 82.8 72.1 75.3 89.6 83.4 87.7 46.3 83.3 58.4 84.6 80.6 81.4 81.5 65.8 79.5 56.0 80.7 74.1 75.8',\n",
       " 'TRWS [32] 81.6 70.9 83.8 72.0 75.1 89.5 82.5 88.0 51.7 86.6 61.9 85.8 83.3 80.8 81.1 65.3 81.5 58.8 77.6 75.9 76.7\\nGraph Cuts [5] 82.5 72.4 84.6 73.3 77.2 89.7 83.3 88.8 49.3 84.0 60.3 85.4 82.2 81.2 81.9 66.7 79.8 58.0 82.3 74.9 76.9\\nQPBO [26] 82.6 72.3 84.7 73.1 76.7 89.9 83.6 89.3 49.7 85.0 61.1 86.2 82.9 81.3 82.3 67.1 80.5 58.8 82.2 75.1 77.2\\nDense-CRF [19] 83.4 71.5 84.9 72.6 76.2 89.5 83.3 89.1 50.4 86.7 61.0 86.8 83.5 81.8 82.3 66.9 82.2 58.2 81.9 75.1 77.3',\n",
       " 'BNF-SB 81.9 72.5 84.9 73.3 76.0 90.3 83.1 89.2 51.2 86.7 61.5 86.6 83.2 81.3 81.9 66.2 81.7 58.6 81.6 75.8 77.4\\nBNF-SB-SM 82.2 73.1 85.1 73.8 76.7 90.6 83.4 89.5 51.3 86.7 61.4 86.8 83.3 81.7 82.3 67.7 81.9 58.4 82.4 75.4 77.6\\nPI-IOUFCN-Softmax 56.9 35.1 47.8 41.1 27.4 51.1 43.4 52.7 22.2 43.1 29.2 54.2 40.5 45.6 59.1 24.2 43.6 24.8 55.9 37.2 41.8\\nBelief Propagation [31] 68.0 38.6 52.9 45.8 31.9 55.9 47.2 58.2 24.6 49.9 31.7 60.2 44.9 50.1 62.4 25.2 49.9 27.6 62.3 42.2 46.2',\n",
       " 'ICM 65.3 40.9 56.4 45.3 33.7 58.9 49.5 61.9 25.8 53.5 33.2 62.1 48.0 53.2 63.4 24.1 54.8 34.0 63.7 47.7 48.8\\nTRWS [32] 67.5 40.7 60.3 46.3 35.6 63.4 49.6 69.3 29.7 58.9 37.8 67.4 57.3 53.8 64.1 26.3 62.0 36.9 63.1 49.9 52.0\\nGraph Cuts [5] 72.1 47.8 64.5 50.8 36.0 70.8 51.4 71.6 31.7 65.8 34.4 71.8 62.0 59.4 64.8 29.0 60.9 38.7 70.3 51.6 55.3\\nQPBO [26] 71.6 46.8 65.6 49.6 38.0 72.6 52.7 76.7 32.5 69.6 38.9 74.4 61.4 61.0 66.2 30.3 68.7 41.4 72.2 52.8 57.2',\n",
       " 'Dense-CRF [19] 68.0 39.5 58.0 45.0 33.4 62.8 47.7 66.0 29.4 60.9 36.0 68.5 54.6 51.4 63.7 28.3 57.6 37.1 65.9 48.2 51.1\\nBNF-SB 71.6 48.1 67.2 52.3 37.8 79.5 52.9 80.8 33.3 71.5 39.5 75.1 65.7 63.4 65.1 31.1 67.5 39.6 73.2 54.7 58.5\\nBNF-SB-SM 72.0 48.9 66.5 52.9 39.1 79.0 53.4 78.6 32.9 72.2 39.4 74.6 65.9 64.2 65.8 31.7 66.9 39.0 73.1 53.9 58.5\\nTable 3: Semantic segmentation results on the SBD dataset according to PP-IOU (per pixel) and PI-IOU (per image) evaluation metrics. We use BNF-SB to denote the variant of our method that uses only semantic boundary based afﬁnities.',\n",
       " 'Additionally, we use BNF-SB-SM to indicate our method that uses boundary and softmax based afﬁnities (See Eq. (3)). We\\nobserve that our proposed globalization method outperforms other globalization techniques according to both metrics by\\nat least 0:3%and1:3%respectively. Note that in this experiment, all of the inference methods use the same FCN unary\\npotentials . Additionally, for each method except Dense-CRF (it is challenging to incorporate boundary based afﬁnities into\\nthe Dense-CRF framework) we use our boundary based afﬁnities, since those lead to better results.\\nAdditionally, these results indicate that most beneﬁt comes\\nfrom the semantic boundary afﬁnity term rather than the\\nsoftmax afﬁnity term.\\nIn Fig. 5, we also present qualitative semantic segmentation results. Note that, compared to the segmentation output from the softmax layer, our segmentation is much better localized around the object boundaries. Additionally,\\nin comparison to Dense-CRF predictions, our method produces segmentations that are much spatially smoother.\\n4.3. Semantic Boundary Classiﬁcation\\nWe can also label our boundaries with a speciﬁc object',\n",
       " '4.3. Semantic Boundary Classiﬁcation\\nWe can also label our boundaries with a speciﬁc object\\nclass, using the same classiﬁcation strategy as in the H FL\\nsystem [4]. Since the SBD dataset provides annotations for\\nsemantic boundary classiﬁcation, we can test our results\\nagainst the state-of-the-art H FL [4] method for this task.\\nDue to the space limitation, we do not include full results for\\neach category. However, we observe that our produced results achieve mean Max F-Score of 54:5%(averaged across\\nall20classes) whereas H FL method obtains 51:7%.\\n5. Conclusions\\nIn this work we introduced a Boundary Neural Field\\n(BNF), an architecture that employs a semantic segmentation FCN to predict semantic boundaries and then uses the\\npredicted boundaries and the FCN output to produce an improved semantic segmentation maps a global optimization.\\nWe showed that our predicted boundaries are better suited\\nfor semantic segmentation than the commonly used low-level color based afﬁnities. Additionally, we introduced a\\nglobal energy function that decomposes semantic segmentation into multiple binary problems and relaxes an integrality constraint. We demonstrated that the minimization\\nof this global energy allows us to predict segmentations\\nthat are better localized around the object boundaries and',\n",
       " 'of this global energy allows us to predict segmentations\\nthat are better localized around the object boundaries and\\nthat are spatially smoother compared to the segmentations\\nachieved by prior methods. We made the code of our globalization technique available at http://www.seas.upenn.\\nedu/~gberta/publications.html .\\nThe main goal of this work was to show the effectiveness of boundary-based afﬁnities for semantic segmentation. However, due to differentiability of our global energy,\\nit may be possible to add more parameters inside the BNFs\\nand learn them in a front-to-end fashion. We believe that\\noptimizing the entire architecture jointly could capture the\\ninherent relationship between semantic segmentation and\\nboundary detection even better and further improve the performance of BNFs. We will investigate this possibility in\\nour future work.\\n6. Acknowledgements\\nThis research was funded in part by NSF award CNS1205521.\\nReferences\\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. IEEE\\nTrans. Pattern Anal. Mach. Intell. , 33(5):898–916, May 2011. 2, 4, 7',\n",
       " 'Trans. Pattern Anal. Mach. Intell. , 33(5):898–916, May 2011. 2, 4, 7\\n[2] Pablo Arbelaez, J. Pont-Tuset, Jon Barron, F. Marqués, and Jitendra\\nMalik. Multiscale combinatorial grouping. In Computer Vision and\\nPattern Recognition (CVPR) , 2014. 2, 4\\n[3] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Deepedge: A\\nmulti-scale bifurcated deep network for top-down contour detection.\\nInThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2015. 2, 3, 4\\n[4] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. High-for-low\\nand low-for-high: Efﬁcient boundary detection from deep object features and its applications to high-level vision. In The IEEE International Conference on Computer Vision (ICCV) , December 2015. 2,\\n3, 4, 7, 8\\n[5] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. IEEE Trans. Pattern Anal. Mach.\\nIntell. , 23(11):1222–1239, November 2001. 5, 7, 8',\n",
       " 'Intell. , 23(11):1222–1239, November 2001. 5, 7, 8\\n[6] João Carreira, Rui Caseiro, Jorge Batista, and Cristian Sminchisescu.\\nSemantic segmentation with second-order pooling. In Proceedings\\nof the 12th European Conference on Computer Vision - Volume Part\\nVII, ECCV’12, pages 430–443, Berlin, Heidelberg, 2012. SpringerVerlag. 2\\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin\\nMurphy, and Alan L. Yuille. Semantic image segmentation with deep\\nconvolutional nets and fully. In ICLR , 2015. 1, 2, 3\\n[8] Jifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation. In The IEEE International Conference on Computer Vision\\n(ICCV) , December 2015. 3\\n[9] Piotr Dollár and C. Lawrence Zitnick. Fast edge detection using\\nstructured forests. PAMI , 2015. 2, 4',\n",
       " '[9] Piotr Dollár and C. Lawrence Zitnick. Fast edge detection using\\nstructured forests. PAMI , 2015. 2, 4\\n[10] Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence , August 2013.\\n2\\n[11] Yaroslav Ganin and Victor S. Lempitsky. N4-ﬁelds: Neural network\\nnearest neighbor ﬁelds for image transforms. ACCV , 2014. 2, 4\\n[12] Saurabh Gupta, Ross Girshick, Pablo Arbeláez, and Jitendra Malik.\\nLearning rich features from RGB-D images for object detection and\\nsegmentation. In Proceedings of the European Conference on Computer Vision (ECCV) , 2014. 2\\n[13] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu\\nMaji, and Jitendra Malik. Semantic contours from inverse detectors.\\nInInternational Conference on Computer Vision (ICCV) , 2011. 4, 7',\n",
       " 'Maji, and Jitendra Malik. Semantic contours from inverse detectors.\\nInInternational Conference on Computer Vision (ICCV) , 2011. 4, 7\\n[14] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In European Conference on Computer Vision (ECCV) , 2014. 2\\n[15] Bharath Hariharan, Pablo Andrés Arbeláez, Ross B. Girshick, and\\nJitendra Malik. Hypercolumns for object segmentation and ﬁnegrained localization. In The IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , June 2015. 2, 3\\n[16] Seunghoon Hong, Hyeonwoo Noh, and Bohyung Han. Decoupled\\ndeep neural network for semi-supervised semantic segmentation. In\\nNIPS) , December 2015. 3\\n[17] Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H. Adelson.\\nCrisp boundary detection using pointwise mutual information. In\\nECCV , 2014. 2\\n[18] Jyri J Kivinen, Christopher KI Williams, and Nicolas Heess. Visual\\nboundary prediction: A deep neural prediction network and quality',\n",
       " '[18] Jyri J Kivinen, Christopher KI Williams, and Nicolas Heess. Visual\\nboundary prediction: A deep neural prediction network and quality\\ndissection. AISTATS , 1(2):9, 2014. 2\\n[19] Philipp Krähenbühl and Vladlen Koltun. Efﬁcient inference in fully\\nconnected crfs with gaussian edge potentials. In J. Shawe-Taylor,\\nR.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors,\\nAdvances in Neural Information Processing Systems 24 , pages 109–\\n117. Curran Associates, Inc., 2011. 1, 5, 7, 8[20] Joseph Lim, C. Lawrence Zitnick, and Piotr Dollár. Sketch tokens:\\nA learned mid-level representation for contour and object detection.\\nInCVPR , 2013. 2\\n[21] Guosheng Lin, Chunhua Shen, Ian D. Reid, and Anton van den Hengel. Efﬁcient piecewise training of deep structured models for semantic segmentation. CoRR , abs/1504.01013, 2015. 3\\n[22] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In The IEEE Conference',\n",
       " '[22] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In The IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR) , June 2015. 1,\\n3\\n[23] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human\\nsegmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th\\nInt’l Conf. Computer Vision , volume 2, pages 416–423, July 2001. 4\\n[24] Mohammadreza Mostajabi, Payman Yadollahpour, and Gregory\\nShakhnarovich. Feedforward semantic segmentation with zoom-out\\nfeatures. CoRR , abs/1412.0774, 2014. 2\\n[25] X. Ren and L. Bo. Discriminatively Trained Sparse Code Gradients\\nfor Contour Detection. In Advances in Neural Information Processing Systems , December 2012. 4\\n[26] Carsten Rother, Vladimir Kolmogorov, Victor Lempitsky, and Martin\\nSzummer. Optimizing binary mrfs via extended roof duality. In Proc\\nComp. Vision Pattern Recogn. (CVPR) , June 2007. 7, 8\\n[27] Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, and Zhijiang',\n",
       " 'Comp. Vision Pattern Recogn. (CVPR) , June 2007. 7, 8\\n[27] Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, and Zhijiang\\nZhang. Deepcontour: A deep convolutional feature learned by\\npositive-sharing loss for contour detection. June 2015. 2, 4\\n[28] Jonathan R Shewchuk. An introduction to the conjugate gradient\\nmethod without the agonizing pain. Technical report, Pittsburgh, PA,\\nUSA, 1994. 6\\n[29] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 22:888–905, 1997. 2\\n[30] K. Simonyan and A. Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. CoRR , abs/1409.1556, 2014. 3\\n[31] Marshall F. Tappen and William T. Freeman. Comparison of graph\\ncuts with belief propagation for stereo, using identical mrf parameters. In Proceedings of the Ninth IEEE International Conference on\\nComputer Vision - Volume 2 , ICCV ’03, pages 900–, Washington,\\nDC, USA, 2003. IEEE Computer Society. 5, 7, 8',\n",
       " 'Computer Vision - Volume 2 , ICCV ’03, pages 900–, Washington,\\nDC, USA, 2003. IEEE Computer Society. 5, 7, 8\\n[32] Martin Wainwright, Tommi Jaakkola, and Alan Willsky. Map estimation via agreement on (hyper)trees: Message-passing and linear\\nprogramming approaches. IEEE Transactions on Information Theory, 51:3697–3717, 2002. 7, 8\\n[33] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.\\nInThe IEEE International Conference on Computer Vision (ICCV) ,\\nDecember 2015. 2, 4\\n[34] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip Torr.\\nConditional random ﬁelds as recurrent neural networks. In International Conference on Computer Vision (ICCV) , 2015. 3\\n[35] Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston,\\nand Bernhard Schölkopf. Learning with local and global consistency.\\nIn S. Thrun, L.K. Saul, and B. Schölkopf, editors, Advances in Neural Information Processing Systems 16 , pages 321–328. MIT Press,',\n",
       " '2004. 5',\n",
       " 'Unitary Evolution Recurrent Neural Networks\\nMartin Arjovsky\\x03MARJOVSKY @DC.UBA.AR\\nAmar Shah\\x03AS793@ CAM .AC.UK\\nYoshua Bengio\\nUniversidad de Buenos Aires, University of Cambridge,\\nUniversit ´e de Montr ´eal. Yoshua Bengio is a CIFAR Senior Fellow.\\n\\x03Indicates ﬁrst authors. Ordering determined by coin ﬂip.\\nAbstract\\nRecurrent neural networks (RNNs) are notoriously difﬁcult to train. When the eigenvalues\\nof the hidden to hidden weight matrix deviate\\nfrom absolute value 1, optimization becomes difﬁcult due to the well studied issue of vanishingandexploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture\\nthat learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an\\nexpressive unitary weight matrix by composing\\nseveral structured matrices that act as building\\nblocks with parameters to be learned. Optimization with this parameterization becomes feasible',\n",
       " 'expressive unitary weight matrix by composing\\nseveral structured matrices that act as building\\nblocks with parameters to be learned. Optimization with this parameterization becomes feasible\\nonly when considering hidden states in the complex domain. We demonstrate the potential of\\nthis architecture by achieving state of the art results in several hard tasks involving very longterm dependencies.\\n1. Introduction\\nDeep Neural Networks have shown remarkably good performance on a wide range of complex data problems including speech recognition (Hinton et al., 2012), image\\nrecognition (Krizhevsky et al., 2012) and natural language\\nprocessing (Collobert et al., 2011). However, training very\\ndeep models remains a difﬁcult task. The main issue surrounding the training of deep networks is the vanishing\\nandexploding gradients problems introduced by HochreProceedings of the 33rdInternational Conference on Machine\\nLearning , New York, NY , USA, 2016. JMLR: W&CP volume\\n48. Copyright 2016 by the author(s).iter (1991) and shown by Bengio et al. (1994) to be necessarily arising when trying to learn to reliably store bits\\nof information in any parametrized dynamical system. If\\ngradients propagated back through a network vanish, the\\ncredit assignment role of backpropagation is lost, as information about small changes in states in the far past has no',\n",
       " 'gradients propagated back through a network vanish, the\\ncredit assignment role of backpropagation is lost, as information about small changes in states in the far past has no\\ninﬂuence on future states. If gradients explode, gradientbased optimization algorithms struggle to traverse down a\\ncost surface, because gradient-based optimization assumes\\nsmall changes in parameters yield small changes in the objective function. As the number of time steps considered\\nin the sequence of states grows, the shrinking or expanding\\neffects associated with the state-to-state transformation at\\nindividual time steps can grow exponentially, yielding respectively vanishing or exploding gradients. See Pascanu\\net al. (2010) for a review.\\nAlthough the long-term dependencies problem appears\\nintractable in the absolute (Bengio et al., 1994) for\\nparametrized dynamical systems, several heuristics have\\nrecently been found to help reduce its effect, such as the\\nuse of self-loops and gating units in the LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Cho et al., 2014) recurrent architectures. Recent work also supports the idea\\nof using orthogonal weight matrices to assist optimization\\n(Saxe et al., 2014; Le et al., 2015).\\nIn this paper, we explore the use of orthogonal and unitary',\n",
       " '(Saxe et al., 2014; Le et al., 2015).\\nIn this paper, we explore the use of orthogonal and unitary\\nmatrices in recurrent neural networks. We start in Section 2\\nby showing a novel bound on the propagated gradients in\\nrecurrent nets when the recurrent matrix is orthogonal. Section 3 discusses the difﬁculties of parameterizing real valued orthogonal matrices and how they can be alleviated by\\nmoving to the complex domain.\\nWe discuss a novel approach to constructing expressive\\nunitary matrices as the composition of simple unitary matrices which require at most O(nlogn)computation and\\nO(n)memory, when the state vector has dimension n.\\nThese are unlike general matrices, which require O(n2)\\ncomputation and memory. Complex valued representationsarXiv:1511.06464v4  [cs.LG]  25 May 2016\\nUnitary Evolution Recurrent Neural Networks\\nhave been considered for neural networks in the past, but\\nwith limited success and adoption (Hirose, 2003; Zimmermann et al., 2011). We hope our ﬁndings will change this.\\nWhilst our model uses complex valued matrices and parameters, all implementation and optimization is possible\\nwith real numbers and has been done in Theano (Bergstra',\n",
       " 'Whilst our model uses complex valued matrices and parameters, all implementation and optimization is possible\\nwith real numbers and has been done in Theano (Bergstra\\net al., 2010). This along with other implementation details\\nare discussed in Section 4, and the code used for the experiments is available online. The potential of the developed\\nmodel for learning long term dependencies with relatively\\nfew parameters is explored in Section 5. We ﬁnd that the\\nproposed architecture generally outperforms LSTMs and\\nprevious approaches based on orthogonal initialization.\\n2. Orthogonal Weights and Bounding the\\nLong-Term Gradient\\nA matrix, W, is orthogonal if W>W=WW>=I.\\nOrthogonal matrices have the property that they preserve\\nnorm (i.e.kWhk2=khk2) and hence repeated iterative\\nmultiplication of a vector by an orthogonal matrix leaves\\nthe norm of the vector unchanged.\\nLethTandhtbe the hidden unit vectors for hidden layers\\nTandtof a neural network with Thidden layers and T\\x1d\\nt. IfCis the objective we are trying to minimize, then\\nthevanishing andexploding gradient problems refer to the\\ndecay or growth of@C\\n@htas the number of layers, T, grows.\\nLet\\x1bbe a pointwise nonlinearity function, and',\n",
       " 'decay or growth of@C\\n@htas the number of layers, T, grows.\\nLet\\x1bbe a pointwise nonlinearity function, and\\nzt+1=Wtht+Vtxt+1\\nht+1=\\x1b(zt+1) (1)\\nthen by the chain rule\\n@C\\n@ht=@C\\n@hT@hT\\n@ht\\n=@C\\n@hTT\\x001Y\\nk=t@hk+1\\n@hk=@C\\n@hTT\\x001Y\\nk=tDk+1WT\\nk (2)\\nwhere Dk+1=diag(\\x1b0(zk+1))is the Jacobian matrix of\\nthe pointwise nonlinearity.\\nIn the following we deﬁne the norm of a matrix to refer to\\nthe spectral radius norm (or operator 2-norm) and the norm\\nof a vector to mean L2-norm. By deﬁnition of the operator norms, for any matrices A;Band vectorvwe have\\nkAvk\\x14k AkkvkandkABk\\x14k AkkBk. If the weight\\nmatrices Wkare norm preserving (i.e. orthogonal), thenweprove\\n\\r\\r\\r\\r@C',\n",
       " 'matrices Wkare norm preserving (i.e. orthogonal), thenweprove\\n\\r\\r\\r\\r@C\\n@ht\\r\\r\\r\\r=\\r\\r\\r\\r\\r@C\\n@hTT\\x001Y\\nk=tDk+1WT\\nk\\r\\r\\r\\r\\r\\n\\x14\\r\\r\\r\\r@C\\n@hT\\r\\r\\r\\rT\\x001Y\\nk=t\\r\\rDk+1WT\\nk\\r\\r\\n=\\r\\r\\r\\r@C\\n@hT\\r\\r\\r\\rT\\x001Y\\nk=tkDk+1k: (3)\\nSince Dkis diagonal,kDkk= max j=1;:::;nj\\x1b0(z(j)\\nk)j,\\nwithz(j)\\nkthej-th pre-activation of the k-th hidden layer.\\nIf the absolute value of the derivative \\x1b0can take some\\nvalue\\x1c > 1, then this bound is useless, since k@C\\n@htk\\x14\\r\\r\\r@C\\n@hT\\r\\r\\r\\x1cT\\x00twhich grows exponentially in T. We therefore cannot effectively bound@C\\n@htfor deep networks, resulting potentially in exploding gradients.\\nIn the casej\\x1b0j< \\x1c < 1, equation 3 proves that that@C',\n",
       " '@htfor deep networks, resulting potentially in exploding gradients.\\nIn the casej\\x1b0j< \\x1c < 1, equation 3 proves that that@C\\n@httends to 0 exponentially fast as Tgrows, resulting in guaranteed vanishing gradients. This argument makes the rectiﬁed linear unit (ReLU) nonlinearity an attractive choice\\n(Glorot et al., 2011; Nair & Hinton, 2010). Unless all the\\nactivations are killed at one layer, the maximum entry of\\nDkis 1, resulting inkDkk= 1 for all layers k. With\\nReLU nonlinearities, we thus have\\n\\r\\r\\r\\r@C\\n@ht\\r\\r\\r\\r\\x14\\r\\r\\r\\r@C\\n@hT\\r\\r\\r\\rT\\x001Y\\nk=tkDk+1k=\\r\\r\\r\\r@C\\n@hT\\r\\r\\r\\r: (4)\\nMost notably, this result holds for a network of arbitrary\\ndepth and renders engineering tricks like gradient clipping\\nunnecessary (Pascanu et al., 2010).\\nTo the best of our knowledge, this analysis is a novel contribution and the ﬁrst time a neural network architecture has\\nbeen mathematically proven to avoid exploding gradients.\\n3. Unitary Evolution RNNs\\nUnitary matrices generalize orthogonal matrices to the',\n",
       " 'been mathematically proven to avoid exploding gradients.\\n3. Unitary Evolution RNNs\\nUnitary matrices generalize orthogonal matrices to the\\ncomplex domain. A complex valued, norm preserving matrix,U, is called a unitary matrix and is such that U\\x03U=\\nUU\\x03=I, where U\\x03is the conjugate transpose of U. Directly parametrizing the set of unitary matrices in such a\\nway that gradient-based optimization can be applied is not\\nstraightforward because a gradient step will typically yield\\na matrix that is not unitary, and projecting on the set of unitary matrices (e.g., by performing an eigendecomposition)\\ngenerally costsO(n3)computation when Uisn\\x02n.\\nThe most important feature of unitary and orthogonal matrices for our purpose is that they have eigenvalues \\x15jwith\\nUnitary Evolution Recurrent Neural Networks\\nabsolute value 1. The following lemma, proved in (Hoffman & Kunze, 1971), may shed light on a method which\\ncan be used to efﬁciently span a large set of unitary matrices.\\nLemma 1. A complex square matrix Wis unitary if and\\nonly if it has an eigendecomposition of the form W=',\n",
       " 'Lemma 1. A complex square matrix Wis unitary if and\\nonly if it has an eigendecomposition of the form W=\\nVDV\\x03, where\\x03denotes the conjugate transpose. Here,\\nV;D2Cn\\x02nare complex matrices, where Vis unitary,\\nandDis a diagonal such that jDj;jj= 1. Furthermore, W\\nis a real orthogonal matrix if and only if for every eigenvalue Dj;j=\\x15jwith eigenvector vj, there is also a complex conjugate eigenvalue \\x15k=\\x15jwith corresponding\\neigenvectorvk=vj.\\nWriting\\x15j=eiwjwithwj2R, a naive method to learn a\\nunitary matrix would be to ﬁx a basis of eigenvectors V2\\nCn\\x02nand set\\nW=VDV\\x03; (5)\\nwhere Dis a diagonal such that Dj;j=\\x15j.\\nLemma 1 informs us how to construct a real orthogonal\\nmatrix, W. We must (i) ensure the columns of Vcome\\nin complex conjugate pairs, vk=vj, and (ii) tie weights',\n",
       " 'matrix, W. We must (i) ensure the columns of Vcome\\nin complex conjugate pairs, vk=vj, and (ii) tie weights\\nwk=\\x00wjin order to achieve eiwj=eiwk. Most neural network objective functions are differentiable with respect to the weight matrices, and consequently wjmay be\\nlearned by gradient descent.\\nUnfortunately the above approach has undesirable properties. Fixing Vand learning wrequiresO\\x00\\nn2\\x01\\nmemory,\\nwhich is unacceptable given that the number of learned parameters isO(n). Further note that calculating Vufor an\\narbitrary vector urequiresO(n2)computation. Setting V\\nto the identity would satisfy the conditions of the lemma,\\nwhilst reducing memory and computation requirements to\\nO(n), however, Wwould remain diagonal, and have poor\\nrepresentation capacity.\\nWe propose an alternative strategy to parameterize unitary\\nmatrices. Since the product of unitary matrices is itself a\\nunitary matrix, we compose several simple, parameteric,\\nunitary matrices to construct a single, expressive unitary\\nmatrix. The four unitary building blocks considered are\\n\\x0fD, a diagonal matrix with Dj;j=eiwj, with parameterswj2R,\\n\\x0fR=I\\x002vv\\x03',\n",
       " '\\x0fD, a diagonal matrix with Dj;j=eiwj, with parameterswj2R,\\n\\x0fR=I\\x002vv\\x03\\nkvk2, a reﬂection matrix in the complex\\nvectorv2Cn,\\n\\x0f\\x05, a ﬁxed random index permutation matrix, and\\n\\x0f F andF\\x001, the Fourier and inverse Fourier transforms.Appealingly, D,Rand\\x05all permitO(n)storage and\\nO(n)computation for matrix vector products. FandF\\x001\\nrequire no storage and O(nlogn)matrix vector multiplication using the Fast Fourier Transform algorithm. A major\\nadvantage of composing unitary matrices of the form listed\\nabove, is that the number of parameters, memory and computational cost increase almost linearly in the size of the\\nhidden layer. With such a weight matrix, immensely large\\nhidden layers are feasible to train, whilst being impossible\\nin traditional neural networks.\\nWith this in mind, in this work we choose to consider recurrent neural networks with unitary hidden to hidden weight\\nmatrices. Our claim is that the ability to have large hidden\\nlayers where hidden states norms are preserved provides\\na powerful tool for modeling long term dependencies in\\nsequence data. (Bengio et al., 1994) suggest that having\\na large memory may be crucial for solving difﬁcult tasks',\n",
       " 'sequence data. (Bengio et al., 1994) suggest that having\\na large memory may be crucial for solving difﬁcult tasks\\nwith long ranging dependencies: the smaller the state dimension, the more information necessarily has to be eliminated when mapping a long sequence to a ﬁxed-dimension\\nstate.\\nWe call any RNN architecture which uses a unitary hidden\\nto hidden matrix a unitary evolution RNN (uRNN). After\\nexperimenting with several structures, we settled on the following composition\\nW=D3R2F\\x001D2\\x05R 1FD1: (6)\\nWhilst each but the permutation matrix is complex, we\\nparameterize and represent them with real numbers for implementation purposes. When the ﬁnal cost is real and differentiable, we may perform gradient descent optimization\\nto learn the parameters. (Yang et al., 2015) construct a real\\nvalued, non-orthogonal matrix using a similar parameterization with the motivation of parameter reduction by an\\norder of magnitude on an industrial sized network. This\\ncombined with earlier work (Le et al., 2010) suggests that it\\nis possible to create highly expressive matrices by composing simple matrices with few parameters. In the following\\nsection, we explain details on how to implement our model\\nand illustrate how we bypass the potential difﬁculties of\\nworking in the complex domain.',\n",
       " 'section, we explain details on how to implement our model\\nand illustrate how we bypass the potential difﬁculties of\\nworking in the complex domain.\\n4. Architecture details\\nIn this section, we describe the nonlinearity we used, how\\nwe incorporate real valued inputs with complex valued hidden units and map from complex hidden states to real outputs.\\n4.1. Complex hidden units\\nOur implementation represents all complex numbers using real values in terms of their real and imaginary parts.\\nUnitary Evolution Recurrent Neural Networks\\nUnder this framework, we sidestep the lack of support\\nfor complex numbers by most deep learning frameworks.\\nConsider multiplying the complex weight matrix W=\\nA+iBby the complex hidden vector h=x+iy,\\nwhere A;B;x;y are real. It is trivially true that Wh=\\n(Ax\\x00By) +i(Ay+Bx). When we represent v2Cnas\\x00\\nRe(v)>;Im(v)>\\x01>2R2n, we compute complex matrix\\nvector products with real numbers as follows\\n\\x12Re(Wh)\\nIm(Wh)\\x13\\n=\\x12A\\x00B\\nB A\\x13\\x12Re(h)\\nIm(h)\\x13\\n: (7)\\nMore generally, let f:Cn!Cnbe any complex function andz=x+iyany complex vector. We may write',\n",
       " 'Im(h)\\x13\\n: (7)\\nMore generally, let f:Cn!Cnbe any complex function andz=x+iyany complex vector. We may write\\nf(z) =\\x0b(x;y) +i\\x0c(x;y)where\\x0b;\\x0c :Rn!Rn. This\\nallows us to implement everything using real valued operations, compatible with any any deep learning framework\\nwith automatic differentiation such as Theano.\\n4.2. Input to Hidden, Nonlinearity, Hidden to Output\\nAs is the case with most recurrent networks, our uRNN follows the same hidden to hidden mapping as equation 1 with\\nVt=VandWt=W. Denote the size of the complex\\nvalued hidden states as nh. The input to hidden matrix is\\ncomplex valued, V2Cnh\\x02nin. We learn the initial hidden\\nstateh02Cnhas a parameter of the model.\\nChoosing an appropriate nonlinearity is not trivial in the\\ncomplex domain. As discussed in the introduction, using a\\nReLU is a natural choice in combination with a norm preserving weight matrix. We ﬁrst experimented with placing\\nseparate ReLU activations on the real and imaginary parts',\n",
       " 'ReLU is a natural choice in combination with a norm preserving weight matrix. We ﬁrst experimented with placing\\nseparate ReLU activations on the real and imaginary parts\\nof the hidden states. However, we found that such a nonlinearity usually performed poorly. Our intuition is that applying separate ReLU nonlinearities to the real and imaginary parts brutally impacts the phase of a complex number,\\nmaking it difﬁcult to learn structure.\\nWe speculate that maintaining the phase of hidden states\\nmay be important for storing information across a large\\nnumber of time steps, and our experiments supported this\\nclaim. A variation of the ReLU that we name modReLU ,\\nis what we ﬁnally chose. It is a pointwise nonlinearity,\\n\\x1bmodReLU (z) :C!C, which affects only the absolute\\nvalue of a complex number, deﬁned as\\n\\x1bmodReLU (z) =\\x1a(jzj+b)z\\njzjifjzj+b\\x150\\n0 ifjzj+b<0(8)\\nwhereb2Ris a bias parameter of the nonlinearity. For a\\nnhdimensional hidden space we learn nhnonlinearity bias\\nparameters, one per dimension. Note that the modReLU\\nis similar to the ReLU in spirit, in fact more concretely',\n",
       " 'parameters, one per dimension. Note that the modReLU\\nis similar to the ReLU in spirit, in fact more concretely\\n\\x1bmodReLU (z) =\\x1bReLU (jzj+b)z\\njzj.To map hidden states to output, we deﬁne a matrix U2\\nRno\\x022nh, wherenois the output dimension. We calculate\\na linear output as\\not=U\\x12Re(ht)\\nIm(ht)\\x13\\n+bo; (9)\\nwherebo2Rnois the output bias. The linear output is real\\nvalued (ot2Rno) and can be used for prediction and loss\\nfunction calculation akin to typical neural networks (e.g. it\\nmay be passed through a softmax which is used for cross\\nentropy calculation for classiﬁcation tasks).\\n4.3. Initialization\\nDue to the stability of the norm preserving operations of\\nour network, we found that performance was not very sensitive to initialization of parameters. For full disclosure\\nand reproducibility, we explain our initialization strategy\\nfor each parameter below.\\n\\x0fWe initialize VandU(the input and output matrices) as in (Glorot & Bengio, 2010),\\nwith weights sampled independently from uniforms,\\nUh\\n\\x00p\\n6pnin+nout;p\\n6pnin+nouti\\n.',\n",
       " 'with weights sampled independently from uniforms,\\nUh\\n\\x00p\\n6pnin+nout;p\\n6pnin+nouti\\n.\\n\\x0fThe biases,bandboare initialized to 0. This implies\\nthat at initialization, the network is linear with unitary\\nweights, which seems to help early optimization (Saxe\\net al., 2014).\\n\\x0fThe reﬂection vectors for R1andR2are initialized\\ncoordinate-wise from a uniform U[\\x001;1]. Note that\\nthe reﬂection matrices are invariant to scalar multiplication of the parameter vector, hence the width of the\\nuniform initialization is unimportant.\\n\\x0fThe diagonal weights for D1;D2andD3are sampled from a uniform, U[\\x00\\x19;\\x19]. This ensures that the\\ndiagonal entries Dj;jare sampled uniformly over the\\ncomplex unit circle.\\n\\x0fWe initialize h0with a uniform,Uh\\n\\x00q\\n3\\n2nh;q\\n3\\n2nhi\\n,\\nwhich results in E\\x02\\nkh0k2\\x03\\n= 1. Since the norm of\\nthe hidden units are roughly preserved through unitary\\nevolution and inputs are typically whitened to have\\nnorm 1, we have hidden states, inputs and linear outputs of the same order of magnitude, which seems to\\nhelp optimization.\\n5. Experiments',\n",
       " 'norm 1, we have hidden states, inputs and linear outputs of the same order of magnitude, which seems to\\nhelp optimization.\\n5. Experiments\\nIn this section we explore the performance of our uRNN\\nin relation to (a) RNN with tanh activations, (b) IRNN (Le\\net al., 2015), that is an RNN with ReLU activations and\\nwith the recurrent weight matrix initialized to the identity,\\nUnitary Evolution Recurrent Neural Networks\\nFigure 1. Results of the copying memory problem for time lags of 100;200;300;500. The LSTM is able to beat the baseline only for\\n100times steps. Conversely the uRNN is able to completely solve each time length in very few training iterations, without getting stuck\\nat the baseline.\\nand (c) LSTM (Hochreiter & Schmidhuber, 1997) models. We show that the uRNN shines quantitatively when\\nit comes to modeling long term dependencies and exhibits\\nqualitatively different learning properties to the other models.\\nWe chose a handful of tasks to evaluate the performance\\nof the various models. The tasks were especially created\\nto be be pathologically hard, and have been used as benchmarks for testing the ability of a model to capture long-term\\nmemory (Hochreiter & Schmidhuber, 1997; Le et al., 2015;',\n",
       " 'memory (Hochreiter & Schmidhuber, 1997; Le et al., 2015;\\nGraves et al., 2014; Martens & Sutskever, 2011)\\nOf the handful of optimization algorithms we tried on the\\nvarious models, RMSProp (Tieleman & Hinton, 2012) lead\\nto fastest convergence and is what we stuck to for all experiments here on in. However, we found the IRNN to be\\nparticularly unstable; it only ran without blowing up with\\nincredibly low learning rates and gradient clipping. Since\\nthe performance was so poor relative to other models we\\ncompare against, we do not show IRNN curves in the ﬁgures. In each experiment we use a learning rate of 10\\x003\\nand a decay rate of 0:9. For the LSTM and RNN models,\\nwe had to clip gradients at 1 to avoid exploding gradients.Gradient clipping was unnecessary for the uRNN.\\n5.1. Copying memory problem\\nRecurrent networks have been known to have trouble remembering information about inputs seen many time steps\\npreviously (Bengio et al., 1994; Pascanu et al., 2010). We\\ntherefore want to test the uRNN’s ability to recall exactly\\ndata seen a long time ago.\\nFollowing a similar setup to (Hochreiter & Schmidhuber,',\n",
       " 'data seen a long time ago.\\nFollowing a similar setup to (Hochreiter & Schmidhuber,\\n1997), we outline the copy memory task. Consider 10 categories,faig9\\ni=0. The input takes the form of a T+20 length\\nvector of categories, where we test over a range of values\\nofT. The ﬁrst 10entries are sampled uniformly, independently and with replacement from faig7\\ni=0, and represent\\nthe sequence which will need to be remembered. The next\\nT\\x001entries are set to a8, which can be thought of as the\\n’blank’ category. The next single entry is a9, which represents a delimiter, which should indicate to the algorithm\\nthat it is now required to reproduce the initial 10categories\\nin the output. The remaining 10entries are set to a8. The\\nrequired output sequence consists of T+ 10 repeated entries ofa8, followed by the ﬁrst 10categories of the input\\nUnitary Evolution Recurrent Neural Networks\\nFigure 2. Results of the adding problem for T= 100 ;200;400;750. The RNN with tanh is not able to beat the baseline for any time\\nlength. The LSTM and the uRNN show similar performance across time lengths, consistently beating the baseline.',\n",
       " 'length. The LSTM and the uRNN show similar performance across time lengths, consistently beating the baseline.\\nsequence in exactly the same order. The goal is to minimize the average cross entropy of category predictions at\\neach time step of the sequence. The task amounts to having to remember a categorical sequence of length 10, for T\\ntime steps.\\nA simple baseline can be established by considering an\\noptimal strategy when no memory is available, which we\\ndeem the memoryless strategy. The memoryless strategy\\nwould be to predict a8forT+ 10 entries and then predict\\neach of the ﬁnal 10categories from the set faig7\\ni=0independently and uniformly at random. The categorical cross\\nentropy of this strategy is10 log(8)\\nT+20.\\nWe ran experiments where the RNN with tanh activations,\\nIRNN, LSTM and uRNN had hidden layers of size 80, 80,\\n40 and 128 respectively. This equates to roughly 6500 parameters per model. In Figure 1, we see that aside from the\\nsimplest case, both the RNN with tanh and more surprisingly the LSTMs get almost exactly the same cost as the\\nmemoryless strategy. This behaviour is consistent with the\\nresults of (Graves et al., 2014), in which poor performance',\n",
       " 'memoryless strategy. This behaviour is consistent with the\\nresults of (Graves et al., 2014), in which poor performance\\nis reported for the LSTM for a very similar long term memory problem.\\nThe uRNN consistently achieves perfect performance inrelatively few iterations, even when having to recall sequences after 500 time steps. What is remarkable is that\\nthe uRNN does not get stuck at the baseline at all, whilst\\nthe LSTM and RNN do. This behaviour suggests that the\\nrepresentations learned by the uRNN have qualitatively different properties from both the LSTM and classical RNNs.\\n5.2. Adding Problem\\nWe closely follow the adding problem deﬁned in (Hochreiter & Schmidhuber, 1997) to explain the task at hand. Each\\ninput consists of two sequences of length T. The ﬁrst sequence, which we denote x, consists of numbers sampled\\nuniformly at random U[0;1]. The second sequence is an indicator sequence consisting of exactly two entries of 1 and\\nremaining entries 0. The ﬁrst 1 entry is located uniformly\\nat random in the ﬁrst half of the sequence, whilst the second 1 entry is located uniformly at random in the second\\nhalf. The output is the sum of the two entries of the ﬁrst sequence, corresponding to where the 1 entries are located in',\n",
       " 'half. The output is the sum of the two entries of the ﬁrst sequence, corresponding to where the 1 entries are located in\\nthe second sequence. A naive strategy of predicting 1 as the\\noutput regardless of the input sequence gives an expected\\nmean squared error of 0:167, the variance of the sum of two\\nindependent uniform distributions. This is our baseline to\\nbeat.\\nUnitary Evolution Recurrent Neural Networks\\nFigure 3. Results on pixel by pixel MNIST classiﬁcation tasks. The uRNN is able to converge in a fraction of the iterations that the\\nLSTM requires. The LSTM performs better on MNIST classiﬁcation, but the uRNN outperforms on the more complicated task of\\npermuted pixels.\\nWe chose to use 128 hidden units for the RNN with tanh,\\nIRNN and LSTM and 512 for the uRNN. This equates to\\nroughly 16K parameters for the RNN with tanh and IRNN,\\n60K for the LSTM and almost 9K for the uRNN. All models were trained using batch sizes of 20 and 50 with the best\\nresults being reported. Our results are shown in Figure 2.\\nThe LSTM and uRNN models are able to convincingly beat\\nthe baseline up to T= 400 time steps. Both models do well',\n",
       " 'The LSTM and uRNN models are able to convincingly beat\\nthe baseline up to T= 400 time steps. Both models do well\\nwhenT= 750 , but the mean squared error does not reach\\nclose to 0. The uRNN achieves lower test error, but it’s\\ncurve is more noisy. Despite having vastly more parameters, we monitored the LSTM performance to ensure no\\noverﬁtting.\\nThe RNN with tanh and IRNN were not able to beat the\\nbaseline for any number of time steps. (Le et al., 2015) report that their RNN solve the problem for T= 150 and the\\nIRNN forT= 300 , but they require over a million iterations before they start learning. Neither of the two models came close to either the uRNN or the LSTM in performance. The stark difference in our ﬁndings are best explained by our use of RMSprop with signiﬁcantly higher\\nlearning rates ( 10\\x003as opposed to 10\\x008) than (Le et al.,\\n2015) use for SGD with momentum.\\n5.3. Pixel-by-pixel MNIST\\nIn this task, suggested by (Le et al., 2015), algorithms are\\nfed pixels of MNIST (LeCun et al., 1998) sequentially and',\n",
       " 'In this task, suggested by (Le et al., 2015), algorithms are\\nfed pixels of MNIST (LeCun et al., 1998) sequentially and\\nrequired to output a class label at the end. We consider two\\ntasks: one where pixels are read in order (from left to right,\\nbottom to top) and one where the pixels are all randomly\\npermuted using the same randomly generated permutation\\nmatrix. The same model architectures as for the addingproblem were used for this task, except we now use a softmax for category classiﬁcation. We ran the optimization\\nalgorithms until convergence of the mean categorical cross\\nentropy on test data, and plot test accuracy in Figure 3.\\nBoth the uRNN and LSTM perform applaudably well here.\\nOn the correct unpermuted MNIST pixels, the LSTM performs better, achieving 98.2 % test accurracy versus 95.1%\\nfor the uRNN. However, when we permute the ordering of\\nthe pixels, the uRNN dominates with 91.4% of accuracy in\\ncontrast to the 88% of the LSTM, despite having less than\\na quarter of the parameters. This result is state of the art on\\nthis task, beating the IRNN (Le et al., 2015), which reaches\\nclose to 82% after 1 million training iterations. Notice that',\n",
       " 'this task, beating the IRNN (Le et al., 2015), which reaches\\nclose to 82% after 1 million training iterations. Notice that\\nuRNN reaches convergence in less than 20 thousand iterations, while it takes the LSTM from 5 to 10 times as many\\nto ﬁnish learning.\\nPermuting the pixels of MNIST images creates many\\nlonger term dependencies across pixels than in the original pixel ordering, where a lot of structure is local. This\\nmakes it necessary for a network to learn and remember\\nmore complicated dependencies across varying time scales.\\nThe results suggest that the uRNN is better able to deal\\nwith such structure over the data, where the LSTM is better\\nsuited to more local sequence structure tasks.\\n5.4. Exploratory experiments\\nNorms of hidden state gradients. As discussed in Section 2, key to being able to learn long term dependencies\\nis in controlling@C\\n@ht. With this in mind, we explored how\\neach model propagated gradients, by examining\\r\\r\\r@C\\n@ht\\r\\r\\ras\\na function of t. Gradient norms were computed at the beUnitary Evolution Recurrent Neural Networks\\nFigure 4. From left to right. Norms of the gradients with respect to hidden states i.e.\\r\\r\\r@C',\n",
       " 'Figure 4. From left to right. Norms of the gradients with respect to hidden states i.e.\\r\\r\\r@C\\n@ht\\r\\r\\rat (i) beginning of training, (ii) after 100\\niterations. (iii) Norms of the hidden states and (iv) L2distance between hidden states and ﬁnal hidden state. The gradient norms of\\nuRNNs do not decay as fast as for other models as training progresses. uRNN hidden state norms stay much more consistent over time\\nthan the LSTM. LSTM hidden states stay almost the same after a number of time steps, suggesting that it is not able to use new input\\ninformation.\\nginning of training and again after 100 iterations of training\\non the adding problem. The curves are plotted in Figure 4.\\nIt is clear that at ﬁrst, the uRNN propagates gradients perfectly, while each other model has exponentially vanishing\\ngradients. After 100 iterations of training, each model experiences vanishing gradients, but the uRNN is best able to\\npropagate information, having much less decay.\\nHidden state saturation. We claim that typical recurrent\\narchitectures saturate, in the sense that after they acquire\\nsome information, it becomes much more difﬁcult to acquire further information pertaining to longer dependencies. We took the uRNN and LSTM models trained on',\n",
       " 'some information, it becomes much more difﬁcult to acquire further information pertaining to longer dependencies. We took the uRNN and LSTM models trained on\\nthe adding problem with T= 200 , and computed a forward pass with newly generated data for the adding problem withT= 1000 . In order to show saturation effects,\\nwe plot the norms of the hidden states and the L2distance\\nbetween each state and the last in Figure 4.\\nIn our experiments, it is clear that the uRNN does not suffer\\nas much as other models do. Notice that whilst the norms of\\nhidden states in the uRNN grow very steadily over time, in\\nthe LSTM they grow very fast, and then stay constant after\\nabout 500time steps. This behaviour may suggest that the\\nLSTM hidden states saturate in their ability to incorporate\\nnew information, which is vital for modeling long complicated sequences. It is interesting to see that the LSTM\\nhidden state at t= 500 , is close to that of t= 1000 , whilst\\nthis is far from the case in the uRNN. Again, this suggests\\nthat the LSTM’s capacity to use new information to alter\\nits hidden state severly degrades with sequence length. The\\nuRNN does not suffer from this difﬁculty nearly as badly.\\nA clear example of this phenomenon was observed in the',\n",
       " 'uRNN does not suffer from this difﬁculty nearly as badly.\\nA clear example of this phenomenon was observed in the\\nadding problem with T= 750 . We found that the Pearson\\ncorrelation between the LSTM output prediction and the\\nﬁrst of the two uniform samples (whose sum is the target\\noutput) was\\x1a= 0:991. This suggests that the LSTM learnt\\nto simply ﬁnd and store the ﬁrst sample, as it was unable\\nto incorporate any more information by the time it reachedthe second, due to saturation of the hidden states.\\n6. Discussion\\nThere are a plethora of further ideas that may be explored\\nfrom our ﬁndings, both with regards to learning representation and efﬁcient implementation. For example, one hurdle\\nof modeling long sequences with recurrent networks is the\\nrequirement of storing all hidden state values for the purpose of gradient backpropagation. This can be prohibitive,\\nsince GPU memory is typically a limiting factor of neural\\nnetwork optimization. However, since our weight matrix is\\nunitary, its inverse is its conjugate transpose, which is just\\nas easy to operate with. If further we were to use an invertible nonlinearity function, we would no longer need to store',\n",
       " 'as easy to operate with. If further we were to use an invertible nonlinearity function, we would no longer need to store\\nhidden states, since they can be recomputed in the backward pass. This could have potentially huge implications,\\nas we would be able to reduce memory usage by an order\\nofT, the number of time steps. This would make having\\nimmensely large hidden layers possible, perhaps enabling\\nvast memory representations.\\nIn this paper we demonstrate state of the art performance\\non hard problems requiring long term reasoning and memory. These results are based on a novel parameterization\\nof unitary matrices which permit efﬁcient matrix computations and parameter optimization. Whilst complex domain modeling has been widely succesful in the signal processing community (e.g. Fourier transforms, wavelets), we\\nhave yet to exploit the power of complex valued representation in the deep learning community. Our hope is that\\nthis work will be a step forward in this direction. We motivate the idea of unitary evolution as a novel way to mitigate\\nthe problems of vanishing and exploding gradients. Empirical evidence suggests that our uRNN is better able to pass\\ngradient information through long sequences and does not\\nsuffer from saturating hidden states as much as LSTMs,\\ntypical RNNs, or RNNs initialized with the identity weight\\nmatrix (IRNNs).',\n",
       " 'suffer from saturating hidden states as much as LSTMs,\\ntypical RNNs, or RNNs initialized with the identity weight\\nmatrix (IRNNs).\\nUnitary Evolution Recurrent Neural Networks\\nAcknowledgments : We thank the developers of\\nTheano (Bergstra et al., 2010) for their great work.\\nWe thank NSERC, Compute Canada, Canada Research\\nChairs and CIFAR for their support. We would also like\\nto thank C ¸ aglar Gulc ¸ehre, David Krueger, Soroush Mehri,\\nMarcin Moczulski, Mohammad Pezeshki and Saizheng\\nZhang for helpful discussions, comments and code sharing.\\nReferences\\nBengio, Yoshua, Simard, Patrice, and Frasconi, Paolo.\\nLearning long-term dependencies with gradient descent\\nis difﬁcult. IEE Transactions on Neural Networks , 5,\\n1994.\\nBergstra, James, Breuleux, Olivier, Bastien, Fr ´ed´eric,\\nLamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression',\n",
       " 'compiler. Proceedings of the Python for Scientiﬁc Computing Conference (SciPy) , 2010.\\nCho, Kyunghyun, van Merri ¨enboer, Bart, Bahdanau,\\nDzmitry, and Bengio, Yoshua. On the properties of neural machine translation: Encoder–Decoder approaches.\\nInEighth Workshop on Syntax, Semantics and Structure\\nin Statistical Translation , October 2014.\\nCollobert, Ronan, Weston, Jason, Bottou, L ´eon, Karlen,\\nMichael, Kavukcuoglu, Koray, and Kuksa, Pavel. Natural language processing (almost) from scratch. Journal\\nof Machine Learning Research , 12:2493–2537, 2011.\\nGlorot, Xavier and Bengio, Yoshua. Understanding the\\ndifﬁculty of training deep feedforward neural networks.\\nInternational Conference on Artiﬁcial Intelligence and\\nStatistics (AISTATS) , 2010.\\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua.\\nDeep sparse rectiﬁer neural networks. International\\nConference on Artiﬁcial Intelligence and Statistics (AISTATS) , 2011.\\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural',\n",
       " 'Conference on Artiﬁcial Intelligence and Statistics (AISTATS) , 2011.\\nGraves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural\\nturing machines. arXiv preprint arXiv:1410.5401 , 2014.\\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew,\\nVanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara, and\\nKingsbury, Brian. Deep neural networks for acoustic\\nmodeling in speech recognition. Signal Processing Magazine , 2012.\\nHirose, Akira. Complex-valued neural networks: theories\\nand applications , volume 5. World Scientiﬁc Publishing\\nCompany Incorporated, 2003.Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. M ¨unich, 1991.\\nHochreiter, Sepp and Schmidhuber, J ¨urgen. Long shortterm memory. Neural Computation , 8(9):1735–1780,\\n1997.\\nHoffman, Kenneth and Kunze, Ray. Linear Algebra . Pearson, second edition, 1971.',\n",
       " '1997.\\nHoffman, Kenneth and Kunze, Ray. Linear Algebra . Pearson, second edition, 1971.\\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\\nImagenet classiﬁcation with deep convolutional neural networks. Neural Information Processing Systems ,\\n2012.\\nLe, Quoc, Sarl ´os, Tam ´as, and Smola, Alex. Fastfood - approximating kernel expansions in loglinear time. International Conference on Machine Learning , 2010.\\nLe, Quoc V ., Navdeep, Jaitly, and Hinton, Geoffrey E. A\\nsimple way to initialize recurrent networks of rectiﬁed\\nlinear units. arXiv preprint arXiv:1504.00941 , 2015.\\nLeCun, Yann, Bottou, L ´eon, Bengio, Yoshua, and Haffner,\\nPatrick. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE , 1998.\\nMartens, James and Sutskever, Ilya. Learning recurrent\\nneural networks with hessian-free optimization. International Conference on Machine Learning , 2011.\\nNair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units\\nimprove restricted boltzmann machines. International\\nConference on Machine Learning , 2010.',\n",
       " 'Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units\\nimprove restricted boltzmann machines. International\\nConference on Machine Learning , 2010.\\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\\nOn the difﬁculty of training recurrent neural networks.\\nInternational Conference on Machine Learning , 2010.\\nSaxe, Andrew M., McLelland, James L., and Ganguli,\\nSurya. Exact solutions to the nonlinear dynamics of\\nlearning in deep linear neural networks. International\\nConference in Learning Representations , 2014.\\nTieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5rmsprop: Divide the gradient by a running average of\\nits recent magnitude. Coursera: Neural Networks for\\nMachine Learning , 2012.\\nYang, Zichao, Moczulski, Marcin, Denil, Misha, de Freitas, Nando, Smola, Alex, Song, Le, and Wang, Ziyu.\\nDeep fried convnets. International Conference on Computer Vision (ICCV) , 2015.\\nZimmermann, Hans-Georg, Minin, Alexey, and\\nKusherbaeva, Victoria. Comparison of the complex valued and real valued neural networks trained\\nwith gradient descent and random search algorithms. In\\nESANN , 2011.',\n",
       " 'Higher Order Conditional Random Fields in Deep\\nNeural Networks\\nAnurag Arnab, Sadeep Jayasumana, Shuai Zheng, Philip H.S. Torr\\nUniversity of Oxford\\n{firstname.lastname }@eng.ox.ac.uk\\nAbstract. We address the problem of semantic segmentation using deep learning.\\nMost segmentation systems include a Conditional Random Field (CRF) to produce\\na structured output that is consistent with the image’s visual features. Recent deep\\nlearning approaches have incorporated CRFs into Convolutional Neural Networks\\n(CNNs), with some even training the CRF end-to-end with the rest of the network.\\nHowever, these approaches have not employed higher order potentials, which\\nhave previously been shown to signiﬁcantly improve segmentation performance.\\nIn this paper, we demonstrate that two types of higher order potential, based on\\nobject detections and superpixels, can be included in a CRF embedded within a\\ndeep network. We design these higher order potentials to allow inference with\\nthe differentiable mean ﬁeld algorithm. As a result, all the parameters of our\\nricher CRF model can be learned end-to-end with our pixelwise CNN classiﬁer.\\nWe achieve state-of-the-art segmentation performance on the PASCAL VOC',\n",
       " 'We achieve state-of-the-art segmentation performance on the PASCAL VOC\\nbenchmark with these trainable higher order potentials.\\nKeywords: Semantic Segmentation, Conditional Random Fields, Deep Learning,\\nConvolutional Neural Networks\\n1 Introduction\\nSemantic segmentation involves assigning a visual object class label to every pixel in an\\nimage, resulting in a segmentation with a semantic meaning for each segment. While a\\nstrong pixel-level classiﬁer is critical for obtaining high accuracy in this task, it is also\\nimportant to enforce the consistency of the semantic segmentation output with visual\\nfeatures of the image. For example, segmentation boundaries should usually coincide\\nwith strong edges in the image, and regions in the image with similar appearance should\\nhave the same label.\\nRecent advances in deep learning have enabled researchers to create stronger classiﬁers, with automatically learned features, within a Convolutional Neural Network\\n(CNN) [1 –3]. This has resulted in large improvements in semantic segmentation accuracy on widely used benchmarks such as PASCAL VOC [4]. CNN classiﬁers are now\\nconsidered the standard choice for pixel-level classiﬁers used in semantic segmentation.\\nOn the other hand, probabilistic graphical models have long been popular for structured prediction of labels, with constraints enforcing label consistency. Conditional',\n",
       " 'On the other hand, probabilistic graphical models have long been popular for structured prediction of labels, with constraints enforcing label consistency. Conditional\\nRandom Fields (CRFs) have been the most common framework, and various rich and\\narXiv:1511.08119v4  [cs.CV]  29 Jul 2016',\n",
       " 'Higher Order Conditional Random Fields in Deep Neural Networks 3\\nWe evaluate our higher order potentials on the PASCAL VOC 2012 semantic segmentation benchmark as well as the PASCAL Context dataset, to show signiﬁcant\\nimprovements over our baseline and achieve state-of-the art results.\\n2 Related Work\\nBefore deep learning became prominent, semantic segmentation was performed with\\ndense hand-crafted features which were fed into a per-pixel or region classiﬁer [14]. The\\nindividual predictions made by these classiﬁers were often noisy as they lacked global\\ncontext, and were thus post-processed with a CRF, making use of prior knowledge such\\nas the fact that nearby pixels, as well as pixels of similar appearance, are likely to share\\nthe same class label [14, 15].\\nThe CRF model of [14] initially contained only unary and pairwise terms in an\\n8-neighbourhood, which [16] showed can result in shrinkage bias. Numerous improvements to this model were subsequently proposed including: densely connected pairwise\\npotentials facilitating interactions between all pairs of image pixels [17], formulating\\nhigher order potentials deﬁned over cliques larger than two nodes [5, 16] in order to\\ncapture more context, modelling co-occurrence of object classes [18 –20], and utilising',\n",
       " 'capture more context, modelling co-occurrence of object classes [18 –20], and utilising\\nthe results of object detectors [6, 21, 22].\\nRecent advances in deep learning have allowed us to replace hand-crafted features\\nwith features learned speciﬁcally for semantic segmentation. The strength of these\\nrepresentations was illustrated by [3] who achieved signiﬁcant improvements over\\nprevious hand-crafted methods without using any CRF post-processing. Chen et al. [12]\\nshowed further improvements by post-processing the results of a CNN with a CRF.\\nSubsequent works [9 –11, 23] have taken this idea further by incorporating a CRF as\\nlayers within a deep network and then learning parameters of both the CRF and CNN\\ntogether via backpropagation.\\nIn terms of enhancements to conventional CRF models, Ladicky et al. [6] proposed using an off-the-shelf object detector to provide additional cues for semantic\\nsegmentation. Unlike other approaches that reﬁne a bounding-box detection to produce\\na segmentation [8, 24], this method used detector outputs as a soft constraint and can\\nthus recover from object detection errors. Their formulation, however, used graph-cut\\ninference, which was only tractable due to the absence of dense pairwise potentials.',\n",
       " 'thus recover from object detection errors. Their formulation, however, used graph-cut\\ninference, which was only tractable due to the absence of dense pairwise potentials.\\nObject detectors have also been used by [21, 25], who also modelled variables that\\ndescribe the degree to which an object hypothesis is accepted.\\nWe formulate the detection potential in a different manner to [6, 21, 25] so that it is\\namenable to mean ﬁeld inference. Mean ﬁeld permits inference with dense pairwise connections, which results in substantial accuracy improvements [10,12, 17]. Furthermore,\\nmean ﬁeld updates related to our potentials are differentiable and its parameters can thus\\nbe learned in our end-to-end trainable architecture.\\nWe also note that while the semantic segmentation problem has mostly been formulated in terms of pixels [3,10,14], some have expressed it in terms of superpixels [26 –28].\\nSuperpixels can capture more context than a single pixel and computational costs can\\nalso be reduced if one considers pairwise interactions between superpixels rather than individual pixels [21]. However, such superpixel representations assume that the segments\\n4 Arnab et al.\\nshare boundaries with objects in an image, which is not always true. As a result, several',\n",
       " '4 Arnab et al.\\nshare boundaries with objects in an image, which is not always true. As a result, several\\nauthors [5, 7] have employed higher order potentials deﬁned over superpixels that encourage label consistency over regions, but do not strictly enforce it. This approach also\\nallows multiple, non-hierarchical layers of superpixels to be integrated. Our formulation\\nuses this kind of higher order potential, but in an end-to-end trainable CNN.\\nGraphical models have been used with CNNs in other areas besides semantic segmentation, such as in pose-estimation [29] and group activity recognition [30]. Alternatively,\\nIonescu et al. [31] incorporated structure into a deep network with structured matrix\\nlayers and matrix backpropagation. However, the nature of models used in these works\\nis substantially different to ours. Some early works that advocated gradient backpropagation through graphical model inference for parameter optimisation include [32, 33]\\nand [34].\\nOur work differentiates from the above works since, to our knowledge, we are the ﬁrst\\nto propose and conduct a thorough experimental investigation of higher order potentials\\nthat are based on detection outputs and superpixel segmentation in a CRF which is\\nlearned end-to-end in a deep network. Note that although [7] formulated mean ﬁeld',\n",
       " 'learned end-to-end in a deep network. Note that although [7] formulated mean ﬁeld\\ninference with higher order potentials, they did not consider object detection potentials\\nat all, nor were the parameters learned.\\n3 Conditional Random Fields\\nWe now review conditional random ﬁelds used in semantic segmentation and introduce\\nthe notation used in the paper. Take an image IwithNpixels, indexed 1,2,...,N . In\\nsemantic segmentation, we attempt to assign every pixel a label from a predeﬁned set of\\nlabels L={l1,l2,...,l L}. Deﬁne a set of random variables X1,X2,...,X N, one for\\neach pixel, where each Xi∈ L. LetX= [X1X2... X N]T. Any particular assignment\\nxtoXis thus a solution to the semantic segmentation problem.\\nWe use notations {V}, andV(i)to represent the set of elements of a vector V, and\\ntheithelement of V, respectively. Given a graph Gwhere the vertices are from {X}\\nand the edges deﬁne connections among these variables, the pair (I,X)is modelled as a',\n",
       " 'and the edges deﬁne connections among these variables, the pair (I,X)is modelled as a\\nCRF characterised by Pr(X=x|I) = (1/Z(I)) exp( −E(x|I)), whereE(x|I)is the\\nenergy of the assignment xandZ(I)is the normalisation factor known as the partition\\nfunction. We drop the conditioning on Ihereafter to keep the notation uncluttered. The\\nenergyE(x)of an assignment is deﬁned using the set of cliques Cin the graph G. More\\nspeciﬁcally, E(x) =/summationtext\\nc∈Cψc(xc),where xcis a vector formed by selecting elements\\nofxthat correspond to random variables belonging to the clique c, andψc(.)is the\\ncost function for the clique c. The function, ψc(.), usually uses prior knowledge about a\\ngood segmentation, as well as information from the image, the observation the CRF is\\nconditioned on.\\nMinimising the energy yields the maximum a posteriori (MAP) labelling of the\\nimage i.e.the most probable label assignment given the observation (image). When\\ndense pairwise potentials are used in the CRF to obtain higher accuracy, exact inference',\n",
       " 'image i.e.the most probable label assignment given the observation (image). When\\ndense pairwise potentials are used in the CRF to obtain higher accuracy, exact inference\\nis impracticable, and one has to resort to an approximate inference method such as mean\\nﬁeld inference [17]. Mean ﬁeld inference is particularly appealing in a deep learning\\nsetting since it is possible to formulate it as a Recurrent Neural Network [10].\\nHigher Order Conditional Random Fields in Deep Neural Networks 5\\n4 CRF with Higher Order Potentials\\nMany CRF models that have been incorporated into deep learning frameworks [10, 12]\\nhave so far used only unary and pairwise potentials. However, potentials deﬁned on\\nhigher order cliques have been shown to be useful in previous works such as [7, 16]. The\\nkey contribution of this paper is to show that a number of explicit higher order potentials\\ncan be added to CRFs to improve image segmentation, while staying compatible with\\ndeep learning. We formulate these higher order potentials in a manner that mean ﬁeld\\ninference can still be used to solve the CRF. Advantages of mean ﬁeld inference are\\ntwofold: First, it enables efﬁcient inference when using densely-connected pairwise',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"chunk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = \"5c83f505-ce5d-4790-ad9f-3ab2a148fbd8\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m embed_model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m res \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      4\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      5\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mWe would have some text to embed here\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mAnd maybe another chunk here too\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m     ], engine\u001b[39m=\u001b[39;49membed_model_id\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/openai/api_resources/embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "embed_model_id = \"text-embedding-ada-002\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[\n",
    "        \"We would have some text to embed here\",\n",
    "        \"And maybe another chunk here too\"\n",
    "    ], engine=embed_model_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone with your API key\n",
    "pinecone_instance = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# check if index already exists (it shouldn't if this is first time)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m index_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pinecone_instance\u001b[39m.\u001b[39mlist_indexes():\n\u001b[1;32m      5\u001b[0m     \u001b[39m# if does not exist, create index\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     pinecone_instance\u001b[39m.\u001b[39mcreate_index(\n\u001b[1;32m      7\u001b[0m         index_name,\n\u001b[0;32m----> 8\u001b[0m         dimension\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(res[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m      9\u001b[0m         metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     \u001b[39m# wait for index to be initialized\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m pinecone_instance\u001b[39m.\u001b[39mdescribe_index(index_name)\u001b[39m.\u001b[39mstatus[\u001b[39m'\u001b[39m\u001b[39mready\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "index_name = \"nemo-guardrails-rag-with-actions\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone_instance.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone_instance.create_index(\n",
    "        index_name,\n",
    "        dimension=len(res['data'][0]['embedding']),\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pinecone_instance.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pinecone_instance.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(query: str) -> list:\n",
    "    # create query embedding\n",
    "    res = openai.Embedding.create(input=[query], engine=embed_model_id)\n",
    "    xq = res['data'][0]['embedding']\n",
    "    # get relevant contexts from pinecone\n",
    "    res = index.query(xq, top_k=5, include_metadata=True)\n",
    "    # get list of retrieved texts\n",
    "    contexts = [x['metadata']['chunk'] for x in res['matches']]\n",
    "    return contexts\n",
    "\n",
    "async def rag(query: str, contexts: list) -> str:\n",
    "    print(\"> RAG Called\")  # we'll add this so we can see when this is being used\n",
    "    context_str = \"\\n\".join(contexts)\n",
    "    # place query and contexts into RAG prompt\n",
    "    prompt = f\"\"\"You are a helpful assistant, below is a query from a user and\n",
    "    some relevant contexts. Answer the question given the information in those\n",
    "    contexts. If you cannot find the answer to the question, say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {context_str}\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Answer: \"\"\"\n",
    "    # generate answer\n",
    "    res = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.0,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return res['choices'][0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = \"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: text-davinci-003\n",
    "\"\"\"\n",
    "\n",
    "rag_colang_content = \"\"\"\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a personal assistant, I don't like to talk of politics.\"\n",
    "\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\n",
    "# define RAG intents and flow\n",
    "define user ask llms\n",
    "    \"tell me about llama 2?\"\n",
    "    \"what is large language model\"\n",
    "    \"where did meta's new model come from?\"\n",
    "    \"what is the falcon model?\"\n",
    "    \"have you ever meta llama?\"\n",
    "\n",
    "define flow llms\n",
    "    user ask llms\n",
    "    $contexts = execute retrieve(query=$last_user_message)\n",
    "    $answer = execute rag(query=$last_user_message, contexts=$contexts)\n",
    "    bot $answer\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosmayorga/anaconda3/envs/AI_FP/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe34cd5ed2c84340bebc3ecc43106b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3ad182d7f249cba2aeeab019936794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a029b80bc24cea8b4b09cf43332e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3ad2f7fcb24a25b92bada884d70ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0697fd57c314988b60242035d79a006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f12172be0549f1b120cf08520f5ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    colang_content=rag_colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "# create rails\n",
    "rag_rails = LLMRails(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_rails.register_action(action=retrieve, name=\"retrieve\")\n",
    "rag_rails.register_action(action=rag, name=\"rag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rag_colang_content = \"\"\"\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a shopping assistant, I don't like to talk of politics.\"\n",
    "    \"Sorry I can't talk about politics!\"\n",
    "\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\"\"\"\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    colang_content=no_rag_colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "# create rails\n",
    "no_rag_rails = LLMRails(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask about Llama 2 again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await no_rag_rails.generate_async(prompt=\"tell me about llama 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not the Llama 2 we were looking for — let's try some more questions and compare RAG vs. no-RAG answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no RAG\n",
    "await no_rag_rails.generate_async(\n",
    "    prompt=\"what was red teaming used for in llama 2 training?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our no RAG rails provide an exciting but utterly wrong answer. Let's try the same with our RAG-enabled rails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with RAG\n",
    "await rag_rails.generate_async(\n",
    "    prompt=\"what was red teaming used for in llama 2 training?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_FP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
