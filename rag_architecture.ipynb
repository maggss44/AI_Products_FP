{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carlosmayorga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from typing import List, Tuple, Callable, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Carga el archivo .env\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "data = data.map(lambda x: {\n",
    "    'uid': f\"{x['doi']}-{x['chunk-id']}\"\n",
    "})\n",
    "\n",
    "data = data[\"chunk\"]\n",
    "\n",
    "filtered_data = [text for text in data if len(text) <= 700]\n",
    "\n",
    "# Calcular el 30% del nÃºmero total de elementos\n",
    "num_elements = len(filtered_data)\n",
    "num_to_retain = int(num_elements * 0.5)\n",
    "\n",
    "# Seleccionar aleatoriamente el 30% de los elementos\n",
    "data = random.sample(filtered_data, num_to_retain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [\n",
    "#     \"Carbon trading is a market-based approach to reducing greenhouse gas emissions by providing economic incentives for companies to limit their carbon footprint.\",\n",
    "#     \"In a carbon trading system, companies are allocated a certain number of carbon credits, which represent the right to emit a specific amount of carbon dioxide or other greenhouse gases.\",\n",
    "#     \"Companies that emit less than their allocated carbon credits can sell their excess credits to companies that exceed their emissions limits, creating a market for carbon credits.\",\n",
    "#     \"The goal of carbon trading is to encourage companies to invest in cleaner technologies and adopt more sustainable practices to reduce their emissions and avoid the cost of purchasing additional carbon credits.\",\n",
    "#     \"Environmental, Social, and Governance (ESG) criteria are a set of standards used by investors to evaluate a company's sustainability and ethical impact.\",\n",
    "#     \"ESG factors consider a company's environmental impact, such as its carbon footprint, waste management, and use of renewable energy.\",\n",
    "#     \"Social aspects of ESG include a company's labor practices, diversity and inclusion policies, and community engagement.\",\n",
    "#     \"Governance factors in ESG assess a company's leadership structure, executive compensation, and transparency in decision-making processes.\",\n",
    "#     \"Investors are increasingly using ESG criteria to identify companies that are better positioned to manage risks and opportunities related to sustainability and social responsibility.\",\n",
    "#     \"Companies with strong ESG performance tend to have better long-term financial prospects, as they are more resilient to environmental and social challenges and are favored by environmentally and socially conscious consumers.\",\n",
    "#     \"Carbon trading and ESG are closely related, as companies with lower carbon emissions and better sustainability practices tend to have higher ESG ratings.\",\n",
    "#     \"Governments and international organizations are promoting carbon trading and ESG investing as key strategies for mitigating climate change and transitioning to a low-carbon economy.\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource:\n",
    "    def __init__(self, data: List[str]):\n",
    "        \"\"\"\n",
    "        Initializes the DataSource object with a list of data.\n",
    "\n",
    "        Args:\n",
    "            data (List[str]): The input list of text data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, and removing extra whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        text = text.lower()  # Convert text to lowercase\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
    "        return text\n",
    "\n",
    "    def preprocess_text_advanced(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocesses the text by lowercasing, removing punctuation, removing extra whitespace,\n",
    "        removing numbers, removing stop words, and lemmatizing the words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed and cleaned text.\n",
    "        \"\"\"\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower() \n",
    "        # Remove punctuation and numeric characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize the text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Lemmatize the words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        # Join the tokens back into a string\n",
    "        text = ' '.join(tokens)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the preprocessed text into a list of words.\n",
    "\n",
    "        Args:\n",
    "            text (str): The preprocessed text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of tokens (words) from the text.\n",
    "        \"\"\"\n",
    "        return self.preprocess_text_advanced(text).split()  # Tokenize the preprocessed text by splitting on spaces\n",
    "\n",
    "    def process_data(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Processes the data by applying advanced preprocessing to each sentence in the data list.\n",
    "\n",
    "        Updates the object's processed_data attribute with the cleaned and preprocessed text.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.processed_data = [self.preprocess_text_advanced(sentence) for sentence in self.data] # Apply preprocessing to each sentence in the data list\n",
    "\n",
    "        return self.processed_data  # Apply preprocessing to each sentence in the data list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserQuery:\n",
    "    def __init__(self, query: str):\n",
    "        self.query = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, model_name: str, device: str = 'cpu', use_local: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the Embedding object with the specified model name, device, and whether to use a local model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the embedding model to be used.\n",
    "            device (str): The device to be used for running the model (\"cpu\" or \"cuda\"). Default is \"cpu\".\n",
    "            use_local (bool): Whether to use a locally available model. Default is False.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.use_local = use_local\n",
    "        # Mapping of model names to their expected dimensions\n",
    "        self.model_dimensions = {\n",
    "            'all-MiniLM-L6-v2': 384,  # Example dimension for a SentenceTransformer model\n",
    "            'jina-v2-base-en-embed': 768,  # Specified dimension for your local model\n",
    "            # Add other models and their dimensions here\n",
    "        }\n",
    "        self.current_model_dimension = self.model_dimensions.get(model_name, None)\n",
    "\n",
    "        if use_local:\n",
    "            raise ValueError(f\"Not yet implemented\")\n",
    "        else:\n",
    "            self.model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches the embedding model to a different model and device.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new embedding model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cpu\" or \"cuda\").\n",
    "        \"\"\"\n",
    "        # Here we assume the model switch is successful and just set the dimension\n",
    "        if model_name in self.model_dimensions:\n",
    "            self.current_model_dimension = self.model_dimensions[model_name]\n",
    "            print(f\"Switched to model '{model_name}' with dimension {self.current_model_dimension}\")\n",
    "        else:\n",
    "            print(f\"Model '{model_name}' not recognized. Unable to switch models.\")\n",
    "\n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Embeds the input text into a vector representation using the current embedding model.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: The vector representation of the input text.\n",
    "        \"\"\"\n",
    "        if self.use_local:\n",
    "          raise ValueError(f\"Not yet implemented!\")\n",
    "        else:\n",
    "            result = self.model.encode(text).tolist()\n",
    "            if len(result) != self.current_model_dimension:\n",
    "                print(f\"Dimension mismatch detected: Expected {self.current_model_dimension}, got {len(result)}\")\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStorage:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]]):\n",
    "        # Placeholder method for storing vectors\n",
    "        pass\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int) -> List[int]:\n",
    "        # Placeholder method for searching vectors\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeVectorStorage(VectorStorage):\n",
    "    def __init__(self, index_name: str, embedding: Embedding):\n",
    "        \"\"\"\n",
    "        Initializes the PineconeVectorStorage object with the specified index name and embedding model.\n",
    "\n",
    "        Args:\n",
    "            index_name (str): The name of the Pinecone index to be created or used.\n",
    "            embedding (Embedding): An instance of the Embedding class providing the embedding model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "        self.pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        if index_name not in self.pinecone.list_indexes().names():\n",
    "            self.pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=embedding.model.get_sentence_embedding_dimension(),\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "            )\n",
    "        self.index = self.pinecone.Index(index_name)\n",
    "\n",
    "    def store_vectors(self, vectors: List[List[float]], metadatas: List[dict]):\n",
    "        \"\"\"\n",
    "        Stores vectors and associated metadata in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            vectors (List[List[float]]): List of vectors to be stored in the index.\n",
    "            metadatas (List[dict]): List of dictionaries containing metadata associated with each vector.\n",
    "        \"\"\"\n",
    "        ids = [str(i) for i in range(len(vectors))]\n",
    "        records = zip(ids, vectors, metadatas)\n",
    "        self.index.upsert(vectors=records)\n",
    "\n",
    "    def search_vectors(self, query_vector: List[float], top_n: int, filter_metadata: Dict[str, str] = None) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Searches for vectors similar to the given query vector in the Pinecone index.\n",
    "\n",
    "        Args:\n",
    "            query_vector (List[float]): The query vector for similarity search.\n",
    "            top_n (int): The number of top results to return.\n",
    "            filter_metadata (Dict[str, str], optional): Metadata filters to apply during search. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: List of dictionaries containing metadata of the top similar vectors.\n",
    "        \"\"\"\n",
    "        query_params = {\n",
    "            'top_k': top_n,\n",
    "            'vector': query_vector,\n",
    "            'include_metadata': True,\n",
    "            'include_values': False\n",
    "        }\n",
    "        if filter_metadata is not None:\n",
    "            query_params['metadata'] = filter_metadata\n",
    "\n",
    "        results = self.index.query(**query_params)\n",
    "        return results['matches']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalAndRanking:\n",
    "    def __init__(self, data_source: DataSource, embedding: Embedding, vector_storage: VectorStorage):\n",
    "        \"\"\"\n",
    "        Initializes the RetrievalAndRanking object with a data source, embedding model, and vector storage.\n",
    "\n",
    "        Args:\n",
    "            data_source (DataSource): The data source containing the preprocessed text data.\n",
    "            embedding (Embedding): The embedding model used to generate vector representations of text.\n",
    "            vector_storage (VectorStorage): The storage for vector representations of text data.\n",
    "        \"\"\"\n",
    "        self.data_source = data_source\n",
    "        self.embedding = embedding\n",
    "        self.vector_storage = vector_storage\n",
    "\n",
    "    def retrieve_relevant_chunks(self, query: str, top_n: int = 2) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Jaccard similarity.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            top_n (int): The number of top relevant chunks to retrieve (default is 2).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of top relevant chunks from the data source.\n",
    "        \"\"\"\n",
    "        # Tokenize and convert query to a set of tokens\n",
    "        query_tokens = set(self.data_source.tokenize(query)) \n",
    "        # Initialize a list to store chunks and their similarity scores\n",
    "        similarities: List[Tuple[str, float]] = []  \n",
    "\n",
    "        for chunk in self.data_source.processed_data:\n",
    "            # Tokenize and convert chunk to a set of tokens\n",
    "            chunk_tokens = set(self.data_source.tokenize(chunk))\n",
    "            # Calculate Jaccard similarity between query and chunk\n",
    "            similarity = len(query_tokens.intersection(chunk_tokens)) / len(\n",
    "                query_tokens.union(chunk_tokens)\n",
    "            )\n",
    "            # Append the chunk and its similarity score to the list\n",
    "            similarities.append((chunk, similarity))\n",
    "\n",
    "        # Sort the chunks by similarity score in descending order\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Return the top-n most relevant chunks\n",
    "        return [chunk for chunk, _ in similarities[:top_n]]\n",
    "\n",
    "    def retrieve_relevant_chunks_euclidean(self, query: str, top_n: int = 2) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Euclidean distance on TF-IDF vectors.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            top_n (int): The number of top relevant chunks to retrieve (default is 2).\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of top relevant chunks from the data source.\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer()  # Create a TF-IDF vectorizer\n",
    "        # Tokenize each string in the data source and join the tokens back into strings\n",
    "        tokenized_data_source = [' '.join(self.data_source.tokenize(text)) for text in self.data_source.processed_data]\n",
    "        # Fit and transform the tokenized data source into a TF-IDF matrix\n",
    "        tfidf_matrix = vectorizer.fit_transform(tokenized_data_source)\n",
    "        # Tokenize the query and join the tokens back into a string\n",
    "        tokenized_query = ' '.join(self.data_source.tokenize(query))\n",
    "        # Transform the tokenized query into a TF-IDF vector\n",
    "        query_vector = vectorizer.transform([tokenized_query])\n",
    "        # Calculate the Euclidean distance between the query vector and each chunk vector\n",
    "        similarities = euclidean_distances(query_vector, tfidf_matrix).flatten()\n",
    "        # Get the indices of the top-n closest chunks\n",
    "        top_indices = similarities.argsort()[:top_n]\n",
    "\n",
    "        # Return the top-n closest chunks from the data source\n",
    "        return [self.data_source.processed_data[i] for i in top_indices]\n",
    "\n",
    "    def retrieve_relevant_chunks_pinecone(self, queries: List[str], top_n: int = 2, filter_metadata: Dict[str, str] = None) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant chunks from the data source based on the query using Pinecone vector storage.\n",
    "\n",
    "        Args:\n",
    "            queries (List[str]): A list of user queries.\n",
    "            top_n (int): The number of top relevant chunks to retrieve for each query (default is 2).\n",
    "            filter_metadata (Dict[str, str], optional): A dictionary containing metadata key-value pairs to filter the results (default is None).\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A list containing the top-n most relevant chunks for each query.\n",
    "        \"\"\"\n",
    "        relevant_chunks = []\n",
    "\n",
    "        for query in queries:\n",
    "            query_embedding = self.embedding.embed(query)\n",
    "            results = self.vector_storage.search_vectors(query_embedding, top_n)\n",
    "\n",
    "            filtered_results = []\n",
    "            if filter_metadata is not None:\n",
    "                for result in results:\n",
    "                    metadata = result.get('metadata', {})\n",
    "                    if all(metadata.get(key) == value for key, value in filter_metadata.items()):\n",
    "                        filtered_results.append(result)\n",
    "            else:\n",
    "                filtered_results = results\n",
    "\n",
    "            relevant_chunks.append([result['metadata']['text'] for result in filtered_results])\n",
    "\n",
    "        return relevant_chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, api_key: str = None, model_name: str = None, device: str = \"cuda\", consumer_group: str = \"mistral\"):\n",
    "        \"\"\"\n",
    "        Initializes the LLM object with the specified API key, model name, device, and consumer group.\n",
    "        Sets up the OpenAI client if an API key is provided.\n",
    "\n",
    "        Args:\n",
    "            api_key (str, optional): The API key for accessing the OpenAI service. Default is None.\n",
    "            model_name (str, optional): The name of the model to be used. Default is None.\n",
    "            device (str, optional): The device to be used for running the model (\"cuda\" or \"cpu\"). Default is \"cuda\".\n",
    "            consumer_group (str, optional): The consumer group to be used. Default is \"mistral\".\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.consumer_group = consumer_group\n",
    "\n",
    "        if self.api_key:\n",
    "            self.client = OpenAI(api_key=self.api_key)  # Set up the OpenAI client with the provided API key\n",
    "        else:\n",
    "            print('Coming Soon')  # Placeholder message for when API key is not provided\n",
    "\n",
    "    def switch_model(self, model_name: str, device: str):\n",
    "        \"\"\"\n",
    "        Switches to a different model and device for the LLM. Creates a new reader if necessary.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the new model to switch to.\n",
    "            device (str): The device to be used for the new model (\"cuda\" or \"cpu\").\n",
    "        \"\"\"\n",
    "        current_readers = self.takeoff_client.get_readers()  # Retrieve the current readers from the takeoff client\n",
    "\n",
    "        # Check if a reader for the desired model already exists\n",
    "        reader_id = None\n",
    "        for group, readers in current_readers.items():\n",
    "            for reader in readers:\n",
    "                if reader['model_name'] == model_name:  # Check if the desired model is already in use\n",
    "                    reader_id = reader['reader_id']\n",
    "                    break\n",
    "            if reader_id:\n",
    "                break\n",
    "\n",
    "        if reader_id:\n",
    "            print(f\"Reader for model '{model_name}' already exists with reader_id: {reader_id}\")\n",
    "        else:\n",
    "            reader_config = {\n",
    "                \"model_name\": model_name,\n",
    "                \"device\": device,\n",
    "                \"consumer_group\": self.consumer_group\n",
    "            }\n",
    "\n",
    "            reader_id, _ = self.takeoff_client.create_reader(reader_config=reader_config)  # Create a new reader\n",
    "            print(f\"Created a new reader with reader_id {reader_id}\")\n",
    "\n",
    "    def answer_query(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates an answer to a query based on the provided context using the LLM.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            context (str): The context to be used for answering the query.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated answer to the query.\n",
    "        \"\"\"\n",
    "        prompt = f\"Based on the provided context, answer the following query: {query}\\n\\nContext:\\n{context}. Do not use your knowledge, only the context\"\n",
    "        \n",
    "        if self.api_key:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query,\n",
    "                    },\n",
    "                ],\n",
    "                model=\"gpt-3.5-turbo\",  # Specify the model to be used for generating the response\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content.strip()  # Return the generated response\n",
    "        else:\n",
    "            response = self.takeoff_client.generate(prompt, consumer_group=self.consumer_group)  # Generate response using the takeoff client\n",
    "            if 'text' in response:\n",
    "                return response['text'].strip()  # Return the generated response\n",
    "            else:\n",
    "                print(f\"Error generating response: {response}\")\n",
    "                return \"Unable to generate a response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(queries: List[str], data_source: DataSource, retrieval_and_ranking: RetrievalAndRanking, llm: LLM, retrieval_method: str = \"default\") -> List[str]:\n",
    "    user_queries = [UserQuery(query) for query in queries]\n",
    "    answers = []\n",
    "\n",
    "    for user_query in user_queries:\n",
    "        if retrieval_method == \"default\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks(user_query.query)\n",
    "            context = \"\\n\".join(relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        elif retrieval_method == \"euclidean\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_euclidean(user_query.query)\n",
    "            context = \"\\n\".join(relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        elif retrieval_method == \"pinecone\":\n",
    "            relevant_chunks = retrieval_and_ranking.retrieve_relevant_chunks_pinecone([user_query.query])\n",
    "            context = \"\\n\".join(\"\\n\".join(chunks) for chunks in relevant_chunks)\n",
    "            answer = llm.answer_query(user_query.query, context)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown retrieval method: {retrieval_method}\")\n",
    "\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_source:DataSource,\n",
    "         retrieval_method: str = \"default\",\n",
    "         model_choice: str = \"openai\",\n",
    "         model_name: str = None,\n",
    "         device: str = \"cpu\",\n",
    "         embedding_model_name = 'all-MiniLM-L6-v2',\n",
    "         use_local=False,\n",
    "         index_name=\"my-index\"\n",
    "         ):\n",
    "    \"\"\"\n",
    "    Main function for running the retrieval and ranking system with user interaction.\n",
    "\n",
    "    Args:\n",
    "        data_source (DataSource): An instance of the DataSource class providing the data.\n",
    "        retrieval_method (str, optional): The method to use for retrieval and ranking. Defaults to \"default\".\n",
    "        model_choice (str, optional): The choice of model for language understanding. Defaults to \"openai\".\n",
    "        model_name (str, optional): The name of the model to be used. Defaults to None.\n",
    "        device (str, optional): The device to be used for embedding. Defaults to \"cpu\".\n",
    "        embedding_model_name (str, optional): The name of the embedding model to be used. Defaults to 'all-MiniLM-L6-v2'.\n",
    "        use_local (bool, optional): Whether to use a local model for embedding. Defaults to False.\n",
    "        index_name (str, optional): The name of the index for vector storage. Defaults to \"my-index\".\n",
    "    \"\"\"\n",
    "    embedding = Embedding(model_name=embedding_model_name,\n",
    "                          device=device,\n",
    "                          use_local=use_local)\n",
    "    vector_storage = PineconeVectorStorage(index_name, embedding)\n",
    "    processed_data = data_source.process_data()\n",
    "    metadatas = [{'text': text, 'category': 'finance'} for text in processed_data]\n",
    "    vectors = [embedding.embed(text) for text in processed_data]\n",
    "    vector_storage.store_vectors(vectors, metadatas)\n",
    "\n",
    "    retrieval_and_ranking = RetrievalAndRanking(data_source, embedding, vector_storage)\n",
    "\n",
    "    if model_choice == \"openai\":\n",
    "        llm = LLM(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Not yet implemented\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        queries = user_input.split(\";\")  # Split multiple queries separated by semicolon\n",
    "        answers = process_query(queries, data_source, retrieval_and_ranking, llm, retrieval_method)\n",
    "\n",
    "        print(\"User Queries:\")\n",
    "        for query, answer in zip(queries, answers):\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_source \u001b[39m=\u001b[39m DataSource(data)\n\u001b[1;32m      2\u001b[0m data_source\u001b[39m.\u001b[39mprocess_data()\n\u001b[0;32m----> 3\u001b[0m main(data_source, retrieval_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpinecone\u001b[39;49m\u001b[39m'\u001b[39;49m, index_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy-index4\u001b[39;49m\u001b[39m\"\u001b[39;49m, embedding_model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mall-MiniLM-L6-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(data_source, retrieval_method, model_choice, model_name, device, embedding_model_name, use_local, index_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m processed_data \u001b[39m=\u001b[39m data_source\u001b[39m.\u001b[39mprocess_data()\n\u001b[1;32m     28\u001b[0m metadatas \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: text, \u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mfinance\u001b[39m\u001b[39m'\u001b[39m} \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m processed_data]\n\u001b[0;32m---> 29\u001b[0m vectors \u001b[39m=\u001b[39m [embedding\u001b[39m.\u001b[39membed(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m processed_data]\n\u001b[1;32m     30\u001b[0m vector_storage\u001b[39m.\u001b[39mstore_vectors(vectors, metadatas)\n\u001b[1;32m     32\u001b[0m retrieval_and_ranking \u001b[39m=\u001b[39m RetrievalAndRanking(data_source, embedding, vector_storage)\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m processed_data \u001b[39m=\u001b[39m data_source\u001b[39m.\u001b[39mprocess_data()\n\u001b[1;32m     28\u001b[0m metadatas \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: text, \u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mfinance\u001b[39m\u001b[39m'\u001b[39m} \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m processed_data]\n\u001b[0;32m---> 29\u001b[0m vectors \u001b[39m=\u001b[39m [embedding\u001b[39m.\u001b[39;49membed(text) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m processed_data]\n\u001b[1;32m     30\u001b[0m vector_storage\u001b[39m.\u001b[39mstore_vectors(vectors, metadatas)\n\u001b[1;32m     32\u001b[0m retrieval_and_ranking \u001b[39m=\u001b[39m RetrievalAndRanking(data_source, embedding, vector_storage)\n",
      "Cell \u001b[0;32mIn[8], line 55\u001b[0m, in \u001b[0;36mEmbedding.embed\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNot yet implemented!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode(text)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_model_dimension:\n\u001b[1;32m     57\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDimension mismatch detected: Expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_model_dimension\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:371\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    368\u001b[0m features\u001b[39m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    370\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 371\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    372\u001b[0m     out_features[\u001b[39m\"\u001b[39m\u001b[39msentence_embedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m truncate_embeddings(\n\u001b[1;32m    373\u001b[0m         out_features[\u001b[39m\"\u001b[39m\u001b[39msentence_embedding\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtruncate_dim\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     96\u001b[0m     trans_features[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     99\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m\"\u001b[39m: output_tokens, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: features[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_FP/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[0;32m--> 365\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attention_probs, value_layer)\n\u001b[1;32m    367\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    368\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_source = DataSource(data)\n",
    "data_source.process_data()\n",
    "main(data_source, retrieval_method='pinecone', index_name=\"my-index4\", embedding_model_name=\"all-MiniLM-L6-v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_FP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
